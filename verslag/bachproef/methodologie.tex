%%=============================================================================
%% Methodologie
%%=============================================================================

\chapter{\IfLanguageName{dutch}{Methodologie}{Methodology}}%
\label{ch:methodologie}

Met de verworven kennis uit de literatuursudie in Hoofdstuk \ref{ch:stand-van-zaken} over de noden van scholieren met dyslexie, het complexe karakter van wetenschappelijke artikelen en ten slotte de technieken voor ATV en MTV kunnen de onderzoeksmethoden toegepast worden om een antwoord te kunnen vormen op de onderzoeksvraag.

\section{Requirementsanalyse}
\label{sec:requirementsanalyse}

Deze onderzoeksfase doelt op het uittesten van verschillende tools op hun functionaliteiten voor geautomatiseerde en gepersonaliseerde tekstvereenvoudiging. De requirementsanalyse biedt een antwoord op de volgende twee subvragen, aan de hand van de technieken voor tekstvereenvoudiging beschreven in de literatuurstudie \ref{ch:stand-van-zaken}. 

\begin{itemize}
	\item Welke functies ontbreken AI-toepassingen om geautomatiseerde tekstvereenvoudiging mogelijk te maken voor scholieren met dyslexie in de derde graad middelbaar onderwijs?
	\item Welke manuele methoden voor tekstverereenvoudiging komen niet in deze tools voor?
\end{itemize}

De requirementsanalyse analyseert toepassingen die enerzijds de overheid aan scholieren met dyslexie in het onderwijs aanbiedt, anderzijds online tools die leerkrachten kunnen gebruiken om teksten te vereenvoudigen. Om de lijst van beschikbare tools in te perken, beperkt het onderzoek zich op de volgende tools:

\begin{itemize}
	\item Simplish
	\item Resoomer
	\item ChatGPT
	\item Bing Chatbot
\end{itemize}

Vijf verschillende aspecten, namelijk lexicale en syntactische vereenvoudiging, samenvatting, personalisatie en ontwikkelaarsmogelijkheden dienen als de houvast bij deze requirementsanalyse. De resultaten van deze onderzoeksfase dienen als vereisten om een prototype voor tekstvereenvoudiging te ontwikkelen, specifiek op maat om wetenschappelijke artikelen gepersonaliseerd te vereenvoudigen op maat voor scholieren met dyslexie in de derde graad van het middelbaar onderwijs. De tools ondergaan een test om hun vermogen om een wetenschappelijk artikel in PDF-formaat te vereenvoudigen, indien dat mogelijk is. Als dit niet mogelijk is, gebeurt het kopiëren en plakken van de tekst in de toepassing. Op deze manier vindt een evaluatie van de prestaties van de verschillende tools plaats en worden eventuele beperkingen of tekortkomingen geïdentificeerd.

\section{Vergelijkende studie}

Taalmodellen zijn nodig om teksten te kunnen vereenvoudigen, maar een toepassing voor tekstvereenvoudiging binnen de casus van wetenschappelijke artikelen moet gebruik maken van een taalmodel dat het meest aansluit op deze casus. Om het prototype af te stemmen op één taalmodel, vereist een antwoord op de volgende vraag. 

\begin{itemize}
	\item Welk taalmodel of LLM is ideaal voor automatische tekstvereenvoudiging voor vereenvoudigde wetenschappelijke artikelen voor scholieren met dyslexie in de derde graad van het middelbaar onderwijs met dezelfde of gelijkaardige kwaliteiten als manuele tekstvereenvoudiging?
\end{itemize}

De literatuurstudie identificeerde de volgende vier taalmodellen voor opname in de vergelijkende studie.

\begin{itemize}
	\item Een gefinetunede versie van T5\footnote{https://huggingface.co/husseinMoh/t5-small-finetuned-text-simplification}.
	\item Een gefinetunede versie van BART\footnote{https://huggingface.co/sambydlo/bart-large-scientific-lay-summarisation}, getraind op woordenschat van Engelstalige wetenschappelijke artikelen.
	\item Scientific Abstract Simplification\footnote{https://huggingface.co/haining/scientific\_abstract\_simplification}
	\item GPT-3
\end{itemize}

Zoals aangegeven in het onderzoek van \textcite{Nenkova2004} worden leesgraadsformules en referentieteksten ingezet om de kwaliteit van een vereenvoudigde tekst te beoordelen. De taalmodellen en API's ondergaan een vergelijking met elkaar en met een referentietekst om de kwaliteit van de gegenereerde tekstvereenvoudigingen te beoordelen. De vergelijking gebeurt zowel subjectief als objectief aan de hand van de volgende leesgraadsmetrieken: 

\begin{itemize}
	\item Flesch-Reading-Ease
	\item Gunning FOG
\end{itemize}

Deze evaluatie leidt tot de keuze van een taalmodel dat het meest geschikt is voor deze specifieke casus van wetenschappelijke artikelen, die gepersonaliseerd en geautomatiseerd moeten worden vereenvoudigd voor scholieren met dyslexie in de derde graad van het middelbaar onderwijs. Het resultaat van deze onderzoeksfase is één taalmodel wat zal dienen als de basis en als methode om gekregen wetenschappelijke artikelen te vereenvoudigen op maat voor scholieren met dyslexie in de derde graad van het middelbaar onderwijs.

\subsection{Vergelijking met referentieteksten}

Dit onderzoek vergelijkt twee wetenschappelijke artikelen met een referentietekst die handmatig is vereenvoudigd door twee lectoren en twee scholieren. Deze vier personen baseren zich op vooraf meegekregen richtlijnen, toegelicht in bijlage ...

\medspace

GPT-3 en de HuggingFace taalmoddelen kunnen enkel per API worden aangesproken en zijn daarmee niet in staat om direct PDF's te opladen. De inhoud wordt met de code op (TODO REF TOEVOEGEN) opgehaald en vervolgens direct aan het model gegeven om zo een resultaat te bekomen. Bij het gebruik van het GPT-3 model worden vijf prompts gebruikt en om rekening te houden met de tokenlengte, wordt de tekst tussen de subtitels opgebroken. De prompts zijn zoals volgt opgesteld:

\begin{itemize}
	\item P1: Simplify this text in Dutch
	\item P2: Simplify a text for economics students (16-18 years old) by replacing difficult words, keeping technical jargon, replacing words longer than 18 letters, writing acronyms in full, replacing a word only once with a synonym, giving brief explanations when necessary, and avoiding percentages.
	\item P3: Simplify a text by breaking them into shorter sentences with a maximum of ten words. Change pronouns like 'they', 'their', or 'he' to names. Replace complex sentence constructions and prepositional phrases with simpler alternatives, but leave them unchanged if no simpler option is available.
	\item P4: Simplify an article for economics students (16-18 years old) by replacing difficult words (except technical jargon learned in the 2nd grade), words longer than 18 letters, and avoiding percentages. Use synonyms only once and give brief explanations if necessary. Write acronyms in full and replace pronouns with names. Simplify sentence constructions and prepositional phrases, splitting sentences into a maximum of ten words, but leave them unchanged if no simpler alternative is available.
\end{itemize}

\section{Prototype voor tekstvereenvoudiging}

Dit hoofdstuk omschrijft de ontwikkeling van een prototype voor tekstvereenvoudiging voor scholieren met dyslexie in de derde graad van het middelbaar onderwijs. Deze onderzoeksmethode geeft een antwoord op de volgende deelvraag: 
\begin{itemize}
	\item Hoe kan een intuïtieve en lokale webtoepassing worden ontwikkeld die zowel scholieren met dyslexie als docenten helpt bij het vereenvoudigen van wetenschappelijke artikelen met behoud van semantiek, jargon en zinsstructuren?
\end{itemize}

Het prototype is ontwikkeld met de benodigde functionaliteiten en eigenschappen uit de requirementsanalyse en wordt enkel lokaal uitgerold. De uitrol van dit prototype gebeurt enkel lokaal door middel van Docker. Python en het Flask-framework dienen als de basis voor deze webtoepassing. Het onderstaande schema haalt de gebruikte programmeertalen aan waaruit dit prototype bestaat. 

\begin{center}
	\begin{tabular}{ | m{4cm} | m{12cm} | } 
		\hline
		\textbf{Technologie} & \textbf{Functionaliteit} \\
		\hline
		
		\hline
	\end{tabular}
	\label{table:technologies}
\end{center}


\begin{center}
	\begin{tabular}{ | m{4cm} | m{12cm} | } 
		\hline
		\textbf{Python-bibliotheek} & \textbf{Functionaliteit} \\
		\hline
		PDFMiner & Tekstinhoud van PDF's inlezen. \\ 
		EasyOCR	& Pagina van een PDF opslaan als afbeelding. Tekst uit een afbeelding extraheren. \\
		Spacy & PoS-tagging en lemmatizing \\
		NumPy & De reshape-functie vereenvoudigt de manier om arrays van zinnen bij elkaar te plaatsen om zo een paragraaf te bekomen. \\
		\hline
	\end{tabular}
	\label{table:python-libraries}
\end{center}

Het prototype is opgebouwd in Python met het Flask-framework om de webapplicatie te ondersteunen. Aanvullend op Flask maakt het prototype gebruik van Jinja, HTML, CSS en JavaScript (JS) zoals vermeld in \ref{table:technologies}. Tekstvereenvoudigingsfuncties worden eerst in Jupiter notebooks uitgetest, vooraleer deze in het prototype worden geplaatst. Docker wordt gebruikt als opzet voor de ontwikkelaars. Een scriptbestand vereenvoudigt de opstart van deze webapplicatie in tegenstelling tot de opstart per terminal. De nodige Python-bibliotheken worden alvorens opgehaald met Pipreq. Alle taalmodellen worden per API aangesproken, ofwel één Docker-container voor de webapplicatie volstaat voor dit prototype.

\subsection{Tool voor leerkrachten}

\begin{enumerate}
	\item PDF of tekstinhoud opladen met PDFMiner
	\item Alle zinnen uit de opgeladen tekst ophalen met Spacy.
	\item Woorden taggen met de PoS-functie van Spacy.
	\item Opties voor gepersonaliseerde tekstvereenvoudiging aanreiken aan de eindgebruiker.
	\item Tekst vereenvoudigen op basis van gepersonaliseerde keuzeopties.
	\item Nieuw artikel genereren met Pandoc.
\end{enumerate}

\subsection{Tekstinhoud extraheren}

Eindgebruikers kunnen met het prototype wetenschappelijke artikelen op één van twee manieren inladen: \textit{plaintext} of via een PDF-bestand. De werking om een wetenschappelijk artikel in te lezen is identiek bij zowel de toepassing voor scholieren als die voor lectoren en daarmee is de code herbruikbaar. De Flask-applicatie controleert vooraf de type invoer om vervolgens de Reader-klasse aan te spreken die de tekst verder verwerkt. PDF's worden tijdelijk in-memory opgeslaan. Er wordt rekening gehouden met de splitsing tussen normale upload en geavanceerde upload. Deze twee methoden zijn terug te vinden in de Reader-klasse\footnote{https://github.com/Dyashen/text-simplification-tool/blob/main/web-app/Reader.py}.

\begin{itemize}
	\item PDFMiner itereert doorheen de PDF en extraheert vervolgens de tekst op iedere pagina. Deze methode resulteert in een string-object.
	\item PDF extractors, waaronder PDFMiner, kunnen tekstinhoud verliezen tijdens het extraheren zoals eerder aangewezen in \ref{sec:requirementsanalyse}. Als vangnet biedt het prototype een tweede optie aan waarbij de PDF-pagina's als afbeelding worden opgeslaan. De Python-bibliotheek EasyOCR voorziet een eenduidige en ontwikkelaarsvriendelijke manier om PDF-pagina's op te slaan als JPG of PNG. Tesserat biedt een even eenduidige oplossing aan, maar vergt meer configuraties en daarmee vergroot de omvang van de Docker-container. Vervolgens worden de afbeeldingen per tekst-chunk ingelezen en de tekst wordt opgeslaan. Het gebruik van deze alternatieve methode kan de omvang van de Docker-container vergroten. Daarom verwijdert het prototype deze afbeeldingen nadat deze de tekstinhoud hebben ingelezen en opgeslaan om ruimte te besparen. Net zoals bij de eerste methode resulteert deze methode in een string-object.
\end{itemize}

% Wetenschappelijke artikelen wijken niet af van het standaard PDF-formaat. Dit formaat is eenvoudig te extraheren met behulp van PDFMiner. De code om tekstinhoud te extraheren is herbruikbaar voor het aansluitende scholieren-component.

\subsection{Zinnen ophalen}

De eerste fase van het prototype slaat de tekstinhoud op in meerdere arrays die zinnen voorstellen. Door middel van een aparte functie wordt de tekst opgesplitst per zin. Het resultaat van deze transformatie is een tweedimensionale array. Deze transformatie bevoordeelt het proces om vervolgens de teksten per zin op de webpagina uit te printen. 

\subsection{PoS-tagging}

De key-value paren dienen om de woorden aan hun respectievelijke PoS-tag te koppelen. De sleutel verwijst naar het woord in de zin en de waarde verwijst naar de PoS-tag die aan dit woord toebehoord.  Dit prototype houdt enkel rekening met de PoS-tagging van Nederlandstalige en Engelstalige teksten en daarom laadt het prototype enkel twee embeddingsmodellen op, zoals aangeduid in \ref{table:wordembeddings-spacy}. Hardcoderen is uit den boze en zo maakt het prototype gebruik van een dictionary die de naam van deze embeddingsmodellen bijhoudt. Het prototype hoeft daarmee enkel de taal te herkennen en die vervolgens door te geven aan de dictionary. Deze methode fouttolerant maken kan door Engels als standaardtaal mee te geven of door vooraf de gebruiker te vragen in welke taal de opgelade tekst staat.

\begin{center}
	\begin{tabular}{ | m{4cm} | m{12cm} | } 
		\hline
		\textbf{Taal} & \textbf{Embeddingsmodel} \\
		\hline
		Nederlands & ... \\ 
		\hline
		Engels & ... \\
		\hline
	\end{tabular}
	\label{table:wordembeddings-spacy}
\end{center}

\subsection{Opties voor gepersonaliseerde tekstvereenvoudiging aanreiken.}

\subsection{Tekstvereenvoudiging met API}

Om afwijkende resultaten op een GPT-prompt te vermijden, wordt de temperature op nul geplaatst en de \textit{top\_p} waarde wordt ingeschat op 80\%. SpaCy wordt gebruikt om woordkenmerken zoals de PoS-tag op te halen, maar het systeem is vatbaar voor het niet kunnen vinden van afwisselende en meertalige woordenschat. Een mogelijke oplossing is om de taal te veranderen naar Engels of Frans, of een aangepast taalherkenningsmodel te gebruiken. Een andere optie is om de tekst voor te verwerken om de Nederlandse en Engelse woorden te scheiden voordat ze worden verwerkt met SpaCy. Adjectieven uit de tekst verwijderen is mogelijk zonder taalmodel. Aangezien alle woorden gekoppeld worden aan een PoS-tag, is het eenvoudig om de woorden gelinkt aan de span-tag van de adjectieven uit te filteren.

\subsubsection{Annotaties van woordenschat}

De eenduidige HTML-structuur van online woordenboeken maken het mogelijk om gratis en eenvoudig de definities van woorden op te halen. Zo is het mogelijk om annotaties op te halen zoals aangewezen in het onderzoek van \textcite{Bulte2018}. Met behulp van Requests en BeautifulSoup is het mogelijk om lijsten met definities te scrapen van deze sites. De stam van het gemarkeerde woord wordt opgehaald en vervolgens meegegeven als zoekopdracht. De bron wordt samen met het resultaat aan de eindgebruiker getoond. 

\subsubsection{Tekstvereenvoudiging}

Het prototype gebruikt een taalmodel van \textit{HuggingFace} voor extraherende samenvattingen en zowel gratis taalmodellen van \textit{HuggingFace} als het GPT-3 taalmodel voor abstraherende samenvattingen. Het model kan parameters, zoals maximale lengte van de gegenereerde tekst, ontvangen en biedt zowel gepersonaliseerde als niet-gepersonaliseerde vereenvoudiging. Het gebruik van \textit{HuggingFace} vereist een internetverbinding en kan geen extra trainingsdata bevatten. De opstarttijd voor alle \textit{HuggingFace}-taalmodellen wordt bij de start van de applicatie afgehandeld door middel van een extra parameter de request. Sleutels worden standaard bijgehouden in env-bestanden. Via de webtoepassing kan een gebruiker deze sleutel aanpassen. Binnen een lokale omgeving is dit in orde, al moeten ontwikkelaars rekening houden met beveiligingsmaatregelen wanneer een dergelijke tool wordt uitgerold. Het merendeel van de gebruikte taalmodellen is Engelstalig of is nadrukkelijk getraind op basis van Engelstalige datasets. De ingegeven tekst wordt eerst vertaald naar het Engels om zo de kans op een accurate vereenvoudiging te verhogen. Voor de vertaling wordt de Google Translate Python-package gebruikt. Deze is minder accuraat vergeleken met DeepL, maar biedt een gratis beschikbaar en aanvaardbaar alternatief aan. Factoren zoals topic diversity en semantische redundantie moeten overwogen worden bij het kiezen van een taalmodel voor extraherend samenvatten. Lange documenten samenvatten kan zoals aangeduid in literatuurstudie door extraherende samenvatting, gevolgd door abstraherende samenvatting om de tekst coherent te doen blijken. Eerder werd er gekozen om de voltekst per paragraaf bij te houden. Uit iedere paragraaf wordt een ideaal aantal zinnen gemarkeerd om nadien geparafraseerd te worden door GPT-3 of een \textit{HuggingFace} taalmodel, afhankelijk van de keuze van de eindgebruiker.


\subsection{Personaliseerbaarheid aanreiken}

Voor de webtoepassing worden de standaardparameters gebruikt die uitgewezen zijn in \textcite{Rello2013a, Rello2013b}. Met JavaScript is het mogelijk om deze parameters dynamisch en on-the-spot aan te passen. Deze gekozen parameters worden opgeslaan als sessievariabelen, zodat de eindgebruiker niet per pagina deze parameters moet instellen. Om het uitvoerbestand personaliseerbaar te maken, worden opties in een formulier opgevraagd en vervolgens meegegeven in de Pandoc YAML-header.

\subsubsection{Taalmodellen}

SC en BART-SC transformeren de tekst op lexicaal en syntactisch niveau. Zij bekijken enkel de gekregen zin. Andere taalmodellen zijn eerder geneigd om extra tekst toe te voegen. Er kan niet achterhaald worden waarom dat deze extra tekst wordt meegegeven. BART-SC kan bijzaak behouden, terwijl SC sneller de neiging heeft om enkel de kernzaak te behouden in de vereenvoudigde tekst. Bij de inference API's moet er expliciet worden aangegeven om welke transformatie dit gaat door kernwoorden zoals 'summarize:'.

\subsection{Tekstinhoud uitschrijven naar PDF/DOCX}

De zelfgebouwde Creator-klasse bouwt PDF's en docx-documenten op volgens de meegegeven personalisatie. Het prototype maakt gebruik van Pandoc, of PyPandoc via Python, om tekstinhoud naar een PDF of een docx-bestand uit te schrijven. Pandoc maakt gebruik van een tweestapsbeweging waarbij rauwe tekst eerst naar een Markdown-formaat wordt omgezet.

\subsubsection{YAML-header}

Met Python wordt eerst een YAML-header in het te-transformeren Markdown-bestand geschreven. De YAML-header omvat de titel, standaardlettertype en lettertype voor de titel, de datum, het type document dat moet worden gegenereerd, de marge-instelling, de standaardlettergrootte, woord-spatiëring en ten slotte de instelling voor de regeleinde. De meegekregen gepersonaliseerde instellingen wordt meegegeven in een LateX YAML-header. 

\subsubsection{Woordenlijst en vereenvoudigde tekst uitschrijven}

De structuur om de woordenlijst op te bouwen, is identiek zoals dat van een Markdown-tabel. De  woordenlijst wordt in dictionary-structuur meegegeven. De sleutels worden overlopen en vervolgens wordt ieder woord samen met de PoS-tag en de definitie uitgeprint. De vereenvoudigde tekst is eveneens in een dictionary-structuur opgeslaan. De keys stellen titels voor en worden uitgeprint voorafgegaan door twee hekje-symbolen. Tussen de titels worden breaklines toegevoegd, gevolgd door de tekst die bij de titel bijhoort. Indien gekozen werd voor een opsomming, dan wordt er gebruik gemaakt van een geneste for-lus waarbij iedere zin wordt voorafgegaan aan een asterisk-symbool. De woordenlijst en vereenvoudigde tekst worden naar hetzelfde Markdown-bestand uitgeschreven. 

\subsubsection{Documenten genereren}

Als invoer wordt het pad naar opgevulde Markdown-bestand meegegeven. De uitvoer is het pad waarnaar het PDF- of DOCX-bestand moet worden opgeslaan. Vervolgens zet Pandoc het Markdown-bestand om naar een PDF-bestand gebouwd met de XeLateX engine of een Word-bestand op basis van meegekregen binaries. Pandoc Flask kan enkel één bestand aan de gebruiker teruggeven. Als oplossing comprimeert het prototype met \textit{zipfile} de PDF- en Wordbestand tot één bestand. 

\subsection{Conclusie}

Flowchart voor ontwikkeling
1. Requirementsanalyse + opzet moscow.
2. Te gebruiken taalmodellen achterhalen.
3. Functies schrijven in Jupyter notebooks of soortgelijke scripts.

Dit prototype wordt enkel binnen een lokale omgeving opgezet en is nog niet bruikbaar voor het grote publiek. Met PDF's of voltekst als invoer is het prototype in staat om teksten lexicaal en syntactisch te vereenvoudigen. Het prototype is functioneel voor zowel de doelgroep lectoren als leerlingen, twee doelgroepen die elk een andere functionaliteit prioriteren. 

\medspace

Het prototype gebruikt API's waaronder de \textit{HuggingFace} Inference APIs en de GPT-3 API. Aanvullend hierop kunnen ontwikkelaars deze modellen extra trainen op basis van de gewenste casus. 

\medspace

Ontwikkelaars moeten rekening houden met het gebrek aan structuur bij het ophalen van tekstinhoud uit een PDF-bestand.