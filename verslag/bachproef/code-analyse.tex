\chapter{\IfLanguageName{dutch}{Code: Vergelijkende Studie}{Attachment 1}}%
\label{ch:bijlage-code}

\begin{lstlisting}[language=Python, caption={Script voor fase 1 van de vergelijkende studie.}, label={code:verg-studie-phase-1}]
import os, re

def add_newline_after_dot(input_file, output_file):
	with open(input_file, 'r', encoding='utf-8') as file:
		text = file.read()
	text = re.sub(r'\d', '', text)
	modified_text = text.replace('.', '.\n')
	with open(output_file, 'w', encoding='utf-8') as file:
		file.write(modified_text)

folder_path = 'scripts\pdf'
original_scientific_papers = [f for f in os.listdir(folder_path)]

for paper in original_scientific_papers:
	input_file =  folder_path + '/' + paper
	output_file = folder_path + '/' + 'RE_' + paper
	add_newline_after_dot(input_file, output_file)
\end{lstlisting}

\newpage

\begin{center}
	\begin{lstlisting}[language=Python, caption={Script voor de tweede fase van de vergelijkende studie.}, label={code:verg-studie-phase-2}]
import os
import pandas as pd
from deep_translator import GoogleTranslator

output_csv = 'results.csv'

def translate_dutch_to_english(dutch_text_file):
	with open(dutch_text_file, 'r', encoding='utf-8') as file:
		dutch_sentences = file.readlines()
		dutch_sentences = [sentence.strip() for sentence in dutch_sentences]

	english_sentences = []
	for sentence in dutch_sentences:
		translated = GoogleTranslator(source='nl', target='en').translate(sentence)
		english_sentences.append(translated)
		df = pd.DataFrame({'Dutch': dutch_sentences, 'English': english_sentences})
		df.to_csv(str(dutch_text_file).split('.')[0] + '.csv', index=False)


	folder_path = 'scripts/pdf/'
	original_scientific_papers = [f for f in os.listdir(folder_path)]

	for paper in original_scientific_papers:
		if paper.startswith('RE_') and paper.endswith('.txt'):
			print(f'STARTING {paper}')
			dutch_text_file = folder_path + paper
			translate_dutch_to_english(dutch_text_file)
	\end{lstlisting}
\end{center}

\newpage

\begin{center}
	\begin{lstlisting}[language=Python, caption={Script voor de derde fase van de vergelijkende studie}, label={code:verg-studie-phase-3}]
from pdfminer.high_level import extract_pages
from pdfminer.layout import LTTextContainer
import spacy
from langdetect import detect
import pandas as pd
import os
import readability
import requests, spacy, os, numpy as np
import time, json, requests
from langdetect import detect
from googletrans import Translator
from bs4 import BeautifulSoup
from deep_translator import GoogleTranslator



folder_path = 'scripts\pdf'
dutch_spacy_model = "nl_core_news_md"
english_spacy_model = "en_core_web_sm"

dict = {
	'nl':'nl_core_news_md',
	'en':'en_core_web_sm'
}

total_df = None
gt = Translator()

huggingfacemodels = {
	'T1':"https://api-inference.huggingface.co/models/haining/scientific_abstract_simplification",
	'T2': "https://api-inference.huggingface.co/models/sambydlo/bart-large-scientific-lay-summarisation",
	'T3': "https://api-inference.huggingface.co/models/philippelaban/keep_it_simple"
}

max_length = 2000
COMPLETIONS_MODEL = "text-davinci-003"
EMBEDDING_MODEL = "text-embedding-ada-002"

languages = {
	'nl':'nl_core_news_md',
	'en':'en_core_web_md'
}

class HuggingFaceModels:
	def __init__(self, key=None):
		global huggingface_api_key
		try:
		huggingface_api_key = key
		except:
		huggingface_api_key = 'not_submitted'
	
	""""""
	def query(self, payload, API_URL):
		headers = {"Authorization": f"Bearer {huggingface_api_key}"}
		response = requests.post(API_URL, headers=headers, json=payload)
		return response.json()
	
	""""""
	def scientific_simplify(self, text, lm_key):
		try:
			API_URL = huggingfacemodels.get(lm_key)
			translated = GoogleTranslator(source='auto', target='en').translate(str(text))
		
		if lm_key == 'T1':
			result = self.query({"inputs": str('simplify: ' + str(translated)),"parameters": {"max_length": len(sentence)+10},"options":{"wait_for_model":True}}, API_URL)
		else:
			result  = self.query({"inputs": str(translated),"parameters": {"max_length": len(sentence)+10},"options":{"wait_for_model":True}}, API_URL)
		
		
		if 'generated_text' in result[0]:
			translated = GoogleTranslator(source='auto', target='nl').translate(str(result[0]['generated_text']))
			return translated
		elif 'summary_text' in result[0]:
			translated = GoogleTranslator(source='auto', target='nl').translate(str(result[0]['summary_text']))
			return translated
		else:
			return None
		except:
			return text
		
	def get_sentence_length(sentence):
		txt_language = detect(sentence)
		dic_language = languages.get(txt_language)
		nlp = spacy.load(dic_language)
		doc = nlp(sentence)
		return len()	
	
	def tokenize_text(text):
		txt_language = detect(text)
		dic_language = languages.get(txt_language)
		nlp = spacy.load(dic_language)
		doc = nlp(text)
		return doc.sents
		
	
	def process_file(file_path):
		with open(folder_path + '/' + file_path, "r", encoding='utf8') as file:
		text = file.read()
		tokens = tokenize_text(text)
		return tokens
		
	
	hf = HuggingFaceModels(key='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')
	original_scientific_papers = [f for f in os.listdir(folder_path)]
	
	for paper in original_scientific_papers[3:]:
		sentence_tokens = process_file(paper) 
		for sentence in sentence_tokens:
			for model in huggingfacemodels.keys():
				filename = "SIMPLIFIED_"+model+'_'+paper
				with open(filename, 'a', encoding='utf-8') as f:
					output = hf.scientific_simplify(str(sentence), model)
					f.write(str(output)) 	
	\end{lstlisting}
\end{center}

\newpage

\begin{center}
	\begin{lstlisting}[language=Python, caption={Script voor fase 4 van de vergelijkende studie}, label={code:verg-studie-phase-4}]
import os
from langdetect import detect
import spacy
import pandas as pd
import readability

simplified_folder = 'scripts/vereenvoudigde_artikelen'
original_folder = 'scripts/pdf'

scientific_papers = [original_folder + "/" + f for f in os.listdir(original_folder)] + [simplified_folder + "/" + f for f in os.listdir(simplified_folder)]

languages = {
	'nl':'nl_core_news_md',
	'en':'en_core_web_md'
}

df = pd.DataFrame()

for paper in scientific_papers:
with open(paper, 'r', encoding='utf-8') as file:
text = file.read()
nlp = spacy.load(languages.get('nl'))
doc = nlp(text)

for sent in doc.sents:
try:
metrics = readability.getmeasures(sent.text, lang='nl')
row = {
	'Paper': paper.split('/')[2].split('.')[0],
	'Sentence': sent.text,
	'FRE': metrics['readability grades']['FleschReadingEase'],
	'FOG': metrics['readability grades']['GunningFogIndex'],
}

for key, value in metrics['sentence info'].items():
row[key] = value

for key, value in metrics['word usage'].items():
row[key] = value

for key, value in metrics['sentence beginnings'].items():
row[key] = value

df = df.append(row, ignore_index=True)
except Exception as e:
print(e)

df.to_csv('result.csv', index=False)
	\end{lstlisting}
\end{center}
