Sinds het einde van de 20e eeuw zijn er door technologische ontwikkelingen nieuwe manieren ontstaan om gegevens te verzamelen en te analyseren.
De opkomst van 'big data' en de verbeterde mogelijkheden van kunstmatige intelligentie in de 21e eeuw zijn met veel interesse ontvangen door de politie.
Het gebruik van deze technologieën door de politie wordt algoritmische surveillance genoemd. Dit zijn systemen die gebruikmaken van regelgebaseerde algoritmen om gegevens te classificeren, op te slaan, te combineren en te doorzoeken, om vervolgens overeenkomsten te vinden met andere gegevens. Ook maken ze gebruik van algoritmen die leren van de data om patronen en nuttige kennis te voorspellen. Ondanks de toegenomen regelgeving, die bedoeld is om democratische waarborgen te bieden voor algoritmische surveillance, lijkt het gebruik ervan juist gestimuleerd te worden. Bijvoorbeeld, er is een aanzienlijke toename van het gebruik van 'intelligent' cameratoezicht in België en andere delen van Europa. Dit roept de vraag op of de huidige controlemechanismen voldoende zijn om alle burgers te beschermen tegen mogelijke gevolgen van het gebruik van algoritmische surveillance door de politie. Het doel van deze bijdrage is om na te denken over de vraag of de bestaande controle- en handhavingsmechanismen voor het gebruik van algoritmische surveillance door de politie heroverwogen moeten worden. In het eerste deel van dit artikel zal ik drie sociaal-technische ontwikkelingen bespreken die druk uitoefenen op het huidige kader. In het tweede deel zal ik de huidige controle- en handhavingsmechanismen bekijken vanuit het perspectief van de relationele ethiek om te onderzoeken hoe we kunnen leren van deze situatie en controlemechanismen kunnen heroverwegen. Als gevolg van de opkomst van algoritmische surveillance in het politiewerk kunnen drie sociaal-technische ontwikkelingen worden geïdentificeerd die het traditionele kader voor controle en handhaving onder druk zetten: 1) de versnippering en privatisering van politiewerk, 2) de democratisering van surveillance, en 3) de toename van schade en sociale gevolgen op collectief niveau. Deze ontwikkelingen overlappen en zijn met elkaar verweven, en moeten niet als afzonderlijke ontwikkelingen worden gezien. Ten eerste is de versnippering en privatisering van politiewerk niet nieuw. Sinds het einde van de 20e eeuw is er in het Westen een toename van samenwerking met de private sector en spelen particuliere actoren een steeds grotere rol in politiewerk. Dit is grotendeels te wijten aan de groeiende macht en omvang van de private sector en bezuinigingen in de publieke sector.

De ontwikkeling van grote hoeveelheden data en kunstmatige intelligentie (AI) aan het begin van de 21e eeuw heeft geleid tot een toenemende macht van technologiebedrijven. Deze bedrijven verzamelen gegevens als economische drijfveer. De politie maakt steeds meer gebruik van digitale platformen en digitale opsporingstechnologie, wat ook wel platform policing wordt genoemd. Dit zorgt ervoor dat de politie steeds afhankelijker wordt van de technologiebedrijven, waardoor de machtsverhoudingen verschuiven van de publieke naar de private sector. Hierdoor wordt transparantie en controle bemoeilijkt.

Een tweede ontwikkeling is de "democratisering" van surveillance, waarbij er een verschuiving is van gerichte surveillance naar grootschalige surveillance. Hierdoor komt een veel groter deel van de bevolking onder surveillance te staan, wat het risico op machtsmisbruik door zowel de overheid als private actoren vergroot. Vroeger waren grootschalige surveillancepraktijken vooral in handen van inlichtingendiensten of waren ze beperkt tot specifieke contexten, zoals op luchthavens. Nu spelen zowel politiediensten als technologiebedrijven een steeds grotere rol. Een voorbeeld hiervan is de aanzienlijke uitbreiding van het gebruik van "intelligent" cameratoezicht in België, dat nog meer is toegenomen tijdens de coronapandemie. Andere voorbeelden zijn de infiltratie van het versleutelde Encrochat-netwerk, het gebruik van gezichtsherkenningssoftware zoals Clearview AI door de federale politie in België, de dataverzamelingspraktijken van Europol en het gebruik van de spionagesoftware Pegasus om data te verzamelen van mobiele telefoons wereldwijd.

Ten derde is er een toenemende impact van collectieve en sociale schade, naast individuele schade. Een belangrijk kenmerk van big data-analyses is dat ze op geaggregeerd niveau plaatsvinden.

Op het eerste gezicht lijkt het erop dat er geen persoonlijke gegevens worden verwerkt. Een van de gevolgen hiervan is dat sociale ongelijkheid toeneemt, waarbij bepaalde maatschappelijke groepen onevenredig worden beïnvloed. Omdat big data onregelmatigheden en afwijkingen in datasets reproduceert, kunnen bepaalde groepen of gemeenschappen onevenredig worden benadeeld. Dit kan leiden tot discriminatie en oneerlijke behandeling van kwetsbare groepen in de samenleving, omdat ze vaak het doelwit zijn van deze technologieën. Een duidelijk voorbeeld hiervan is predictive policing, waarbij feedback loops ervoor zorgen dat de politie herhaaldelijk naar dezelfde wijken wordt gestuurd, ongeacht het werkelijke misdaadcijfer. Dit leidt tot overmatig politietoezicht en stigmatisering van bepaalde wijken en gemeenschappen die al onder druk staan.

Deze risico's van discriminatie en stigmatisering door het gebruik van big data-analyses worden ook bevestigd in een uitspraak in Nederland over het gebruik van SyRI, een algoritmisch systeem dat sociale fraude opspoort. Deze uitspraak toont ook aan hoe big data-technologie sociale gevolgen heeft, zoals het criminaliseren van armoede en het bijdragen aan ongelijkheid in de samenleving.

In deze bijdrage heb ik drie ontwikkelingen besproken die de huidige controlemechanismen onder druk zetten. Ik zal nu reflecteren op de vraag of de huidige controlemechanismen kunnen omgaan met deze ontwikkelingen, vanuit het perspectief van relationele ethiek. Het is de vraag of het bestaande juridische kader voldoende is om met deze drie ontwikkelingen om te gaan en effectieve democratische waarborgen te bieden. Het juridische kader is momenteel gebaseerd op regels met betrekking tot gegevensbescherming. De controle-instrumenten die momenteel worden gebruikt voor gegevensverwerking met behulp van AI, zoals toezichtsorganen, functionarissen voor gegevensbescherming en gegevensbeschermingseffectbeoordelingen (DPIA's), zijn vaak beperkt in hun reikwijdte. De nadruk ligt voornamelijk op informatieveiligheid en formele naleving van de wet.

Er wordt echter niet genoeg nadruk gelegd op het beschermen van fundamentele rechten, specifiek zoals vastgelegd in artikel 8 van het Europees Verdrag voor de Rechten van de Mens. De manier waarop deze instrumenten in België functioneren, is ook weinig democratisch, omdat burgers en maatschappelijke organisaties er niet bij worden betrokken. Bovendien is de politie niet verplicht om gegevensbeschermingseffectbeoordelingen (DPIA's) openbaar te maken volgens de richtlijnen van de politie en justitie. Dit bemoeilijkt publieke controle. Er zijn ook geen standaarden waar DPIA's aan moeten voldoen, en er zijn geen standaardprofielen voor functionarissen die verantwoordelijk zijn voor gegevensbescherming.

Het huidige wettelijke kader is alleen van toepassing op toepassingen van algoritmische surveillance die persoonlijke gegevens verzamelen en verwerken. De Europese Unie heeft inmiddels een voorstel voor een AI-wetgeving gepubliceerd, met als doel de bescherming van individuele grondrechten tegen de negatieve gevolgen van AI en het harmoniseren van regelgeving binnen lidstaten om handelsbelemmeringen op de interne markt weg te nemen. De negatieve gevolgen van AI worden ingedeeld in risicocategorieën, van laag naar hoog, en de verordening behandelt zowel risico's voor individuen als risico's voor de samenleving. De verordening geeft echter niet duidelijk aan wat deze risico's precies inhouden.

Wat betreft controle- en handhavingsmechanismen is het voorstel van de verordening hoopgevend. Het vereist dat lidstaten één of meer nationale autoriteiten aanwijzen om toezicht te houden op de toepassing en uitvoering van AI en als officieel contactpunt fungeren voor het publiek en andere belanghebbenden. Het benadrukt ook dat handhavingsmechanismen versterkt kunnen worden door de invoering van een Europees coördinatiemechanisme dat audits van AI-systemen vergemakkelijkt en nieuwe eisen stelt op het gebied van documentatie, traceerbaarheid en transparantie. De verordening stelt ook dat er een systeem zal worden opgezet om hoogrisico autonome AI-toepassingen te registreren in een openbare databank voor de hele EU, en dat deze toepassingen alleen op de Europese markt mogen worden gebracht als ze voldoen aan bepaalde verplichte voorschriften en een conformiteitsbeoordeling hebben ondergaan. De concrete toepassing en handhaving van deze beoordelingen blijft echter vaag.
Het is niet duidelijk hoe de controlemechanismen er precies zullen uitzien. Bovendien is de verordening niet democratisch, omdat burgers en maatschappelijke organisaties niet betrokken worden bij deze mechanismen. Daarnaast zouden burgers ook geen klacht kunnen indienen bij de nationale toezichthoudende autoriteit als ze denken dat de wet niet wordt nageleefd. Hieronder reflecteer ik over wat we kunnen leren uit relationele ethiek, geïnspireerd door Ubuntufilosofie, om op een andere manier na te denken over controle in de praktijk van algoritmische politie, rekening houdend met de drie besproken technologische ontwikkelingen. Ubuntufilosofie vindt zijn oorsprong in de filosofie van landen ten zuiden van de Sahara in Afrika. Ubuntufilosofie verschilt van traditionele rationele ethiek, waarbij de menselijke waardigheid voortkomt uit autonomie, doordat het de menselijke waardigheid ziet als het vermogen om op een gezamenlijke manier met anderen om te gaan. Vanuit deze visie zijn schendingen van de mensenrechten gericht op het ernstig schaden van het vermogen van mensen om zich op een gemeenschappelijke manier tot anderen te verhouden. Verschillende computerwetenschappers, geïnspireerd door Ubuntufilosofie, stellen voor om op een andere manier te denken over onrechtvaardigheid en bestuur van AI, namelijk van rationele ethiek naar relationele ethiek. Relationele ethiek is een kader dat ons dwingt om onze aannames opnieuw te onderzoeken, machtsongelijkheden te bevragen en rekening te houden met de bredere context waarin algoritmische systemen ontstaan en worden ingezet bij het beschermen van de meest kwetsbaren. Deze visie gaat ervan uit dat de schade en onrechtvaardigheid veroorzaakt door algoritmische systemen niet los gezien kan worden van de filosofische principes van technologie en de economische, politieke en sociale structuren die het mede vormgeven. Hoe kunnen we deze visie verenigen met de visie van de politie en justitie die steeds grotere surveillance en samenwerking met de private sector willen? Dit zou betekenen dat ook het politiewerk vanuit dezelfde ethiek zou moeten worden benaderd. Het zou betekenen dat de rol van de politie op een relationele manier moet worden heroverwogen, namelijk als bescherming van collectieve veiligheid. In het huidige beleid wordt veiligheid echter op een beperkte manier geïnterpreteerd, namelijk als bescherming tegen criminaliteit en handhaving van de openbare orde. Vaak draait het zelfs niet meer om veiligheid, maar om politieke motieven, om te laten zien dat er streng wordt opgetreden tegen criminaliteit. Het is een vorm van "surveillance theater". Vanuit een collectieve visie op veiligheid, die tot doel heeft de veiligheid van alle burgers te waarborgen, moet er meer aandacht worden besteed aan andere oorzaken van onveiligheid. Veiligheid omvat meer dan alleen bescherming tegen criminaliteit: gezond voedsel, schoon water, huisvesting, basisinkomen, gezondheidszorg, onderwijs en werk, maar ook het niet het slachtoffer zijn van discriminatie, pesten, haat, geweld en excessieve controle door de overheid. Deze sociale en economische rechten worden vaak niet meegenomen in het veiligheidsbeleid. Vanuit deze visie wordt het bijvoorbeeld duidelijk dat encryptie essentieel is om mensenrechten en de meest kwetsbaren in de samenleving te beschermen. Het inbouwen van achterdeurtjes in technologie belemmert de veiligheid van bijvoorbeeld activisten en journalisten bij het uitoefenen van democratische controle. Aangezien het onwaarschijnlijk is dat veiligheid in het huidige politieke klimaat wordt gezien als sociale veiligheid, moeten we ons afvragen hoe we de mazen van het net kunnen verfijnen, zodat controlemechanismen ervoor zorgen dat de meest kwetsbaren in de samenleving beschermd worden. Als we kijken naar controle- en handhavingsmechanismen voor algoritmische surveillance vanuit het perspectief van relationele ethiek, blijkt dat het "rationele" controlekader, gebaseerd op gegevensbescherming, tekortschiet, vooral in de praktische toepassing en nationale wetgeving. Het rationele kader gaat ervan uit dat betrokkenen mondig zijn en individueel hun rechten kunnen beschermen door middel van informatieverzoeken, zonder rekening te houden met kwetsbare groepen. Niet alle betrokkenen zijn gelijk. Ze hebben verschillende inzichten, kennisniveaus, besluitvaardigheid, bereidheid om hun gegevens bekend te maken en individuele kwetsbaarheden. Factoren zoals leeftijd, mentale capaciteit, armoederisico, geletterdheid en geslacht kunnen van invloed zijn op het genot en de uitoefening van individuele rechten met betrekking tot gegevensbescherming. Controle moet verder gaan dan alleen technische oplossingen en het naleven van de wet. Het moet rekening houden met de historische context en de praktijken waarin technologie wordt gebruikt. Het moet ook aandacht hebben voor de machtsverhoudingen tussen verschillende betrokkenen en zich richten op het beschermen van de meest kwetsbaren in de samenleving. Deze vorm van controle betrekt de belangen van kwetsbare groepen en hun vertegenwoordigers bij het beleid en de controlemechanismen die worden gebruikt voor algoritmische surveillance. Daarnaast is transparantie belangrijk om te voorkomen dat vooroordelen en fouten leiden tot schendingen van de mensenrechten. Op dit moment weten mensen in België vaak niet welke beslissingen de overheid neemt op basis van algoritmen, en het is ook niet altijd duidelijk hoe persoonsgegevens worden verwerkt. Om met deze zaken rekening te houden, moeten controlemechanismen worden heroverwogen. Hoe kunnen ze rekening houden met onevenwichtige machtsverhoudingen en de groeiende macht van technologiebedrijven? Hoe kunnen ze collectieve en sociale schade voorkomen? Voordat de politie besluit te investeren in een bepaalde technologie, moet er een democratische en op bewijs gebaseerde proportionaliteitstoets worden uitgevoerd. Dit houdt in dat burgers betrokken worden bij de besluitvorming en dat er aandacht is voor de groeiende macht van de overheid en private partners, evenals voor collectieve en sociale schade. Deze toets moet gebaseerd zijn op wetenschappelijke en objectieve analyse. Een onafhankelijk orgaan zoals de Nederlandse Raad voor Regeringsbeleid (WRR) zou een rol kunnen spelen bij het uitvoeren van beleidsgericht onderzoek in samenwerking met universiteiten en maatschappelijke organisaties. Dit orgaan zou onderzoek kunnen doen naar de collectieve en sociale schade van algoritmische surveillance en naar innovatieve controle- en handhavingsmechanismen. Daarnaast kan worden gedacht aan de oprichting van een AI-coördinatiecentrum, zoals voorgesteld in het recente WRR-rapport, dat beleidsdirecties, toezichthouders en uitvoeringsorganisaties de mogelijkheid biedt om regelmatig met elkaar in contact te treden en van elkaar te leren. Dit centrum moet politiek verankerd zijn, zodat snel beleid kan worden ontwikkeld indien nodig. Vooral bij grootschalige surveillance door de politie is het belangrijk om de bevolking te betrekken bij besluitvorming om de legitimiteit ervan te waarborgen.Volgens een recent rapport van de WRR over kunstmatige intelligentie (AI) is het steeds belangrijker om het debat te voeren over welke doelen de samenleving wil bereiken en hoe, waarvoor en onder welke voorwaarden AI gebruikt moet worden. Een manier om dit te doen is bijvoorbeeld door het organiseren van openbare discussies, het raadplegen van het publiek, burgerjury's en het ondersteunen van burgerwetenschapsinitiatieven. Door het publiek actief te betrekken bij dit proces, kan de overheid leren van de kennis en ervaring van burgers. Het is echter van cruciaal belang dat kwetsbare groepen en gemeenschappen een echte stem krijgen in besluitvormingsprocessen en dat dit niet alleen voor de show is. Samenvattend biedt relationele controle interessante mogelijkheden om bestaande controlemechanismen te heroverwegen en rekening te houden met de sociale en technische ontwikkelingen die eerder in deze tekst zijn besproken. In deze tekst heb ik nagedacht over de vraag of de huidige controle- en handhavingsmechanismen voor algoritmische surveillance heroverwogen moeten worden. Eerst heb ik drie ontwikkelingen besproken die de bestaande controlemechanismen onder druk zetten. Vervolgens heb ik gekeken naar welke lessen we kunnen trekken als we controle- en handhavingsmechanismen voor algoritmische surveillance bekijken vanuit een relationeel perspectief. Een voorlopig antwoord op de vraag is dat de drie genoemde ontwikkelingen laten zien dat het huidige kader niet voldoende is om hiermee om te gaan. Deze eerste verkenning van relationele controle laat zien dat de "rationele" controlemechanismen tekortschieten. Het relationele perspectief biedt interessante mogelijkheden om verder na te denken over de gestelde vraag. Het antwoord blijft voorlopig echter onzeker, omdat verder onderzoek nodig is om hier beter inzicht in te krijgen.