Sinds het einde van de 20ste eeuw zijn er nieuwe mogelijkheden ontstaan om data te verzamelen en analyseren door technologische ontwikkelingen.
De politie heeft veel interesse getoond in de opkomst van 'big data' en de groeiende mogelijkheden van AI in de 21ste eeuw.
Het gebruik van deze technologieën door de politie wordt algoritmische surveillance genoemd. Het omvat algoritmische systemen die gegevens classificeren, opslaan, combineren en doorzoeken met behulp van regels en machine learning-algoritmen om patronen en bruikbare kennis in grote datasets te voorspellen.
Ondanks toegenomen regelgeving lijkt het gebruik van algoritmische surveillance eerder gestimuleerd te worden dan beperkt.
Dit roept de vraag op of de huidige controlemechanismen voldoende zijn om alle burgers te beschermen tegen de mogelijke gevolgen van algoritmische surveillance door de politie. Het doel van deze bijdrage is om te onderzoeken of de huidige controle- en handhavingsmechanismen voor het gebruik van algoritmische surveillance door de politie herzien moeten worden. In het eerste deel van het artikel worden drie socio-technische ontwikkelingen besproken die het huidige kader onder druk zetten.
In het tweede deel wordt gekeken naar de huidige controle- en handhavingsmechanismen vanuit de relationele ethiek om te verkennen hoe we kunnen leren en de controlemechanismen kunnen heroverwegen. De opkomst van algoritmische surveillance in het politiewerk zorgt voor drie sociotechnische ontwikkelingen die het traditionele controle- en handhavingskader onder druk zetten: 1) fragmentatie en privatisering van politiewerk, 2) democratisering van surveillance en 3) toename van collectieve schade en sociale gevolgen. Deze ontwikkelingen zijn met elkaar verweven en moeten als samenhangend worden gezien. Ten eerste is er sinds het einde van de 20ste eeuw een toename van samenwerking met de private sector in het politiewerk, als gevolg van de groeiende macht van technologiebedrijven en bezuinigingen in de publieke sector. Dit heeft geleid tot de afhankelijkheid van technologiebedrijven en verschuiving van machtsverhoudingen naar de private sector, wat de transparantie en controle negatief beïnvloedt. Politiewerk wordt steeds meer platform policing, waarbij digitale platformen en opsporingstechnologieën worden gebruikt. Ten tweede is er sprake van een 'democratisering' van surveillance, waarbij een groter deel van de bevolking onder surveillance komt te staan. Politiediensten en technologiebedrijven spelen hierin een grotere rol, wat resulteert in een toenemend risico van machtsmisbruik door zowel de staat als private actoren. Voorbeelden hiervan zijn het gebruik van 'intelligent' cameratoezicht voor diverse doeleinden en de uitbreiding van grootschalige surveillancepraktijken. Ten derde is er een groeiende trend van collectieve en sociale schade naast individuele schade. Big data-analyses vinden plaats op geaggregeerd niveau, waardoor het op het eerste gezicht lijkt dat er geen persoonsgegevens worden verwerkt. Een gevolg hiervan is sociale ongelijkheid, waarbij bepaalde groepen of gemeenschappen onevenredig worden getroffen door de reproduceerbaarheid van onregelmatigheden en afwijkingen in big data-sets. Kwetsbare groepen zijn vaak het doelwit van deze technologieën, wat leidt tot cumulatieve nadelen, discriminatie en oneerlijke behandeling. Dit is bijvoorbeeld duidelijk zichtbaar bij predictive policing, waar feedback loops en steekproefbias ervoor zorgen dat de politie steeds terugkeert naar dezelfde wijken, ongeacht de werkelijke misdaadcijfers. Dit resulteert in overmatig politietoezicht en stigmatisering van bepaalde wijken en gemeenschappen. Dit risico op discriminatie en stigmatisering wordt ook bevestigd in een uitspraak in Nederland over het gebruik van SyRI, een algoritme voor het opsporen van sociale fraude. In het vervolg van deze bijdrage reflecteer ik op de vraag of de huidige controlemechanismen kunnen omgaan met deze ontwikkelingen, vanuit het perspectief van relationele ethiek. Sociotechnische ontwikkelingen zetten traditionele controle- en handhavingsmechanismen onder druk, en het is de vraag of het huidige juridische kader voldoende democratische waarborgen biedt. Hoewel er regels zijn voor gegevensbescherming, zoals toezichtsorganen, functionarissen voor gegevensbescherming en gegevensbeschermingseffectbeoordelingen (DPIA's), zijn deze instrumenten vaak beperkt in hun reikwijdte. Ze leggen de nadruk op informatieveiligheid en formele naleving, maar beschermen fundamentele rechten, met name volgens artikel 8 van het EVRM, krijgen niet voldoende aandacht. Het democratische karakter van deze instrumenten is ook beperkt, omdat burgers en het maatschappelijk middenveld niet worden betrokken. Daarnaast is de politie niet verplicht om DPIA's openbaar te maken en ontbreken standaarden en profielen voor functionarissen voor gegevensbescherming, wat de publieke controle bemoeilijkt. Het huidige wettelijke kader is beperkt tot de toepassingen van algoritmische surveillance die persoonsgegevens verzamelen en verwerken. De EU heeft echter een voorstel voor een AI-wet gepubliceerd, met als doel de bescherming van grondrechten tegen de negatieve gevolgen van AI en het harmoniseren van regelgeving in lidstaten. De wet verdeelt de nadelige gevolgen van AI in risicocategorieën en benoemt zowel individuele als maatschappelijke risico's, hoewel het niet specifiek aangeeft wat deze risico's inhouden. Wat betreft controle- en handhavingsmechanismen is het voorstel hoopgevend. Lidstaten moeten nationale autoriteiten aanstellen voor toezicht op AI-toepassingen en als contactpunt fungeren. Ook wordt gesuggereerd dat handhavingsmechanismen kunnen worden versterkt door middel van een Europees coördinatiemechanisme en audits van AI-systemen met vereisten voor documentatie, traceerbaarheid en transparantie. Er wordt ook een systeem voorgesteld om hoogrisico autonome AI-toepassingen te registreren in een openbare EU-databank en ze moeten voldoen aan dwingende voorschriften en conformiteitsbeoordelingen ondergaan voordat ze op de Europese markt worden toegelaten. Niettemin blijven er onduidelijkheden over de concrete toepassing en handhaving van deze beoordelingen en voldoen de mechanismen niet aan democratische normen doordat burgers en het maatschappelijk middenveld er niet bij betrokken worden. Bovendien hebben burgers geen mogelijkheid om een klacht in te dienen bij de nationale toezichthoudende autoriteit als zij van mening zijn dat de wet niet wordt nageleefd. Hieronder reflecteer ik op wat we kunnen leren van relationele ethiek, geïnspireerd door de Ubuntufilosofie, om op een andere manier na te denken over controle in de algoritmische politiepraktijk, rekening houdend met de besproken sociotechnische ontwikkelingen. Ubuntufilosofie vindt zijn oorsprong in Afrikaanse filosofie uit landen ten zuiden van de Sahara. Ubuntufilosofie verschilt van traditionele rationele ethiek doordat het de menselijke waardigheid niet baseert op autonomie, zoals in Kantiaanse ethiek, maar op het vermogen van mensen om op een gemeenschappelijke manier met anderen om te gaan. Mensenrechtenschendingen worden gezien als schadelijk voor het vermogen van mensen om gemeenschappelijke relaties, identiteit en solidariteit aan te gaan. Daarom moet menselijke waardigheid worden begrepen als het vermogen van mensen om op een gemeenschappelijke manier met anderen om te gaan. Sommige computerwetenschappers, geïnspireerd door Ubuntufilosofie, stellen voor om een fundamentele verschuiving te maken van rationele ethiek naar relationele ethiek in het denken over algoritmische onrechtvaardigheid en AI-bestuur. Relationele ethiek dwingt ons om onze aannames opnieuw te onderzoeken, machtsonevenwichtigheden te bevragen en rekening te houden met de bredere, onderling verbonden achtergrond waarin algoritmische systemen ontstaan en worden ingezet om het welzijn van de meest kwetsbaren te beschermen. Deze visie impliceert dat schade en onrechtvaardigheid veroorzaakt door algoritmische systemen niet los kunnen worden gezien van de filosofische principes van technologie en de economische, politieke en sociale structuren die het mede vormgeven. Hoe kan deze visie worden verzoend met een politie- en justitie-apparaat dat steeds meer surveillance en samenwerking met de private sector vraagt? Dit zou betekenen dat ook het politiewerk moet vertrekken vanuit dezelfde ethiek, waarbij de focus ligt op het beschermen van collectieve veiligheid. Echter, het huidige beleid interpreteert veiligheid vaak op een beperkte manier, gericht op criminaliteitsbestrijding en handhaving van de openbare orde. Soms draait het zelfs meer om politieke motieven en het creëren van de illusie van hard optreden tegen criminaliteit, als een vorm van "surveillance theater". Vanuit een collectieve visie op veiligheid, die tot doel heeft de veiligheid van alle burgers te waarborgen, is het belangrijk om aandacht te besteden aan andere oorzaken van onveiligheid. Veiligheid omvat meer dan alleen bescherming tegen criminaliteit: het omvat ook aspecten zoals gezond eten, schoon water, huisvesting, basisinkomen, gezondheidszorg, onderwijs en werk, evenals het vermijden van discriminatie, pesten, haat, geweld en excessieve overheidscontrole. Helaas worden deze sociale en economische rechten vaak niet meegenomen in het veiligheidsbeleid. Als we deze visie hanteren, wordt het duidelijk dat encryptie essentieel is om mensenrechten en de meest kwetsbare mensen in de samenleving te beschermen. Het plaatsen van achterdeurtjes in technologie belemmert bijvoorbeeld de veiligheid van activisten en journalisten die democratische controle uitoefenen. Gezien het huidige politieke klimaat is het echter onwaarschijnlijk dat veiligheid als sociale veiligheid wordt beschouwd. Daarom moeten we nadenken over manieren om controlemechanismen te verfijnen, zodat de meest kwetsbare mensen in de samenleving beschermd worden. Als we kijken naar controle- en handhavingsmechanismen voor algoritmische surveillance vanuit het perspectief van relationele ethiek, zien we dat het "rationele" controlekader, dat gebaseerd is op gegevensbescherming, tekort schiet. Met name de manier waarop dit in de praktijk en in nationale politiewetgeving wordt vertaald, is problematisch. Het rationele kader gaat uit van mondige individuen die hun rechten individueel kunnen beschermen door middel van informatieverzoeken, maar houdt geen rekening met kwetsbare groepen. Niet alle betrokkenen zijn gelijk. Ze hebben verschillende inzichten, kennisniveaus, besluitvaardigheid, bereidheid om hun gegevens bekend te maken en individuele kwetsbaarheden. Factoren zoals leeftijd, mentale capaciteit, armoederisico, geletterdheid en geslacht kunnen invloed hebben op het genot en de uitoefening van individuele rechten met betrekking tot gegevensbescherming. Daarom moet controle verder gaan dan louter statische technische oplossingen en formele naleving van de wet. Het moet een praktijk zijn die rekening houdt met de dynamische historische context en sociaal-technische praktijken waarin technologie is ingebed. Het moet aandacht hebben voor machtsverhoudingen tussen verschillende betrokken actoren en zich richten op de bescherming van de meest kwetsbare mensen in de samenleving. De relationele controle vereist dat de belangen van de meest kwetsbaren en hun vertegenwoordigers worden betrokken bij het beleid en de controlemechanismen die gebaseerd zijn op het sociaal-technische proces van algoritmische surveillance. Transparantie is ook essentieel om te voorkomen dat vooroordelen en fouten leiden tot schendingen van mensenrechten, zoals aangegeven door het Federaal Instituut voor de bescherming en bevordering van de rechten van de mens (FIRM). Op dit moment weten mensen in België vaak niet welke beslissingen de overheid neemt met behulp van algoritmen, en het is ook niet altijd duidelijk hoe persoonsgegevens worden verwerkt. Om hiermee rekening te houden, moeten controlemechanismen worden heroverwogen. Ze moeten asymmetrische machtsrelaties en de toenemende macht van technologiebedrijven in overweging nemen en collectieve en sociale schade voorkomen. Voordat er geïnvesteerd wordt in het ontwerpen van een bepaalde technologie door de politie, moet er een democratische evidence-based proportionaliteitstoets worden uitgevoerd, waarbij burgers betrokken worden bij de besluitvorming. Deze toets moet ook rekening houden met de groeiende macht van de staat en private partners, evenals collectieve en sociale schade. Wetenschappelijke en objectieve analyse moeten de basis vormen voor deze toets. Een onafhankelijk orgaan zoals de Nederlandse Raad voor Regeringsbeleid (WRR) kan een rol spelen door beleidsgericht onderzoek te verrichten in samenwerking met universiteiten en maatschappelijke organisaties. Dit orgaan kan bijvoorbeeld onderzoek doen naar de collectieve en sociale schade van algoritmische surveillance en naar innovatieve controle- en handhavingsmechanismen. Daarnaast kan gedacht worden aan de oprichting van een AI-coördinatiecentrum, zoals voorgesteld in het recente WRR-rapport, dat beleidsdirecties, toezichthouders en uitvoeringsorganisaties een structuur biedt om regelmatig met elkaar in contact te treden en van elkaar te leren. Dit centrum moet politiek verankerd zijn, zodat snel beleid kan worden ontwikkeld indien nodig. Met name bij grootschalige surveillance door politiediensten is het belangrijk om de bevolking te betrekken bij besluitvorming om de legitimiteit te behouden. Volgens een recent WRR-rapport zal er steeds meer debat plaatsvinden over de doelen en het gebruik van AI in de samenleving. Methoden zoals publieke debatten, openbare raadplegingen, burgerjury's en citizen-science initiatieven kunnen hiervoor worden gebruikt. Het betrekken van het publiek als actieve deelnemers stelt de overheid in staat om te leren van de expertise van burgers. Het is essentieel dat kwetsbare groepen en gemeenschappen een significante stem hebben in besluitvormingsprocessen, zonder dat dit slechts een symbolische geste is. Relationele controle biedt interessante mogelijkheden om de huidige controlemechanismen opnieuw te bekijken, rekening houdend met de sociaal-technische ontwikkelingen die in het begin van deze bijdrage zijn beschreven. In deze bijdrage is gereflecteerd over de vraag of de bestaande controle- en handhavingsmechanismen voor algoritmische surveillance heroverwogen moeten worden. Eerst zijn drie sociotechnische ontwikkelingen besproken die druk uitoefenen op de huidige controlemechanismen. Vervolgens is gekeken naar welke lessen kunnen worden getrokken vanuit de relationele ethiek bij het bekijken van controle- en handhavingsmechanismen voor algoritmische surveillance. Voorlopig geeft het antwoord op de vraag aan dat het huidige kader niet voldoende is om deze ontwikkelingen op te vangen. Deze eerste verkenning van relationele ethiek om op een andere manier na te denken over controle en handhaving van algoritmische surveillance door de politie, laat zien dat 'rationele' controlemechanismen ontoereikend zijn. Het relationele kader biedt interessante mogelijkheden om verder na te denken over de gestelde vraag. Het antwoord in deze bijdrage blijft echter voorlopig, omdat verder (empirisch) onderzoek nodig is om hier beter inzicht in te krijgen.