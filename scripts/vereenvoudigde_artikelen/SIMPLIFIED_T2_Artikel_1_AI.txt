Sinds het einde van de 20e eeuw hebben technologische ontwikkelingen nieuwe mogelijkheden gecreëerd voor het verzamelen en analyseren van data.De opkomst van 'big data' en de toegenomen mogelijkheden van kunstmatige intelligentie (AI) in de 21e eeuw worden door de politie met grote belangstelling omarmd.
Het gebruik van deze technologieën door de politie kan worden omschreven als algoritmische surveillance.Algoritmische systemen zijn algoritmische systemen die 1. op regels gebaseerde algoritmen gebruiken om gestructureerde en ongestructureerde gegevens te classificeren, op te slaan, te combineren en te doorzoeken, vastgelegde gegevens met andere gegevens te vergelijken en overeenkomsten te vinden; en 2 .In 2. 1 2. 2 .Algoritmen voor machinaal leren gebruiken om te proberen patronen en bruikbare kennis in big data-sets te voorspellen op basis van de patronen die in de vastgelegde gegevens worden gevonden.Ondanks de toegenomen regulering3, die de democratische waarborgen van algoritmische surveillance wil waarborgen, lijkt dit eerder het gebruik ervan te stimuleren.Denk bijvoorbeeld aan de forse toename van het gebruik van 'intelligent' cameratoezicht in België, maar ook elders in Europa.Dit roept de vraag op of de huidige controlemechanismen voldoende zijn om alle burgers te beschermen tegen de mogelijke gevolgen van het gebruik van algoritmisch toezicht door de politie.Het doel van deze bijdrage is na te denken over de vraag of de huidige controle- en handhavingsmechanismen voor het gebruik van algoritmisch toezicht door de politie moeten worden heroverwogen.Om tot een (voorlopig) antwoord op die vraag te komen, bespreek ik in het eerste deel van het artikel drie socio-technische ontwikkelingen die het huidige kader onder druk zetten.In het tweede deel zal ik kijken naar de huidige mechanismen voor het afdwingen van controle door de lens van relationele ethiek om te onderzoeken hoe we hiervan kunnen leren om controlemechanismen te heroverwegen.Als gevolg van de opkomst van algoritmisch toezicht in de politie zijn drie socio-technische ontwikkelingen te onderscheiden die het traditionele controle- en handhavingskader onder druk zetten: 1)de versnippering en privatisering van de politie, 2) de democratisering van het toezicht, en 3) de toename van collectieve schade en sociale gevolgen.Deze ontwikkelingen overlappen elkaar en zijn met elkaar verweven en moeten niet als afzonderlijke ontwikkelingen worden gezien.Ten eerste is de versnippering en privatisering van de politie niet nieuw.Sinds het einde van de 20e eeuw wordt er steeds meer samengewerkt met de private sector in het Westen en spelen private spelers een steeds belangrijkere rol in de politie.Dit komt grotendeels door de toegenomen macht en groei van de private sector en bezuinigingen in de publieke sector.6De technologische ontwikkelingen van big data en AI in het begin van de 21e eeuw hebben geleid tot een toenemende macht van technologiebedrijven door middel van surveillancekapitalisme, waarbij dataverzameling een economische motor wordt voor bedrijven.Politiewerk wordt steeds meer platform policing, waarbij de politie gebruik maakt van digitale platformen en digitale opsporingstechnologie.8Hierdoor wordt de politie in toenemende mate afhankelijk van infrastructuur van technologiebedrijven9 en leidt dit tot een verschuiving van machtsverhoudingen van de publieke naar de private sector, wat een negatieve invloed heeft op de transparantie en controle.10 Ten tweede kan een 'democratisering' van het toezicht herkenbaar aan een verschuiving van focus op gericht toezicht naar grootschalig toezichtAls gevolg hiervan wordt nu een veel groter deel van de bevolking in de gaten gehouden, waardoor het risico op een toename van de macht van de staat maar ook van particuliere actoren groter wordt.Waar grootschalige surveillancepraktijken vroeger vooral door inlichtingendiensten of binnen een specifieke context zoals op de luchthaven werden uitgevoerd, spelen politiediensten hierin een steeds belangrijkere rol.Denk in België bijvoorbeeld aan de forse uitbreiding van het gebruik van 'intelligente' camerabewaking voor allerlei doeleinden13, die nog werd aangewakkerd door de coronapandemie.14 Andere voorbeelden zijn de infiltratie van het versleutelde Encrochat-netwerk15, de gezichtsherkenning van Clearview AI software die ook door de federale politie in België is getest16 , of de dataverzamelingspraktijken van Europol17 die door sommigen worden vergeleken met surveillancepraktijken van de Amerikaanse NSA.18 Denk ten slotte aan het gebruik door de spionagesoftware Pegasus om gegevens van de mobiele telefoon te verzamelen telefoons van activisten, politici en journalisten overalEen belangrijk kenmerk van big data-analyse is dat het op geaggregeerd niveau plaatsvindt.Op het eerste gezicht worden er dus geen persoonsgegevens verwerkt.20Een van de gevolgen is een toename van sociale stratificatie, resulterend in een ongelijke verhouding tussen sociale groepen.Omdat big data onregelmatigheden en anomalieën in datasets reproduceert, kan het leiden tot uitkomsten die een onevenredige impact hebben op bepaalde groepen of gemeenschappen.Dit kan dan leiden tot een cumulatief nadeel (discriminatie en oneerlijke behandeling) van bepaalde groepen in de samenleving, omdat deze, vaak kwetsbare, groepen bovengemiddeld het doelwit zijn van deze technologieën.21Dit is bijvoorbeeld duidelijk terug te zien in predictive policing.Als gevolg van terugkoppelingslussen die zijn ontstaan ​​door bemonsteringsbias, wordt de politie herhaaldelijk teruggestuurd naar dezelfde buurten, ongeacht de werkelijke misdaadcijfers.22Dit leidt tot overpolitie en stigmatisering van bepaalde buurten en gemeenschappen die al het doelwit zijn.23 Deze risico's worden ook bevestigd in de Nederlandse uitspraak over het gebruik van SyRI, een algoritmisch systeem voor het opsporen van sociale fraude. detecteren.Daarnaast laat de uitspraak ook zien hoe big data technologie kan worden ingezetheeft sociale gevolgen en draagt ​​naast discriminatie en stigmatisering ook bij aan de strafbaarstelling van armoede en kansarmoede en aan het vergroten van de ongelijkheid in de samenlevingHierbovenIk heb drie socio-technische ontwikkelingen beschreven die de huidige controlemechanismen onder druk zetten.In het verdere verloop van deze bijdrage sta ik stil bij de vraag of de huidige controlemechanismen deze ontwikkelingen aankunnen.Ik doe dit vanuit de lens van relationele ethiek.Zoals deze bijdrage duidelijk maakt, zetten sociaal-technische ontwikkelingen de traditionele controle- en handhavingsmechanismen onder druk.De vraag rijst of het huidige wettelijke kader volstaat om deze drie ontwikkelingen het hoofd te bieden en effectieve democratische waarborgen te bieden.Het wettelijk kader wordt tegenwoordig gevormd door de regels inzake gegevensbescherming.De controlemiddelen die momenteel worden gebruikt voor de verwerking van gegevens door middel van AI, zoals toezichthouders, functionarissen voor de gegevensbescherming en Data Protection Impact Assessments (DPIA's), zijn vaak beperkt in omvang.De focus ligt grotendeels op informatiebeveiliging en formele naleving van het wettelijk kader.Anderzijds wordt er te weinig nadruk gelegd op de bescherming van grondrechten en meer bepaald op basis van artikel 8 EVRM.Bovendien is de manier waarop deze instrumenten in België werken niet erg democratisch, omdat burgers en het maatschappelijk middenveld er niet bij betrokken zijn.Daarnaast is de politie niet verplicht om DPIA’s te publiceren conform de Richtlijn Politie en Justitie.Dit maakt publieke controle moeilijker. Het maakt ook openbaarEr zijn ook geen normen waaraan DPIA’s moeten voldoen.Er zijn ook geen standaardprofielen voor functionarissen voor gegevensbescherming.Het huidige wettelijk kader heeft enkel betrekking op toepassingen van algoritmische surveillance die 'persoonsgegevens' verzamelen en verwerken.26De EU heeft ondertussen een voorstel voor een AI-wet gepubliceerd met een tweeledig doel:de bescherming van de grondrechten van het individu tegen de nadelige effecten van AI, evenals de harmonisatie van de regelgeving van de lidstaten om potentiële handelsbelemmeringen op de interne markt weg te nemen.De nadelige gevolgen van AI zijn onderverdeeld in risicocategorieën van laag naar hoog en naast risico's voor het individu spreekt de verordening ook over risico's voor de samenleving.De regeling maakt echter niet duidelijk wat die risico's nu eigenlijk zijn.27 De voorgestelde regeling is hoopvol wat betreft controle- en handhavingsmechanismen.Het geeft aan dat de lidstaten een of meer nationale bevoegde autoriteiten moeten aanwijzen om toezicht te houden op de toepassing en implementatie van AI en op te treden als officieel contactpunt voor het publiek en andere actoren.Ook wordt benadrukt dat handhavingsmechanismen kunnen worden versterkt “door een Europees coördinatiemechanisme in te voeren dat de juiste capaciteit biedt en audits van de AI-systemen vergemakkelijkt met nieuwe documentatie-, traceerbaarheids- en transparantievereisten”.28 De verordening geeft ook aan dat er een systeem zal worden opgezet om autonome AI-toepassingen met een hoog risico te registreren in een openbare EU-brede database en dat ze alleen op de Europese markt worden toegelaten als ze voldoen aan "bepaalde verplichte vereisten en een voorafgaande conformiteitsbeoordeling ondergaan"De wijze waarop deze beoordelingen in de praktijk zullen worden toegepast en gehandhaafd, blijft echter vaag.Het is onduidelijk hoe de nalevingsmechanismen eruit zullen zien.Ook op democratisch vlak schiet de regeling tekort, omdat burgers of het maatschappelijk middenveld niet betrokken zijn bij deze mechanismen.Bovendien zouden burgers geen klacht kunnen indienen bij de nationale toezichthoudende autoriteit als zij menen dat de wet niet wordt nageleefd.We moeten anders gaan denken over controle in algoritmische politie, rekening houdend met de drie besproken sociaal-technische ontwikkelingen. Hieronder reflecteer ik op wat we kunnen leren van relationele ethiek, geïnspireerd door de Ubuntu-filosofie, om anders te denken over controle enDe Ubuntu-filosofie vindt zijn oorsprong in de Afrikaanse filosofie ten zuiden van de Sahara.31 De Ubuntu-filosofie verschilt van de traditionele rationele ethiek in die zin dat, in tegenstelling tot de rationele Kantiaanse ethiek, waar personen menselijke waardigheid hebben door hun vermogen tot autonomie, personen die menselijke waardigheid zijn omdat ze het vermogen hebben op een collectieve manier met elkaar omgaan.32 Vanuit deze visie zijn mensenrechtenschendingen gericht op het ernstig aantasten van het vermogen van mensen tot gemeenschappelijke relaties, opgevat als identiteit en solidariteit; en menselijke waardigheid moet worden gezien als het menselijk vermogen om op een gemeenschappelijke manier met anderen in contact te komen.Verschillende computerwetenschappers, die zich laten inspireren door de Ubuntu-filosofie, stellen een fundamentele verschuiving voor in het denken over algoritmische onrechtvaardigheid en het beheer van AI van rationele ethiek naar relationele ethiek.33 Volgens Birhane is relationele ethiek “een raamwerk dat ons dwingt om onze onderliggende werkhypothesen te heroverwegen. . dwingt ons (en worden ingezet) om rekening te houden met de bredere, contingente en onderling verbonden achtergrond waaruit algoritmische systemen voortkomen en worden ingezet bij het beschermen van het welzijn van de meest kwetsbaren.Deze visie gaat ervan uit dat de schade en het onrecht dat door algoritmische systemen wordt toegebracht, niet los kan worden gezien van de filosofische principes van technologie en de economische, politieke en sociale structuren die haar mede vormgeven.Hoe is deze visie te rijmen met de visie van politie en justitie die steeds grootschaliger toezicht en samenwerking met het bedrijfsleven vereist?36Dit zou impliceren dat ook het politiewerk vanuit dezelfde ethiek zou moeten vertrekken.Het zou betekenen dat de politiemissie heroverwogen moet worden op een relationele manier, als het beschermen van de collectieve veiligheid.In het huidige beleid wordt veiligheid echter eng opgevat als bescherming tegen criminaliteit en handhaving van de openbare orde.Vaak gaat het niet eens meer om veiligheid, maar om politieke motieven, om te laten zien dat er serieus wordt opgetreden tegen criminaliteit.Het is een vorm van 'bewakingstheater'.37 Vanuit een collectieve visie op veiligheid die de veiligheid van alle burgers beoogt te waarborgen, zou meer aandacht moeten komen voor andere oorzaken van onveiligheid.Veiligheid is meer dan alleen bescherming tegen criminaliteit: gezonde voeding, schoon water, huisvesting, basisinkomen, gezondheidszorg, onderwijs en werk, maar bijvoorbeeld ook niet het onderwerp zijn van discriminatie, intimidatie, haat, geweld en onevenredige overheidscontrole.Deze sociale en economische rechten zijn vaak niet opgenomen in het veiligheidsbeleid.38Vanuit deze visie wordt duidelijk dat encryptie cruciaal is om de mensenrechten en de meest kwetsbaren in de samenleving te beschermen, omdat door het inbouwen van achterdeurtjes in de technologie de veiligheid van bijvoorbeeld activisten en journalisten om democratische controle uit te oefenen in het gedrang komt.belemmerd.39Aangezien het, zeker in het huidige politieke klimaat, onwaarschijnlijk is dat veiligheid wordt gezien als sociale zekerheid, moet de vraag worden gesteld hoe de mazen van het net kunnen worden verfijnd zodat controlemechanismen ervoor zorgen dat de meest kwetsbaren in de samenleving worden beschermd.Als we controle- en handhavingsmechanismen voor algoritmisch toezicht vanuit een relationeel-ethische invalshoek bekijken, betekent dit dat het 'rationele' controlekader dat is opgebouwd vanuit het paradigma van gegevensbescherming tekortschiet in de manier waarop het zich vertaalt naar de praktijk en de nationale politiewetgeving aan het worden is.Het rationele kader gaat uit van mondige betrokkenen die via informatieverzoeken individueel hun rechten kunnen beschermen, zonder rekening te houden met kwetsbare groepen.Niet alle betrokkenen zijn gelijk. Niet iedereen is gelijk.Ze hebben verschillende inzichten, kennisniveaus, daadkracht, neiging om hun gegevens te onthullen en individuele kwetsbaarheden.Factoren zoals leeftijd, geestelijke vermogens, deprivatie, geletterdheid of geslacht kunnen van invloed zijn op het genot en de uitoefening van individuele gegevensbeschermingsrechten.40 Controle moet daarom verder gaan dan louter statische technische oplossingen en formele naleving van de wet, naar een praktijk die rekening houdt met de dynamische historische context en socio-technische praktijken waarin de technologie is ingebed, aandacht heeft voor de machtsverhoudingen van de verschillende betrokken actoren, en waarin de bescherming van de meest kwetsbaren in de samenleving voorop staat.Deze relationele controle impliceert het betrekken van de (belangen van)de meest kwetsbaren en hun vertegenwoordigers zowel in beleid als in controlemechanismen die uitgaan van het socio-technische proces van algoritmische surveillanceDaarnaast is transparantie cruciaal om vooroordelen en fouten die leiden tot mensenrechtenschendingen te voorkomen, zoals aangegeven door het Federaal Instituut voor de bescherming en bevordering van de mensenrechten (FIRM).Volgens de FIRM weten mensen in België momenteel vaak niet voor welke beslissingen de overheid algoritmen gebruikt.Daarnaast is niet altijd duidelijk hoe een algoritme persoonsgegevens verwerkt.verwerkt.42 Concreet betekent dit dat moet worden nagedacht over hoe controlemechanismen kunnen worden herdacht om rekening te houden met het bovenstaandeHoe kunnen zij rekening houden met asymmetrische machtsverhoudingen en de toenemende macht van technologiebedrijven?Hoe kunnen zij collectieve en maatschappelijke schade voorkomen?Voordat een besluit wordt genomen om te investeren in (het ontwerp van) een bepaalde technologie door de politie, moet een democratische evidence-based proportionaliteitstoets worden uitgevoerd.Deze toets betrekt burgers bij de beslissingen.Daarnaast besteedt deze toets aandacht aan de toenemende macht van de staat en private partners, alsook aan collectieve en maatschappelijke schade.Deze test moet gebaseerd zijn op wetenschappelijke en objectieve analyse.Een orgaan als de Onafhankelijke Raad voor het Regeringsbeleid (WRR)43 zou hier bijvoorbeeld een rol kunnen spelen door in nauwe samenwerking met universiteiten en maatschappelijke organisaties beleidsgericht onderzoek te doen.Dit orgaan zou dan bijvoorbeeld ook onderzoek kunnen doen naar de collectieve en maatschappelijke schade van algoritmisch toezicht en naar innovatieve controle- en handhavingsmechanismen.Naast een AI-coördinatiecentrum, zoals het recente AI-rapport van de WRR voorstelt, zou een structuur om regelmatig contact met elkaar op te nemen en van elkaar te leren over uiteenlopende onderwerpen nuttig kunnen zijn voor de ontwikkeling en uitvoering van AI-beleid.Dit centrum dient politiek verankerd te zijn, zodat indien nodig snel beleid gemaakt kan wordenis.44Zeker wanneer het gaat over grootschalige surveillance door politiediensten zou de bevolking betrokken moeten worden bij beslissingen om hun legitimiteit te bewaren.45Volgens het recente WRR-rapport over AI zal steeds vaker debat nodig zijn over de doelen die de samenleving wil nastreven en de vraag waar, waarvoor en onder welke condities de samenleving AI wil gebruiken.Methoden die hiervoor gebruikt kunnen worden, zijn bijvoorbeeld het organiseren van publieke debatten, openbare raadplegingen46, burgerjury’s, maar ook bijvoorbeeld de ondersteuning van citizen-science initiatieven.47 Door het publiek te betrekken als actieve deelnemers aan het proces, kan de overheid leren van de expertise van burgers.48 Vanuit de relationele ethiek is het dan wel van essentieel belang dat kwetsbare groepen en gemeenschappen een significante stem krijgen in beslissingsmakingsprocessen en dat dit niet enkel ‘voor de show’ is.Samenvattend biedt relationele controle interessante pistes aan om huidige controlemechanismen te herdenken, op een manier die rekening houdt met de sociaal-technische ontwikkelingen beschreven in de aanvang van deze bijdrage.In deze bijdrage heb ik gereflecteerd over de vraag of huidige controle- en handhavingsmechanismen voor algoritmische surveillance herdacht zouden moeten worden.Eerst heb ik drie sociotechnische ontwikkelingen besproken die huidige controlemechanismen onder druk zetten.Nadien heb ik gekeken naar welke lessenwe kunnen trekken als we controle- en handhavingsmechanismen voor algoritmische surveillance bekijken vanuit de relationele ethiek.Een voorlopig antwoord op de vraag is dat de drie socio-technische ontwikkelingen aangeven dat het huidig kader niet volstaat om deze ontwikkelingen op te vangen.Deze eerste exploratie van relationele ethiek om op een andere manier na te denken over controle en handhaving van gebruik van algoritmische surveillance door de politie, geeft aan dat ‘rationele’ controlemechanismen tekortschieten.Het relationele kader biedt interessante pistes om verder over de vooropgestelde vraag na te denken.Het antwoord in deze bijdrage blijft evenwel voorlopig, omdat verder (empirisch) onderzoek noodzakelijk zal zijn om hier beter inzicht in te krijgen.Sinds het einde van de 20ste eeuw zijn er als gevolg van technologische ontwikkelingen nieuwe mogelijkheden ontstaan om data te verzamelen en te analyseren. 
De opkomst van ‘big data’ en de toegenomen mogelijkheden van artificiële intelligentie (AI) in de 21ste eeuw zijn met veel interesse omarmd door de politie.
Het gebruik van deze technologieën door de politie kan worden beschreven als algoritmische surveillance.Dit zijn algoritmische systemen die 1. gebruik maken van op regels gebaseerde algoritmen om gestructureerde en ongestructureerde gegevens te classificeren, op te slaan, te combineren en te doorzoeken, om vastgelegde gegevens te vergelijken met andere gegevens en overeenkomsten te vinden;en 2.gebruik maken van machine-lerende algoritmes om patronen en bruikbare kennis in big data sets trachten te voorspellen op basis van de patronen die in de vastgelegde gegevens zijn gevonden.Ondanks de toegenomen regelgeving3, die als doel heeft de democratische waarborgen te garanderen van algoritmische surveillance, lijkt dit het gebruik ervan eerder te stimuleren.Denk bijvoorbeeld aan de significante toename van het gebruik van ‘intelligent’ cameratoezicht in België, maar ook elders in Europa.5Dit roept de vraag op of de huidige controlemechanismen voldoende zijn om alle burgers te beschermen tegen de mogelijke gevolgen van het gebruik van algoritmische surveillance door de politie.Het doel van deze bijdrage is om na te denken over de vraag of huidige controle en handhavingsmechanismen voor het gebruik van algoritmische surveillance door de politie herdacht zouden moeten worden.Om tot een (voorlopig) antwoord op die vraag te komen zal ik in het eerste deel van het artikel drie socio-technische ontwikkelingen bespreken die het huidig kader onder druk zetten.In het tweede deel zal ik huidige controleren handhavingsmechanismen bekijken door de bril van de relationele ethiek om te exploreren hoe we hieruit kunnen leren om controlemechanismen te herdenken.Als gevolg van de opkomst van algoritmische surveillance in het politiewerk kunnen drie sociotechnische ontwikkelingen geïdentificeerd worden die het traditionele controle- en handhavingskader onder druk zetten: 1)de fragmentatie en privatisering van politiewerk, 2) de democratisering van surveillance, en 3) de toename van collectieve schade en sociale gevolgen.Deze ontwikkelingen zijn overlappend en verstrengeld en moeten niet als losstaande ontwikkelingen worden gezien.Ten eerste, fragmentatie en privatisering van politiewerk is niet nieuw.Sinds het einde van 20ste eeuw is er in het Westen een stijging van de samenwerking met de private sector en spelen private spelers een steeds grotere rol in politiewerk.Dit is in belangrijke mate het gevolg van de toegenomen macht en groei van de private sector en bezuinigingen in de publieke sector.6De technologische ontwikkelingen van big data en AI in het begin van de 21ste eeuw hebben geleid tot de toenemende macht van technologiebedrijven door onder meer ‘surveillance kapitalisme’, waarbij gegevensverzameling een economische drijfveer wordt voor bedrijven.Politiewerk wordt in toenemende mate platform policing, waarbij de politie gebruikt maakt van digitale platformen en digitale opsporingstechnologie.8Dit heeft als gevolg dat de politie steeds afhankelijker wordt van infrastructuur van technologiebedrijven9 en leidt tot verschuivende machtsverhoudingen van de publieke naar de private sector, wat een negatieve invloed heeft op transparantie en controle.10 Ten tweede, kan er een ‘democratisering’ van surveillance worden herkend door een verschuiving van aandacht voor gerichte surveillance naar grootschalige surveillance.11Hierdoor staat nu een veel groter deel van de bevolking onder surveillance waardoor het risico op toename van de macht van de staat maar ook van private actoren steeds groter wordt.Waarbij grootschalige surveillancepraktijken vroeger vooral werden uitgevoerd door intelligentiediensten12 of binnen een bepaalde context zoals op het vliegveld, spelen politiediensten maar ook technologiebedrijven hierin een steeds grotere rol.Denk bijvoorbeeld in België aan de significante uitbreiding van het gebruik van ‘intelligent’ cameratoezicht voor allerlei doeleinden13, wat nog meer werd aangewakkerd door de coronapandemie.14 Andere voorbeelden zijn de infiltratie van het versleutelde Encrochat-netwerk15, de gezichtsherkenningssoftware van Clearview AI dat ook uitgeprobeerd is door de federale politie in België16, of de dataverzamelingspraktijken van Europol17 die door sommige vergeleken worden met surveillance praktijken van de Amerikaanse NSA.18 Denk ten slotte aan het gebruik van de spionagesoftware Pegasus om data te verzamelen van mobiele telefoons van activisten, politici en journalisten over de hele wereld.19 Ten derde is er steeds meer sprake van collectieve en sociale schade naast individuele schade.Een belangrijk kenmerk van big data-analyses is dat ze op geaggregeerd niveau plaatsvinden.Er worden dus op het eerste gezicht geen persoonsgegevens verwerkt.20Een van de gevolgen is een toename van sociale stratificatie, met een ongelijke verhouding tussen maatschappelijke groepen als gevolg.Doordat big data onregelmatigheden en afwijkingen in datasets reproduceert kan dit leiden tot uitkomsten die een onevenredige impact hebben voor bepaalde groepen of gemeenschappen.Sinds het einde van de 20ste eeuw zijn er als gevolg van technologische ontwikkelingen nieuwe mogelijkheden ontstaan om data te verzamelen en te analyseren. 
