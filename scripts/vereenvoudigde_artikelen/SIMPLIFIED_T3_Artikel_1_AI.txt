Sinds het einde van de 20e eeuw hebben technologische ontwikkelingen nieuwe mogelijkheden gecreëerd voor het verzamelen en analyseren van data. Voorheen was het verzamelen en analyseren van data een relatief eenvoudige klus. Nu echterDe opkomst van 'big data' en de toegenomen mogelijkheden van kunstmatige intelligentie (AI) in de 21e eeuw worden door de politie met grote belangstelling omarmd.
Het gebruik van deze technologieën door de politie kan worden omschreven als algoritmische surveillance. Ze gebruiken bewakingscamera's en radar om verborgen verdachten op te sporen.Dit zijn algoritmische systemen die 1. op regels gebaseerde algoritmen gebruiken om gestructureerde en ongestructureerde gegevens te classificeren, op te slaan, te combineren en te doorzoeken, vastgelegde gegevens met andere gegevens te vergelijken en overeenkomsten te vinden; dit zijn geen geautomatiseerde systemen. Dit zijn geen systemen die 1. gebruiken omin 2.0 nam hij een vroege voorsprong, maar die van hemalgoritmen voor machinaal leren gebruiken om te proberen patronen en bruikbare kennis in big data-sets te voorspellen op basis van de patronen die in de vastgelegde gegevens worden gevonden.Ondanks de toegenomen regulering3, die tot doel heeft de democratische waarborgen van algoritmische surveillance te waarborgen, lijkt dit eerder het gebruik ervan te stimuleren. Dit garandeert echter niet dat algoritmen dat niet zullen zijnDenk bijvoorbeeld aan de forse toename van het gebruik van 'intelligent' cameratoezicht in België, maar ook elders in Europa.5 Hier bijvoorbeeldDit roept de vraag op of de huidige controlemechanismen voldoende zijn om alle burgers te beschermen tegen de mogelijke gevolgen van het gebruik van algoritmisch toezicht door de politie.Het doel van deze bijdrage is na te denken over de vraag of de huidige controle- en handhavingsmechanismen voor het gebruik van algoritmisch toezicht door de politie moeten worden heroverwogen.Om tot een (voorlopig) antwoord op die vraag te komen, bespreek ik in het eerste deel van het artikel drie socio-technische ontwikkelingen die het huidige kader onder druk zetten.In het tweede deel zal ik kijken naar de huidige mechanismen voor het afdwingen van controle door de lens van relationele ethiek om te onderzoeken hoe we hiervan kunnen leren om controlemechanismen te heroverwegen.Als gevolg van de opkomst van algoritmisch toezicht in de politie zijn drie socio-technische ontwikkelingen te onderscheiden die het traditionele controle- en handhavingskader onder druk zetten: 1) de opkomst van algoritmede versnippering en privatisering van de politie, 2) de democratisering van het toezicht, en 3) de toename van collectieve schade en sociale gevolgen.Deze ontwikkelingen overlappen elkaar en zijn met elkaar verweven en moeten niet als afzonderlijke ontwikkelingen worden gezien. Ze zijn op geen enkele zinvolle manier met elkaar verbonden. DezeTen eerste is de versnippering en privatisering van de politie niet nieuw. Het gebeurt al tientallen jaren, ook in de zaakSinds het einde van de 20e eeuw wordt er steeds meer samengewerkt met de private sector in het Westen en spelen private spelers een steeds belangrijkere rol in de politie.Dit komt grotendeels door de toegenomen macht en groei van het bedrijfsleven en bezuinigingen in de publieke sector.6 Dit komt ook door de stijging van de inkomens enDe technologische ontwikkelingen van big data en AI in het begin van de 21e eeuw hebben geleid tot een toenemende macht van technologiebedrijven door onder meer 'surveillance capitalism', waarin dataverzameling een economische motor wordt voor bedrijven.Politiewerk wordt steeds meer platformpolitie, waarbij de politie gebruik maakt van digitale platformen en digitale opsporingstechnologie.890 Politiewerk wordt meer platformonafhankelijkHierdoor wordt de politie in toenemende mate afhankelijk van infrastructuur van technologiebedrijven9 en leidt dit tot een verschuiving van machtsverhoudingen van de publieke naar de private sector, wat de transparantie en controle negatief beïnvloedt.10 Ten tweede kan een 'democratisering' van het toezicht herkenbaar aan een verschuiving van focus op gericht toezicht naar grootschalig toezicht.11Als gevolg hiervan staat nu een veel groter deel van de bevolking onder toezicht, waardoor het risico op een toename van de macht van de staat, maar ook van private actoren, groter wordt. Met als resultaat een veel grotereWaar grootschalige surveillancepraktijken vroeger vooral door inlichtingendiensten12 of binnen een specifieke context zoals op de luchthaven werden uitgevoerd, spelen politiediensten, maar ook technologiebedrijven, hierin een steeds belangrijkere rol.Denk in België bijvoorbeeld aan de forse uitbreiding van het gebruik van 'intelligente' camerabewaking voor allerlei doeleinden13, die nog eens werd aangewakkerd door de coronapandemie.14 Andere voorbeelden zijn de infiltratie van het versleutelde netwerk Encrochat15, de gezichtsherkenning van Clearview AI software die ook is getest door de federale politie in België16, of de dataverzamelingspraktijken van Europol17 die door sommigen worden vergeleken met surveillancepraktijken van de Amerikaanse NSA.18 Denk ten slotte aan het gebruik van de spionagesoftware Pegasus om gegevens van de mobiele telefoon te verzamelen telefoons van activisten, politici en journalisten over de hele wereld.19 Ten derde is er naast individuele schade ook steeds meer collectieve en maatschappelijke schade. OverwegenEen belangrijk kenmerk van big data-analyse is dat het op geaggregeerd niveau plaatsvindt. Het doet echter nietOp het eerste gezicht worden er dus geen persoonsgegevens verwerkt.20 Op het eerste gezicht dus neeEen van de gevolgen is een toename van sociale stratificatie, resulterend in een ongelijke verhouding tussen sociale groepen. Een van de belangrijkste redenen hiervoor is de highOmdat big data onregelmatigheden en anomalieën in datasets reproduceert, kan het leiden tot uitkomsten die een onevenredige impact hebben op bepaalde groepen of gemeenschappen.Dit kan dan leiden tot een cumulatieve benadeling (discriminatie en oneerlijke behandeling) van bepaalde groepen in de samenleving, omdat deze, vaak kwetsbare, groepen bovengemiddeld het doelwit zijn van deze technologieën.21. Dit kan ook leiden tot discriminatie van bepaalde personenDit is bijvoorbeeld duidelijk terug te zien in predictive policing. Politieagenten zijn opgeleid om misdaadgegevens te analyserenAls gevolg van terugkoppelingslussen die zijn gecreëerd door steekproefbias, wordt de politie herhaaldelijk teruggestuurd naar dezelfde buurten, ongeacht de werkelijke misdaadcijfers.2292.Dit leidt tot overpolitie en stigmatisering van bepaalde buurten en gemeenschappen die al het doelwit zijn.23 Deze risico's van discriminatie en stigmatisering door het gebruik van big data-analyses worden ook bevestigd in de uitspraak in Nederland over het gebruik van SyRI, een algoritmisch systeem voor sociale fraude opsporen. detecteren.23Daarnaast laat de uitspraak ook zien hoe big data-technologie de wereld kan veranderen. Bijvoorbeeld,heeft maatschappelijke gevolgen en draagt ​​naast discriminatie en stigmatisering ook bij aan de strafbaarstelling van armoede en kansarmoede en aan het vergroten van de ongelijkheid in de samenleving.24Hier op de weg, geconfronteerd met een stoereIk heb drie socio-technische ontwikkelingen beschreven die de huidige controlemechanismen onder druk zetten. Ten eerste is er een tekort aanIn het verdere verloop van deze bijdrage sta ik stil bij de vraag of de huidige controlemechanismen deze ontwikkelingen aankunnen. Dit is belangrijk omdat, als huidige controlemechanismenIk doe dit vanuit de lens van relationele ethiek. Ik weet dat wat mensen willen niet alleen hetZoals deze bijdrage duidelijk maakt, zetten sociaal-technische ontwikkelingen de traditionele controle- en handhavingsmechanismen onder druk. Dit in tegenstelling tot hoeDe vraag rijst of het huidige wettelijke kader volstaat om deze drie ontwikkelingen het hoofd te bieden en effectieve democratische waarborgen te bieden.Het wettelijk kader wordt tegenwoordig gevormd door de regels inzake gegevensbescherming. De regels over hoe gegevensbescherming wordt bereikt en hoeDe controlemiddelen die nu worden gebruikt voor de verwerking van gegevens door middel van AI, zoals toezichthouders, functionarissen voor de gegevensbescherming en Data Protection Impact Assessments (DPIA’s), zijn vaak beperkt in omvang.De focus ligt grotendeels op informatiebeveiliging en formele naleving van het wettelijk kader. Het doel is ervoor te zorgen dat bedrijven voldoen aan deAan de andere kant wordt er te weinig nadruk gelegd op de bescherming van grondrechten, en meer specifiek op basis van artikel 8 EVRM.25Bovendien is de manier waarop deze instrumenten in België werken niet erg democratisch, omdat burgers en het maatschappelijk middenveld er niet bij betrokken zijn. Bovendien is de manier waarop dezeDaarnaast is de politie niet verplicht om DPIA’s te publiceren conform de Richtlijn Politie en Justitie.
In aanvulling,Dit maakt publieke controle moeilijker. Het betekent dat het publiek meer betrokken moet wordenEr zijn ook geen normen waaraan DPIA’s moeten voldoen. Er zijn geen regels voor het recht opEr zijn ook geen standaardprofielen voor functionarissen voor gegevensbescherming. Ze hoeven er ook niet voor te zorgen dat hunHet huidige wettelijk kader heeft alleen betrekking op toepassingen van algoritmisch toezicht die 'persoonsgegevens' verzamelen en verwerken.26 Deze benadering wordt niet onderschreven doorDe EU heeft ondertussen een voorstel voor een AI-wet gepubliceerd die een tweeledig doel heeft: het voor individuen gemakkelijker maken om dit te doende bescherming van de grondrechten van het individu tegen de nadelige effecten van AI, evenals de harmonisatie van de regelgeving van de lidstaten om potentiële handelsbelemmeringen op de interne markt weg te nemen.De nadelige gevolgen van AI zijn onderverdeeld in risicocategorieën van laag naar hoog en naast risico's voor het individu spreekt de verordening ook over risico's voor de samenleving.De regeling maakt echter niet duidelijk wat deze risico's nu eigenlijk zijn.27 De voorgestelde regeling is hoopvol wat betreft controle- en handhavingsmechanismen.27 Maar nogmaals,Het geeft aan dat de lidstaten een of meer nationale bevoegde autoriteiten moeten aanwijzen om toezicht te houden op de toepassing en implementatie van AI en op te treden als officieel contactpunt voor het publiek en andere actoren. Het stelt ook voor dat de lidstaten een opleidingsprogramma opstellenOok wordt benadrukt dat handhavingsmechanismen kunnen worden versterkt “door een Europees coördinatiemechanisme in te voeren dat de juiste capaciteit biedt en audits van de AI-systemen vergemakkelijkt met nieuwe documentatie-, traceerbaarheids- en transparantievereisten”.28 De verordening geeft ook aan dat er een systeem zal worden opgezet om autonome AI-toepassingen met een hoog risico te registreren in een openbare EU-brede database en dat ze alleen op de Europese markt worden toegelaten als ze voldoen aan "bepaalde verplichte vereisten en een voorafgaande conformiteitsbeoordeling ondergaan". 29De wijze waarop deze beoordelingen in de praktijk zullen worden toegepast en gehandhaafd, blijft echter vaag. Maar dit betekent niet dat deze beoordelingen dat niet zullen doenHet is onduidelijk hoe de nalevingsmechanismen eruit zullen zien. Het is onduidelijk of de overheid geld zal biedenOok op democratisch vlak schiet de regeling tekort, omdat burgers of het maatschappelijk middenveld niet betrokken zijn bij deze mechanismen. In plaats daarvan richt de verordening zich op de rechten vanBovendien zouden burgers geen klacht kunnen indienen bij de nationale toezichthoudende autoriteit als zij menen dat de wet niet wordt nageleefd.30 Ook burgersHieronder reflecteer ik op wat we kunnen leren van relationele ethiek, geïnspireerd door de Ubuntu-filosofie, om anders te denken over controle in algoritmische politie, rekening houdend met de drie besproken sociaal-technische ontwikkelingen.De Ubuntu-filosofie vindt zijn oorsprong in de Afrikaanse filosofie ten zuiden van de Sahara.31 De Ubuntu-filosofie verschilt van de traditionele rationele ethiek in die zin dat, in tegenstelling tot de rationele kantiaanse ethiek, waar personen menselijke waardigheid hebben door hun vermogen tot autonomie, personen die menselijke waardigheid hebben omdat ze het vermogen hebben zich collectief tot elkaar verhouden.32 Vanuit deze visie zijn mensenrechtenschendingen gericht op het ernstig aantasten van het vermogen van mensen tot gemeenschappelijke relaties, opgevat als identiteit en solidariteit; en menselijke waardigheid moet worden gezien als het menselijk vermogen om op een gemeenschappelijke manier met anderen om te gaan.Verschillende computerwetenschappers, geïnspireerd door de Ubuntu-filosofie, stellen een fundamentele verschuiving voor in het denken over algoritmische onrechtvaardigheid en bestuur van AI van rationele ethiek naar relationele ethiek.33 Volgens Birhane is relationele ethiek “een raamwerk dat ons dwingt onze onderliggende werkhypothesen te heroverwegen. . dwingt ons om hiërarchische machtsasymmetrieën in twijfel te trekken, en brengt ons ertoe de bredere, contingente en onderling verbonden achtergrond te beschouwen waaruit algoritmische systemen voortkomen (en worden ingezet) in het proces van bescherming van het welzijn van de meest kwetsbaren.”34Deze visie gaat ervan uit dat de schade en het onrecht dat door algoritmische systemen wordt toegebracht, niet los kunnen worden gezien van de filosofische principes van technologie en de economische, politieke en sociale structuren die haar mede vormgeven.35. Deze kijk op algoritmische systemen gaat ervan uit dat schade en onrechtvaardigheid bestaanHoe valt deze visie te rijmen met de visie van politie en justitie die steeds grootschaliger toezicht en samenwerking met de private sector vereist?36.Dit zou impliceren dat ook het politiewerk vanuit dezelfde ethiek zou moeten vertrekken. Dit zou suggereren dat politiewerk zou moeten beginnenHet zou betekenen dat de politiemissie heroverwogen moet worden op een relationele manier, als het beschermen van de collectieve veiligheid. Het zou betekenen dat de politie zich zou moeten richten op het beschermen van collectiefIn het huidige beleid wordt veiligheid echter eng opgevat als bescherming tegen criminaliteit en handhaving van de openbare orde. In de praktijk daarentegen wordt beveiliging geïnterpreteerd als handhaving van de openbare ordeVaak gaat het niet eens meer om veiligheid, maar om politieke motieven, om te laten zien dat er serieus wordt opgetreden tegen criminaliteit. Soms gaat het niet meer om veiligheid, maar om politiekHet is een vorm van 'surveillancetheater'.37 Vanuit een collectieve visie op veiligheid die de veiligheid van alle burgers beoogt te waarborgen, zou meer aandacht moeten komen voor andere oorzaken van onveiligheid.Veiligheid is meer dan alleen bescherming tegen criminaliteit: gezonde voeding, schoon water, huisvesting, basisinkomen, gezondheidszorg, onderwijs en werk, maar bijvoorbeeld ook geen onderwerp zijn van discriminatie, intimidatie, haat, geweld en onevenredige overheidscontrole.Deze sociale en economische rechten zijn vaak niet opgenomen in het veiligheidsbeleid.38 Deze rechten zijn vaak niet opgenomen in het veiligheidsbeleidVanuit deze visie wordt bijvoorbeeld duidelijk dat encryptie cruciaal is om de mensenrechten en de meest kwetsbaren in de samenleving te beschermen, omdat door het inbouwen van achterdeurtjes in de technologie de veiligheid van bijvoorbeeld activisten en journalisten om democratische controle uit te oefenen wordt gecompromitteerd.geblokkeerd.395581.395581.Aangezien het, zeker in het huidige politieke klimaat, onwaarschijnlijk is dat veiligheid wordt gezien als sociale zekerheid, moet de vraag worden gesteld hoe de mazen van het net zo verfijnd kunnen worden dat controlemechanismen ervoor zorgen dat de meest kwetsbaren in de samenleving worden beschermd.Als we controle- en handhavingsmechanismen voor algoritmisch toezicht vanuit een relationeel-ethische invalshoek bekijken, impliceert dit dat het 'rationele' controlekader, geconstrueerd vanuit het paradigma van gegevensbescherming, tekortschiet, zeker in de manier waarop het zich vertaalt naar de praktijk en de nationale politiewetgeving wordt.Het rationele kader gaat uit van mondige betrokkenen die via informatieverzoeken individueel hun rechten kunnen beschermen, zonder rekening te houden met kwetsbare groepen.Niet alle betrokkenen zijn gelijk. Sommige mensen zijn meer geïnteresseerd in de sport dan anderenZe hebben verschillende inzichten, kennisniveaus, daadkracht, neiging om hun gegevens vrij te geven en individuele kwetsbaarheden. Ze hebben ook verschillende benaderingen om misdaad te bestrijden, van opsporingFactoren zoals leeftijd, geestelijke vermogens, deprivatie, geletterdheid of geslacht kunnen van invloed zijn op het genot en de uitoefening van individuele gegevensbeschermingsrechten.40 Controle moet daarom verder gaan dan louter statische technische oplossingen en formele naleving van de wet, naar een praktijk die rekening houdt met de dynamische historische context en socio-technische praktijken waarin de technologie is ingebed, aandacht heeft voor de machtsverhoudingen van de verschillende betrokken actoren en waarin de bescherming van de meest kwetsbaren in de samenleving voorop staat.Deze relationele controle impliceert het betrekken van de (belangen van) de betrokkene. Het houdt ook in dat de betrokkenede meest kwetsbaren en hun vertegenwoordigers zowel in beleid als in controlemechanismen die uitgaan van het socio-technische proces van algoritmische surveillance.41Daarnaast is transparantie cruciaal om vooroordelen en fouten die leiden tot mensenrechtenschendingen te voorkomen, zoals aangegeven door het Federaal Instituut voor de bescherming en bevordering van de mensenrechten (FIRM).Volgens de FIRM weten mensen in België momenteel vaak niet voor welke beslissingen de overheid algoritmen gebruikt. "Vaak doen mensen dat nietDaarnaast is niet altijd duidelijk hoe een algoritme persoonsgegevens verwerkt. Het kan de vorm aannemen vanverwerkt.42 Concreet betekent dit dat moet worden nagedacht over hoe controlemechanismen kunnen worden herdacht om rekening te houden met het bovenstaande.Hoe kunnen zij rekening houden met asymmetrische machtsverhoudingen en de toenemende macht van technologiebedrijven? Hoe kunnen ze hun kosten aanpassen aanHoe kunnen zij collectieve en maatschappelijke schade voorkomen? Wat kunnen ze doen om dat te voorkomen? Hoe kanVoordat een besluit wordt genomen om te investeren in (het ontwerp van) een bepaalde technologie door de politie, moet een democratische evidence-based proportionaliteitstoets worden uitgevoerd. Voordat er een beslissing wordt genomen,Deze toets betrekt burgers bij de beslissingen. Als burgers hun leven willen verbeteren, moeten ze dat doenDaarnaast besteedt deze toets aandacht aan de toenemende macht van de staat en private partners, maar ook aan collectieve en maatschappelijke schade.Deze test moet gebaseerd zijn op wetenschappelijke en objectieve analyse. Het moet aantonen dat de resultaten betrouwbaar en objectief zijn.Een orgaan als de Onafhankelijke Raad voor het Regeringsbeleid (WRR)43 zou hier bijvoorbeeld een rol kunnen spelen door in nauwe samenwerking met universiteiten en het maatschappelijk middenveld beleidsgericht onderzoek te doen.Dit orgaan zou dan bijvoorbeeld ook onderzoek kunnen doen naar de collectieve en maatschappelijke schade van algoritmisch toezicht en naar innovatieve controle- en handhavingsmechanismen.Daarnaast kan meer specifiek worden gedacht aan een AI-coördinatiecentrum, zoals het recente WRR AI-rapport voorstelt, dat beleidsdirecties, toezichthouders en uitvoeringsorganisaties een structuur biedt om regelmatig met elkaar in contact te komen en van elkaar te leren over uiteenlopende onderwerpen.

Daarnaast zou men ook kunnen denkenDit centrum zou politiek verankerd moeten zijn, zodat er snel beleid kan worden gemaakt als dat nodigis.44Zeker wanneer het gaat over grootschalige surveillance door politiediensten zou de bevolking betrokken moeten worden bij beslissingen om hun legitimiteit te bewaren.45Volgens het recente WRR-rapport over AI zal steeds vaker debat nodig zijn over de doelen die de samenleving wil nastreven en de vraag waar, waarvoor en onder welke condities de samenleving AI wil gebruiken.Methoden die hiervoor gebruikt kunnen worden, zijn bijvoorbeeld het organiseren van publieke debatten, openbare raadplegingen46, burgerjury’s, maar ook bijvoorbeeld de ondersteuning van citizen-science initiatieven.47 Door het publiek te betrekken als actieve deelnemers aan het proces, kan de overheid leren van de expertise van burgers.48 Vanuit de relationele ethiek is het dan wel van essentieel belang dat kwetsbare groepen en gemeenschappen een significante stem krijgen in beslissingsmakingsprocessen en dat dit niet enkel ‘voor de show’ is.Samenvattend biedt relationele controle interessante pistes aan om huidige controlemechanismen te herdenken, op een manier die rekening houdt met de sociaal-technische ontwikkelingen beschreven in de aanvang van deze bijdrage.In deze bijdrage heb ik gereflecteerd over de vraag of huidige controle- en handhavingsmechanismen voor algoritmische surveillance herdacht zouden moeten worden.Eerst heb ik drie sociotechnische ontwikkelingen besproken die huidige controlemechanismen onder druk zetten.Nadien heb ik gekeken naar welke lessenwe kunnen trekken als we controle- en handhavingsmechanismen voor algoritmische surveillance bekijken vanuit de relationele ethiek.Een voorlopig antwoord op de vraag is dat de drie socio-technische ontwikkelingen aangeven dat het huidig kader niet volstaat om deze ontwikkelingen op te vangen.Deze eerste exploratie van relationele ethiek om op een andere manier na te denken over controle en handhaving van gebruik van algoritmische surveillance door de politie, geeft aan dat ‘rationele’ controlemechanismen tekortschieten.Het relationele kader biedt interessante pistes om verder over de vooropgestelde vraag na te denken.Het antwoord in deze bijdrage blijft evenwel voorlopig, omdat verder (empirisch) onderzoek noodzakelijk zal zijn om hier beter inzicht in te krijgen.Sinds het einde van de 20ste eeuw zijn er als gevolg van technologische ontwikkelingen nieuwe mogelijkheden ontstaan om data te verzamelen en te analyseren. 
De opkomst van ‘big data’ en de toegenomen mogelijkheden van artificiële intelligentie (AI) in de 21ste eeuw zijn met veel interesse omarmd door de politie.
Het gebruik van deze technologieën door de politie kan worden beschreven als algoritmische surveillance.Dit zijn algoritmische systemen die 1. gebruik maken van op regels gebaseerde algoritmen om gestructureerde en ongestructureerde gegevens te classificeren, op te slaan, te combineren en te doorzoeken, om vastgelegde gegevens te vergelijken met andere gegevens en overeenkomsten te vinden;en 2.gebruik maken van machine-lerende algoritmes om patronen en bruikbare kennis in big data sets trachten te voorspellen op basis van de patronen die in de vastgelegde gegevens zijn gevonden.Ondanks de toegenomen regelgeving3, die als doel heeft de democratische waarborgen te garanderen van algoritmische surveillance, lijkt dit het gebruik ervan eerder te stimuleren.Denk bijvoorbeeld aan de significante toename van het gebruik van ‘intelligent’ cameratoezicht in België, maar ook elders in Europa.5Dit roept de vraag op of de huidige controlemechanismen voldoende zijn om alle burgers te beschermen tegen de mogelijke gevolgen van het gebruik van algoritmische surveillance door de politie.Het doel van deze bijdrage is om na te denken over de vraag of huidige controle en handhavingsmechanismen voor het gebruik van algoritmische surveillance door de politie herdacht zouden moeten worden.Om tot een (voorlopig) antwoord op die vraag te komen zal ik in het eerste deel van het artikel drie socio-technische ontwikkelingen bespreken die het huidig kader onder druk zetten.In het tweede deel zal ik huidige controleren handhavingsmechanismen bekijken door de bril van de relationele ethiek om te exploreren hoe we hieruit kunnen leren om controlemechanismen te herdenken.Als gevolg van de opkomst van algoritmische surveillance in het politiewerk kunnen drie sociotechnische ontwikkelingen geïdentificeerd worden die het traditionele controle- en handhavingskader onder druk zetten: 1)de fragmentatie en privatisering van politiewerk, 2) de democratisering van surveillance, en 3) de toename van collectieve schade en sociale gevolgen.Deze ontwikkelingen zijn overlappend en verstrengeld en moeten niet als losstaande ontwikkelingen worden gezien.Ten eerste, fragmentatie en privatisering van politiewerk is niet nieuw.Sinds het einde van 20ste eeuw is er in het Westen een stijging van de samenwerking met de private sector en spelen private spelers een steeds grotere rol in politiewerk.Dit is in belangrijke mate het gevolg van de toegenomen macht en groei van de private sector en bezuinigingen in de publieke sector.6De technologische ontwikkelingen van big data en AI in het begin van de 21ste eeuw hebben geleid tot de toenemende macht van technologiebedrijven door onder meer ‘surveillance kapitalisme’, waarbij gegevensverzameling een economische drijfveer wordt voor bedrijven.Politiewerk wordt in toenemende mate platform policing, waarbij de politie gebruikt maakt van digitale platformen en digitale opsporingstechnologie.8Dit heeft als gevolg dat de politie steeds afhankelijker wordt van infrastructuur van technologiebedrijven9 en leidt tot verschuivende machtsverhoudingen van de publieke naar de private sector, wat een negatieve invloed heeft op transparantie en controle.10 Ten tweede, kan er een ‘democratisering’ van surveillance worden herkend door een verschuiving van aandacht voor gerichte surveillance naar grootschalige surveillance.11Hierdoor staat nu een veel groter deel van de bevolking onder surveillance waardoor het risico op toename van de macht van de staat maar ook van private actoren steeds groter wordt.Waarbij grootschalige surveillancepraktijken vroeger vooral werden uitgevoerd door intelligentiediensten12 of binnen een bepaalde context zoals op het vliegveld, spelen politiediensten maar ook technologiebedrijven hierin een steeds grotere rol.Denk bijvoorbeeld in België aan de significante uitbreiding van het gebruik van ‘intelligent’ cameratoezicht voor allerlei doeleinden13, wat nog meer werd aangewakkerd door de coronapandemie.14 Andere voorbeelden zijn de infiltratie van het versleutelde Encrochat-netwerk15, de gezichtsherkenningssoftware van Clearview AI dat ook uitgeprobeerd is door de federale politie in België16, of de dataverzamelingspraktijken van Europol17 die door sommige vergeleken worden met surveillance praktijken van de Amerikaanse NSA.18 Denk ten slotte aan het gebruik van de spionagesoftware Pegasus om data te verzamelen van mobiele telefoons van activisten, politici en journalisten over de hele wereld.19 Ten derde is er steeds meer sprake van collectieve en sociale schade naast individuele schade.Een belangrijk kenmerk van big data-analyses is dat ze op geaggregeerd niveau plaatsvinden.Er worden dus op het eerste gezicht geen persoonsgegevens verwerkt.20Een van de gevolgen is een toename van sociale stratificatie, met een ongelijke verhouding tussen maatschappelijke groepen als gevolg.Doordat big data onregelmatigheden en afwijkingen in datasets reproduceert kan dit leiden tot uitkomsten die een onevenredige impact hebben voor bepaalde groepen of gemeenschappen.Sinds het einde van de 20ste eeuw zijn er als gevolg van technologische ontwikkelingen nieuwe mogelijkheden ontstaan om data te verzamelen en te analyseren. 
