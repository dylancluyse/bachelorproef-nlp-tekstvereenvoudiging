[Sinds het einde van de 20e eeuw hebben technologische ontwikkelingen nieuwe mogelijkheden gecreëerd voor het verzamelen en analyseren van data.
 De opkomst van 'big data' en de toegenomen mogelijkheden van kunstmatige intelligentie (AI) in de 21e eeuw worden door de politie met grote belangstelling omarmd.

 Het gebruik van deze technologieën door de politie kan worden omschreven als algoritmische surveillance.
 Dit zijn algoritmische systemen die 1. op regels gebaseerde algoritmen gebruiken om gestructureerde en ongestructureerde gegevens te classificeren, op te slaan, te combineren en te doorzoeken, om vastgelegde gegevens met andere gegevens te vergelijken en overeenkomsten te vinden;] en[en 2.
 algoritmen voor machine learning gebruiken om te proberen patronen en bruikbare kennis in big data-sets te voorspellen op basis van de patronen die in de vastgelegde gegevens worden gevonden.
 Ondanks de toegenomen regulering3, die tot doel heeft de democratische waarborgen van algoritmische surveillance te waarborgen, lijkt dit eerder het gebruik ervan te stimuleren.
 Denk bijvoorbeeld aan de forse toename van het gebruik van 'intelligent' cameratoezicht in België, maar ook elders in Europa.5
 Dit roept de vraag op of de huidige controlemechanismen voldoende zijn om alle burgers te beschermen tegen de mogelijke gevolgen van het gebruik van algoritmisch toezicht door de politie.][Het doel van deze bijdrage is na te denken over de vraag of de huidige controle- en handhavingsmechanismen voor het gebruik van algoritmisch toezicht door de politie moeten worden heroverwogen.
 Om tot een (voorlopig) antwoord op die vraag te komen, bespreek ik in het eerste deel van het artikel drie socio-technische ontwikkelingen die het huidige kader onder druk zetten.
 In het tweede deel zal ik kijken naar de huidige mechanismen voor het afdwingen van controle door de lens van relationele ethiek om te onderzoeken hoe we hiervan kunnen leren om controlemechanismen te heroverwegen.
 Door de opkomst van algoritmisch toezicht in de politie zijn drie socio-technische ontwikkelingen te onderscheiden die het traditionele controle- en handhavingskader onder druk zetten: 1)
 de versnippering en privatisering van de politie, 2) de democratisering van het toezicht, en 3) de toename van collectieve schade en sociale gevolgen.][Deze ontwikkelingen overlappen elkaar en zijn met elkaar verweven en moeten niet als afzonderlijke ontwikkelingen worden gezien.
 Ten eerste is de versnippering en privatisering van de politie niet nieuw.
 Sinds het einde van de 20e eeuw wordt er steeds meer samengewerkt met de private sector in het Westen en spelen private spelers een steeds belangrijkere rol in de politie.
 Dit komt grotendeels door de toegenomen macht en groei van de private sector en bezuinigingen in de publieke sector.6
 De technologische ontwikkelingen van big data en AI in het begin van de 21e eeuw hebben geleid tot een toenemende macht van technologiebedrijven door onder meer 'surveillance capitalism', waarin dataverzameling een economische motor wordt voor bedrijven.][Politiewerk wordt steeds meer platformpolitie, waarbij de politie gebruik maakt van digitale platformen en digitale opsporingstechnologie.8
 Hierdoor wordt de politie in toenemende mate afhankelijk van infrastructuur van technologiebedrijven9 en leidt dit tot een verschuiving van machtsverhoudingen van de publieke naar de private sector, wat de transparantie en controle negatief beïnvloedt.10 Ten tweede kan een 'democratisering' van het toezicht herkenbaar aan een verschuiving van focus op gericht toezicht naar grootschalig toezicht.11
 Als gevolg hiervan staat nu een veel groter deel van de bevolking onder toezicht, waardoor het risico op een toename van de macht van de staat, maar ook van private actoren, groter wordt.
 Waar grootschalige surveillancepraktijken vroeger vooral werden uitgevoerd door inlichtingendiensten12 of binnen een specifieke context zoals op de luchthaven, spelen politiediensten maar ook technologiebedrijven hierin een steeds belangrijkere rol.
 Denk in België bijvoorbeeld aan de forse uitbreiding van het gebruik van 'intelligente' camerabewaking voor allerlei doeleinden13, die nog eens werd aangewakkerd door de coronapandemie.14 Andere voorbeelden zijn de infiltratie van het versleutelde netwerk Encrochat15, de gezichtsherkenning van Clearview AI software die ook door de federale politie in België is getest16, of de dataverzamelingspraktijken van Europol17 die door sommigen worden vergeleken met surveillancepraktijken van de Amerikaanse NSA.18 Denk ten slotte aan het gebruik van de spionagesoftware Pegasus om gegevens van de mobiele telefoon te verzamelen telefoons van activisten, politici en journalisten over de hele wereld.19 Ten derde komt naast individuele schade steeds vaker collectieve en sociale schade voor.][Een belangrijk kenmerk van big data-analyse is dat het op geaggregeerd niveau plaatsvindt.
 Er worden dus op het eerste gezicht geen persoonsgegevens verwerkt.20
 Een van de gevolgen is een toename van sociale stratificatie, resulterend in een ongelijke verhouding tussen sociale groepen.
 Omdat big data onregelmatigheden en anomalieën in datasets reproduceert, kan het leiden tot uitkomsten die een onevenredige impact hebben op bepaalde groepen of gemeenschappen.
 Dit kan dan leiden tot een cumulatieve benadeling (discriminatie en oneerlijke behandeling) van bepaalde groepen in de samenleving, omdat deze, vaak kwetsbare, groepen bovengemiddeld het doelwit zijn van deze technologieën.21][Dit komt bijvoorbeeld goed tot uiting in predictive policing.
 Als gevolg van terugkoppelingslussen die zijn ontstaan ​​door bemonsteringsbias, wordt de politie herhaaldelijk teruggestuurd naar dezelfde buurten, ongeacht de werkelijke misdaadcijfers.22
 Dit leidt tot overpolitie en stigmatisering van bepaalde buurten en gemeenschappen die al het doelwit zijn.23 Deze risico's van discriminatie en stigmatisering door het gebruik van big data-analyses worden ook bevestigd in de uitspraak in Nederland over het gebruik van SyRI, een algoritmisch systeem voor sociale fraude opsporen. detecteren.
 Daarnaast laat de uitspraak ook zien hoe big data technologie werkt
 heeft maatschappelijke gevolgen en draagt ​​naast discriminatie en stigmatisering ook bij aan de strafbaarstelling van armoede en kansarmoede en aan het vergroten van de ongelijkheid in de samenleving.24][Hier boven
 Ik heb drie socio-technische ontwikkelingen beschreven die de huidige controlemechanismen onder druk zetten.
 In het verdere verloop van deze bijdrage sta ik stil bij de vraag of de huidige controlemechanismen deze ontwikkelingen aankunnen.
 Ik doe dit vanuit de lens van relationele ethiek.
 Zoals deze bijdrage duidelijk maakt, zetten sociotechnische ontwikkelingen traditionele controle- en handhavingsmechanismen onder druk.][De vraag rijst of het huidige wettelijke kader volstaat om deze drie ontwikkelingen het hoofd te bieden en effectieve democratische waarborgen te bieden.
 Het wettelijk kader wordt tegenwoordig gevormd door de regels inzake gegevensbescherming.
 De controlemiddelen die nu worden gebruikt voor de verwerking van gegevens door middel van AI, zoals toezichthouders, functionarissen voor de gegevensbescherming en Data Protection Impact Assessments (DPIA’s), zijn vaak beperkt in omvang.
 De focus ligt grotendeels op informatiebeveiliging en formele naleving van het wettelijk kader.
 Aan de andere kant wordt er te weinig nadruk gelegd op de bescherming van fundamentele rechten, en meer specifiek op artikel 8 EVRM.25][Bovendien is de manier waarop deze instrumenten in België werken niet erg democratisch, omdat burgers en het maatschappelijk middenveld er niet bij betrokken zijn.
 Daarnaast is de politie niet verplicht om DPIA’s te publiceren conform de Richtlijn Politie en Justitie.
 Dit maakt publieke controle moeilijker.
 Er zijn ook geen normen waaraan DPIA’s moeten voldoen.
 Er zijn ook geen standaardprofielen voor functionarissen voor gegevensbescherming.][Het huidige wettelijk kader heeft alleen betrekking op toepassingen van algoritmisch toezicht waarbij 'persoonsgegevens' worden verzameld en verwerkt.26
 De EU heeft ondertussen een voorstel voor een AI-wet gepubliceerd met een tweeledig doel:
 de bescherming van de grondrechten van het individu tegen de nadelige effecten van AI, evenals de harmonisatie van de regelgeving van de lidstaten om potentiële handelsbelemmeringen op de interne markt weg te nemen.
 De nadelige gevolgen van AI zijn onderverdeeld in risicocategorieën van laag naar hoog en naast risico's voor het individu spreekt de verordening ook over risico's voor de samenleving.
 De verordening maakt echter niet duidelijk wat die risico's precies zijn.27 Qua controle- en handhavingsmechanismen is de voorgestelde verordening hoopvol.][Het specificeert dat de lidstaten een of meer nationale bevoegde autoriteiten moeten aanwijzen om toezicht te houden op de toepassing en implementatie van AI en op te treden als officieel contactpunt voor het publiek en andere actoren.
 Ook wordt benadrukt dat de handhavingsmechanismen kunnen worden versterkt “door een Europees coördinatiemechanisme in te voeren dat de juiste capaciteit biedt en audits van de AI-systemen vergemakkelijkt met nieuwe documentatie-, traceerbaarheids- en transparantievereisten”.28 De verordening geeft ook aan dat er een systeem zal worden opgezet om autonome AI-toepassingen met een hoog risico te registreren in een openbare EU-brede database en dat ze alleen op de Europese markt zullen worden toegelaten als ze voldoen aan "bepaalde verplichte vereisten en een voorafgaande conformiteitsbeoordeling ondergaan". 29
 De wijze waarop deze beoordelingen in de praktijk zullen worden toegepast en gehandhaafd, blijft echter vaag.
 Het is onduidelijk hoe de nalevingsmechanismen eruit zullen zien.
 Ook op democratisch vlak schiet de regeling tekort, omdat burgers of het maatschappelijk middenveld niet betrokken zijn bij deze mechanismen.]28[Bovendien zouden burgers ook geen klacht kunnen indienen bij de nationale toezichthouder als zij menen dat de wet niet wordt nageleefd.30
 Hieronder reflecteer ik op wat we kunnen leren van relationele ethiek, geïnspireerd door de Ubuntu-filosofie, om anders te denken over controle in algoritmische politie, rekening houdend met de drie besproken sociaal-technische ontwikkelingen.
 De Ubuntu-filosofie vindt zijn oorsprong in de Afrikaanse filosofie ten zuiden van de Sahara.31 De Ubuntu-filosofie verschilt van de traditionele rationele ethiek in die zin dat, in tegenstelling tot de rationele kantiaanse ethiek, waar personen menselijke waardigheid hebben door hun vermogen tot autonomie, personen die menselijke waardigheid hebben omdat ze het vermogen hebben zich collectief tot elkaar verhouden.32 Vanuit deze visie zijn mensenrechtenschendingen gericht op het ernstig aantasten van het vermogen van mensen tot gemeenschappelijke relaties, opgevat als identiteit en solidariteit; en menselijke waardigheid moet worden gezien als het menselijk vermogen om op een gemeenschappelijke manier met anderen om te gaan.
 Verschillende computerwetenschappers, geïnspireerd door de Ubuntu-filosofie, stellen een fundamentele verschuiving voor in het denken over algoritmische onrechtvaardigheid en het beheer van AI van rationele ethiek naar relationele ethiek.33 Volgens Birhane is relationele ethiek “een raamwerk dat ons dwingt onze onderliggende werkhypothesen te heroverwegen. . dwingt ons om hiërarchische machtsasymmetrieën in twijfel te trekken, en brengt ons ertoe de bredere, contingente en onderling verbonden achtergrond te beschouwen waaruit algoritmische systemen voortkomen (en worden ingezet) in het proces van bescherming van het welzijn van de meest kwetsbaren.”34
 Deze visie gaat ervan uit dat de schade en het onrecht dat door algoritmische systemen wordt toegebracht, niet los kan worden gezien van de filosofische principes van technologie en de economische, politieke en sociale structuren die haar mede vormgeven.35][Hoe valt deze visie te rijmen met de visie van politie en justitie die steeds grootschaliger toezicht en samenwerking met het bedrijfsleven vraagt?36
 Dit zou impliceren dat ook het politiewerk vanuit dezelfde ethiek zou moeten vertrekken.
 Het zou betekenen dat de politiemissie heroverwogen moet worden op een relationele manier, als het beschermen van de collectieve veiligheid.
 In het huidige beleid wordt veiligheid echter eng opgevat als bescherming tegen criminaliteit en handhaving van de openbare orde.
 Vaak gaat het niet eens meer om veiligheid, maar om politieke motieven, om te laten zien dat er hard wordt opgetreden tegen criminaliteit.]36[Het is een vorm van 'surveillancetheater'.37 Vanuit een collectieve visie op veiligheid die de veiligheid van alle burgers beoogt te waarborgen, zou meer aandacht moeten komen voor andere oorzaken van onveiligheid.
 Veiligheid is meer dan alleen bescherming tegen criminaliteit: gezonde voeding, schoon water, huisvesting, basisinkomen, gezondheidszorg, onderwijs en werk, maar bijvoorbeeld ook geen onderwerp zijn van discriminatie, intimidatie, haat, geweld en onevenredige overheidscontrole.
 Deze sociale en economische rechten zijn vaak niet opgenomen in het veiligheidsbeleid.38
 Vanuit deze visie wordt bijvoorbeeld duidelijk dat encryptie cruciaal is om de mensenrechten en de meest kwetsbaren in de samenleving te beschermen, omdat door het inbouwen van achterdeurtjes in de technologie de veiligheid van bijvoorbeeld activisten en journalisten democratische controle kan uitoefenen wordt gecompromitteerd.
 belemmerd.39][Aangezien het, zeker in het huidige politieke klimaat, onwaarschijnlijk is dat veiligheid wordt gezien als sociale zekerheid, moet de vraag worden gesteld hoe de mazen van het net kunnen worden verfijnd zodat controlemechanismen ervoor zorgen dat de meest kwetsbaren in de samenleving worden beschermd.
 Als we controle- en handhavingsmechanismen voor algoritmisch toezicht vanuit een relationeel-ethische invalshoek bekijken, impliceert dit dat het 'rationele' controlekader, geconstrueerd vanuit het paradigma van gegevensbescherming, tekortschiet, zeker in de manier waarop het zich vertaalt naar de praktijk en de nationale politiewetgeving wordt.
 Het rationele kader gaat uit van mondige betrokkenen die via informatieverzoeken individueel hun rechten kunnen beschermen, zonder rekening te houden met kwetsbare groepen.
 Niet alle betrokkenen zijn gelijk.
 Ze hebben verschillende inzichten, kennisniveaus, daadkracht, neiging om hun gegevens vrij te geven en individuele kwetsbaarheden.][Factoren zoals leeftijd, geestelijke vermogens, deprivatie, geletterdheid of geslacht kunnen van invloed zijn op het genot en de uitoefening van individuele gegevensbeschermingsrechten.40 Controle moet daarom verder gaan dan louter statische technische oplossingen en formele naleving van de wet, naar een praktijk die rekening houdt met de dynamische historische context en socio-technische praktijken waarin de technologie is ingebed, heeft aandacht voor de machtsverhoudingen van de verschillende betrokken actoren, en waarin de bescherming van de meest kwetsbaren in de samenleving voorop staat.
 Deze relationele controle impliceert het betrekken van de (belangen van)
 de meest kwetsbaren en hun vertegenwoordigers zowel in beleid als in controlemechanismen die uitgaan van het socio-technische proces van algoritmische surveillance.41
 Daarnaast is transparantie cruciaal om vooroordelen en fouten die leiden tot mensenrechtenschendingen te voorkomen, zoals aangegeven door het Federaal Instituut voor de bescherming en bevordering van de mensenrechten (FIRM).
 Volgens de FIRM weten mensen in België momenteel vaak niet voor welke beslissingen de overheid algoritmen gebruikt.]40[Daarnaast is niet altijd duidelijk hoe een algoritme persoonsgegevens verwerkt
 verwerkt.42 Concreet betekent dit dat moet worden nagedacht over hoe controlemechanismen kunnen worden herdacht om rekening te houden met het bovenstaande.
 Hoe kunnen zij rekening houden met asymmetrische machtsverhoudingen en de toenemende macht van technologiebedrijven?
 Hoe kunnen zij collectieve en maatschappelijke schade voorkomen?
 Voordat een besluit wordt genomen om te investeren in (het ontwerp van) een bepaalde technologie door de politie, moet er een democratische evidence-based proportionaliteitstoets worden uitgevoerd.][Deze test betrekt burgers bij de beslissingen.
 Daarnaast besteedt deze toets aandacht aan de toenemende macht van de staat en private partners, maar ook aan collectieve en maatschappelijke schade.
 Deze test moet gebaseerd zijn op wetenschappelijke en objectieve analyse.
 Hierbij zou bijvoorbeeld een orgaan als de Onafhankelijke Raad voor het Regeringsbeleid (WRR)43 een rol kunnen spelen door in nauwe samenwerking met universiteiten en maatschappelijke organisaties beleidsgericht onderzoek te doen.
 Dit orgaan zou dan bijvoorbeeld ook onderzoek kunnen doen naar de collectieve en maatschappelijke schade van algoritmisch toezicht en naar innovatieve controle- en handhavingsmechanismen.][Daarnaast zou meer specifiek kunnen worden gedacht aan een AI-coördinatiecentrum, zoals het recente AI-rapport van de WRR voorstelt, dat beleidsdirecties, toezichthouders en uitvoeringsorganisaties een structuur biedt om regelmatig met elkaar in contact te komen en van elkaar te leren over uiteenlopende onderwerpen.
 Dit centrum dient politiek verankerd te zijn, zodat indien nodig snel beleid gemaakt kan worden
 is.44 Zeker als het gaat om grootschalig toezicht door politiediensten, moet de bevolking worden betrokken bij beslissingen om hun legitimiteit te behouden.45
 Volgens het recente WRR-rapport over AI zal er steeds meer debat nodig zijn over de doelen die de samenleving wil nastreven en de vraag waar, met welk doel en onder welke voorwaarden de samenleving AI wil inzetten.
 Methoden die hiervoor ingezet kunnen worden zijn bijvoorbeeld het organiseren van publieke debatten, publieke consultaties46, burgerjury's, maar bijvoorbeeld ook het ondersteunen van citizen science initiatieven.47 Door het publiek als actieve deelnemer in het proces te betrekken, kan de overheid leren van de expertise van burgers.48 Vanuit relationele ethiek is het essentieel dat kwetsbare groepen en gemeenschappen een stem van betekenis krijgen in besluitvormingsprocessen en dat dit niet alleen 'voor de show' is.]44[Samenvattend biedt relationele controle interessante mogelijkheden om de huidige controlemechanismen te herdenken, op een manier die rekening houdt met de sociaal-technische ontwikkelingen die aan het begin van deze bijdrage zijn beschreven.
 In deze bijdrage heb ik nagedacht over de vraag of de huidige controle- en handhavingsmechanismen voor algoritmisch toezicht moeten worden heroverwogen.
 Eerst besprak ik drie sociotechnische ontwikkelingen die de huidige controlemechanismen onder druk zetten.
 Daarna heb ik gekeken welke lessen
 we kunnen tekenen als we kijken naar controle- en handhavingsmechanismen voor algoritmische surveillance vanuit het perspectief van relationele ethiek.][Een voorlopig antwoord op de vraag is dat de drie socio-technische ontwikkelingen aangeven dat het huidige kader niet toereikend is om deze ontwikkelingen op te vangen.
 Deze eerste verkenning van relationele ethiek om de controle en handhaving van het gebruik van algoritmische surveillance door de politie te heroverwegen, geeft aan dat 'rationele' controlemechanismen ontoereikend zijn.
 Het relationele kader biedt interessante wegen voor verdere reflectie op de voorgestelde vraag.
 Het antwoord in deze bijdrage blijft echter voorlopig, omdat verder (empirisch) onderzoek nodig zal zijn om hier meer inzicht in te krijgen.
 Geen]