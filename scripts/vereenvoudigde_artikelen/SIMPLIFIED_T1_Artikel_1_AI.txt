De politie gebruikt machine-learningtechnieken om gegevens te verzamelen en te analyseren. Deze omvatten[en 2. algoritmen voor machine learning gebruiken om te proberen patronen en acties te voorspellenDe opkomst van algoritmische surveillance bij de politie heeft geleid[Politiehervorming is gevormd door twee factoren: de opkomst[Politiewerk wordt steeds meer platformpolitie, metDe analyse van big data wordt meestal op geaggregeerd niveau uitgevoerd, dat wil zeggen[Dit komt bijvoorbeeld duidelijk tot uiting bij predictive policing.
 Als gevolg van feedback loops, die ontstaan door steekproefbias, wordt politie herhaaldelijk teruggestuurd naar dezelfde wijken ongeacht het werkelijke misdaadcijfer.22
 Dit leidt tot overpolicing en stigmatisering van bepaalde al geviseerde wijken en gemeenschappen.23 Deze risico’s op discriminatie en stigmatisering door het gebruik van big data-analyses worden ook bevestigd in de uitspraak in Nederland over het gebruik van SyRI, een algoritmisch systeem om sociale fraude op te sporen.
 Daarnaast toont de uitspraak ook aan hoe big data-technologie
 sociale gevolgen heeft en naast discriminatie en stigmatisering ook bijdraagt tot het criminaliseren van armoede en kansarmoede en aan de toename van ongelijkheid in de samenleving.24][Hierboven
 heb ik drie socio-technische ontwikkelingen beschreven die huidige controlemechanismen onder druk zetten.
 In het verdere verloop van deze bijdrage reflecteer ik over de vraag of huidige controlemechanismen om kunnen gaan met deze ontwikkelingen.
 Ik doe dit vanuit de lens van relationele ethiek.
 Zoals deze bijdrage duidelijk maakt, zetten sociotechnische ontwikkelingen traditionele controle- en handhavingsmechanismen onder druk.][De vraag stelt zich of het huidige juridisch kader voldoende is om met deze drie ontwikkelingen om te gaan en effectieve democratische waarborgen te voorzien.
 Het juridisch kader wordt vandaag gevormd door de regels aangaande de gegevensbescherming.
 De controle-instrumenten die daarbij momenteel ingezet worden voor de verwerking van gegevens door middel van AI, zoals toezichtsorganen, functionarissen voor gegevensbescherming en gegevensbeschermingseffectbeoordelingen (Data Protection Impact Assessments of DPIA’s), zijn vaak beperkt in hun reikwijdte.
 De focus ligt grotendeels op informatieveiligheid en de formele naleving van het wettelijk kader.
 Er wordt daarentegen te weinig nadruk gelegd op de bescherming van fundamentele rechten, en meer specifiek vanuit artikel 8 van het EVRM.25][De manier waarop deze instrumenten werken in België is bovendien weinig democratisch, omdat burgers en het middenveld niet worden betrokken.
 Daarnaast is de politie niet verplicht DPIA’s te publiceren volgens de politie en justitierichtlijn.
 Hierdoor wordt publieke controle bemoeilijkt.
 Er bestaan ook geen standaarden waaraan DPIA’s moeten voldoen.
 Noch zijn er standaardprofielen voor functionarissen voor gegevensbescherming.][Het huidige wettelijke kader betreft enkel toepassingen van algoritmische surveillance die ‘persoonsgegevens’ verzamelen en verwerken.26
 De EU publiceerde intussen een voorstel van AI-wet dat een tweevoudig doel heeft:
 de bescherming van de grondrechten van het individu tegen de nadelige gevolgen van AI, en daarnaast de harmonisatie van de regelgeving van lidstaten om mogelijke handelsbelemmeringen op de interne markt weg te nemen.
 De nadelige gevolgen van AI worden opgesplitst in risico-categorieën van laag naar hoog en er wordt in de verordening naast risico’s voor het individu ook gesproken over risico’s voor de samenleving.
 De verordening maakt echter niet duidelijk wat deze risico’s juist zijn.27 Wat betreft controle- en handhavingsmechanismen is de voorgestelde verordening hoopvol.][Het geeft aan dat lidstaten één of meer nationale bevoegde autoriteiten moeten aanwijzen om toezicht te houden op de toepassing en uitvoering van AI en als officieel contactpunt voor het publiek en andere actoren moeten fungeren.
 Ook wordt benadrukt dat de handhavingsmechanismen versterkt kunnen worden “door de invoering van een Europees coördinatiemechanisme dat in de passende capaciteit voorziet en audits van de AI-systemen vergemakkelijkt met nieuwe eisen inzake documentatie, traceerbaarheid en transparantie”.28 De verordening geeft ook aan dat er een systeem zal opgezet worden om autonome AI-toepassingen met een hoog risico te registreren in een openbare databank voor de hele EU en dat deze enkel toegelaten zullen worden op de Europese markt indien zij voldoen aan “bepaalde dwingende voorschriften en vooraf een conformiteitsbeoordeling ondergaan.”29
 De manier waarop deze beoordelingen concreet in de praktijk toegepast en gehandhaafd zullen worden blijft echter vaag.
 Het is onduidelijk hoe de conformiteitsmechanismen eruit zullen zien.
 Ook schiet de verordening tekort op democratisch vlak, omdat burgers of het middenveld niet betrokken worden bij deze mechanismen.][Bovendien zouden burgers ook geen klacht kunnen indienen bij de nationale toezichthoudende autoriteit, indien zij menen dat de wet niet wordt nageleefd.30
 Hieronder reflecteer ik over wat we kunnen leren uit de relationele ethiek, geïnspireerd door Ubuntufilosofie, om op een andere manier over controle in de algoritmische politiepraktijk na te denken, rekening houdend met de drie besproken socio-technische ontwikkelingen.
 Ubuntufilosofie heeft zijn oorsprong in Afrikaanse filosofie uit landen ten zuiden van de Sahara.31 Ubuntufilosofie verschilt van traditionele rationele ethiek in de zin dat in tegenstelling tot rationele Kantiaanse ethiek, waarbij personen menselijke waardigheid hebben door hun vermogen tot autonomie, personen die menselijke waardigheid hebben omdat ze de capaciteit hebben om zich tot de andere te verhouden op een gezamenlijke manier.32 Vanuit deze visie zijn mensenrechtenschendingen erop gericht om het vermogen van mensen tot gemeenschappelijke betrekkingen, opgevat als identiteit en solidariteit, ernstig te schaden; en moet menselijke waardigheid gezien worden als het menselijk vermogen om zich op een gemeenschappelijke manier tot anderen te verhouden.
 Verschillende computerwetenschappers, die zich inspireren op Ubuntufilosofie, stellen een fundamentele verschuiving voor in het denken over algoritmische onrechtvaardigheid en bestuur van AI, van rationele ethiek naar relationele ethiek.33 Volgens Birhane is relationele ethiek “een kader dat ons ertoe dwingt onze onderliggende werkhypothesen opnieuw te onderzoeken, ons ertoe dwingt hiërarchische machtsasymmetrieën te ondervragen, en ons ertoe aanzet de bredere, contingente en onderling verbonden achtergrond te beschouwen waar algoritmische systemen uit voortkomen (en worden ingezet) in het proces van bescherming van het welzijn van de meest kwetsbaren”.34
 Deze visie veronderstelt dat de schade en onrechtvaardigheid die door algoritmische systemen wordt toegebracht, niet los kan worden gezien van de filosofische beginselen van de technologie en de economische, politieke en sociale structuren die het mee vormgeven.35]Het werk van de Europese Unie in de afgelopen twee decennia is geweest[Het is een vorm van 'surveillance'De vraag of de algoritmen die bij surveillance worden gebruikt, zijn ontworpen om deDe bescherming van de meest kwetsbaren in de samenleving vereist dat rekening wordt gehouden met het socialeHet is niet altijd duidelijk hoe een algoritme verwerkte persoonsgegevens verwerkt.42Deze test moet gebaseerd zijn op wetenschappelijke en objectieve analyse. BijvoorbeeldHet gebruik van kunstmatige intelligentie in de openbare veiligheid is niet zomaar een zaakDeze bijdrage verkent drie ontwikkelingen op het gebied van relationele controle enHet huidige socio-technologische raamwerk voor het beheersen van het gebruik van algoritmen