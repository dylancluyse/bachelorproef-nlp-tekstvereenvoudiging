Exploratie door de lens van de relationele ethiek. Sinds het einde van de twintigste eeuw zijn er door technologische vooruitgang nieuwe mogelijkheden ontstaan om data te verzamelen en te analyseren. Big data en AI in de eenentwintigste eeuw zijn met veel interesse door de politie omarmd. Algoritmische surveillance is het gebruik van deze technologieën door de politie. Om een antwoord te krijgen op de vraag, bespreek ik in het eerste deel van het artikel drie socio-technische ontwikkelingen die het huidige kader onder druk zetten. Deze ontwikkelingen zijn verweven en moeten niet als losstaand worden gezien.

Exploratie door de lens van de relationele ethiek. Technologische vooruitgang heeft nieuwe mogelijkheden gecreëerd om data te verzamelen en te analyseren sinds het einde van de twintigste eeuw. Big data en AI zijn met veel interesse door de politie omarmd in de eenentwintigste eeuw. Algoritmische surveillance is het gebruik van deze technologieën door de politie. Om een antwoord te krijgen op de vraag, bespreek ik in het eerste deel van het artikel drie socio-technische ontwikkelingen die het huidige kader onder druk zetten. Deze ontwikkelingen zijn verweven en niet losstaand.
Sinds het einde van de twintigste eeuw is er in het Westen meer samenwerking met de private sector en spelen private partijen een grotere rol in politiewerk. Door feedback loops, die veroorzaakt worden door steekproefbias, worden politieagenten herhaaldelijk naar dezelfde wijken gestuurd, ongeacht het werkelijke misdaadcijfer. Dit leidt tot overpolicing en stigmatisering van bepaalde wijken en gemeenschappen. Deze risico's op discriminatie en stigmatisering door het gebruik van big data-analyses worden bevestigd door de uitspraak in Nederland over het gebruik van SyRI. Daarnaast toont de uitspraak ook aan dat big data-technologie sociale gevolgen heeft, zoals het criminaliseren van armoede en kansarmoede en de toename van ongelijkheid in de samenleving. De politie is niet verplicht DPIA's te publiceren volgens de politie- en justitierichtlijn. Het huidige wettelijke kader betreft alleen toepassingen van algoritmische surveillance die persoonsgegevens verzamelen en verwerken. De EU heeft intussen een voorstel van AI-wet gepubliceerd met als doel de grondrechten van het individu te beschermen tegen de nadelige gevolgen van AI en de regelgeving van lidstaten te harmoniseren om mogelijke handelsbelemmeringen op de interne markt weg te nemen.
De nadelen van AI worden gerangschikt van laag naar hoog risico. De verordening spreekt over risico's voor individuen en de samenleving, maar maakt niet duidelijk wat deze risico's precies zijn. Er wordt een hoopvolle verordening voorgesteld met betrekking tot controle- en handhavingsmechanismen. Er wordt ook benadrukt dat deze mechanismen versterkt kunnen worden door een Europees coördinatiemechanisme dat audits van AI-systemen vergemakkelijkt met nieuwe eisen. Er zal ook een systeem opgezet worden om autonome AI-toepassingen met een hoog risico te registreren in een openbare databank voor de hele EU. De manier waarop deze beoordelingen toegepast en gehandhaafd worden blijft echter vaag. De verordening schiet ook tekort op democratisch vlak, omdat burgers of het middenveld niet betrokken worden bij deze mechanismen.

De gevolgen van AI worden gerangschikt van laag naar hoog risico. De verordening spreekt over risico's voor individuen en de samenleving, maar maakt niet duidelijk wat deze precies zijn. Er wordt een verordening voorgesteld met controle- en handhavingsmechanismen. Er wordt ook benadrukt dat deze versterkt kunnen worden door een Europees coördinatiemechanisme met nieuwe eisen. Er zal ook een systeem opgezet worden om AI-toepassingen met een hoog risico te registreren in een openbare databank voor de EU. Hoe deze beoordelingen toegepast en gehandhaafd worden blijft echter vaag. De verordening schiet ook tekort op democratisch vlak, omdat burgers of het middenveld niet betrokken worden.

De nadelen van AI worden gerangschikt van laag naar hoog risico. De verordening spreekt over risico's voor individuen en de samenleving, maar maakt niet duidelijk wat deze zijn. Er wordt een verordening voorgesteld met controle- en handhavingsmechanismen. Er wordt ook benadrukt dat deze versterkt kunnen worden door een Europees coördinatiemechanisme met nieuwe eisen. Er zal ook een systeem opgezet worden om AI-toepassingen met een hoog risico te registreren in een openbare databank voor de EU. Hoe deze beoordelingen toegepast en gehandhaafd worden blijft echter vaag. De verordening schiet ook tekort op democratisch vlak, omdat burgers of het middenveld niet betrokken worden.

De nadelen van AI worden gerangschikt van laag naar hoog risico. De verordening spreekt over risico's voor individuen en de samenleving, maar maakt niet duidelijk wat deze zijn. Er wordt een verordening voorgesteld met controle- en handhavingsmechanismen. Er wordt ook benadrukt dat deze versterkt kunnen worden door een Europees coördinatiemechanisme met nieuwe eisen. Er zal ook een systeem opgezet worden om AI-toepassingen met een hoog risico te registreren in een openbare databank voor de EU. Hoe deze beoordelingen toegepast en gehandhaafd worden blijft echter vaag. De verordening schiet ook tek
Politieopdracht moet herdacht worden als bescherming van collectieve veiligheid. Veiligheid wordt echter alleen gezien als bescherming tegen criminaliteit en handhaving van de orde. Mensen moeten hun rechten kunnen beschermen door informatieverzoeken, maar er wordt geen rekening gehouden met kwetsbare groepen. Leeftijd, geestelijke vermogen, kansarmoede, geletterdheid en geslacht kunnen invloed hebben op het genot en de uitoefening van rechten. Controle moet daarom meer zijn dan technische oplossingen en naleving van de wet. Er moet rekening gehouden worden met de dynamische context en praktijken waarin de technologie ingebed is, machtsrelaties van betrokkenen en bescherming van kwetsbaren.
Relationele controle vereist betrokkenheid van de meest kwetsbaren en hun vertegenwoordigers bij beleid en controlemechanismen die algoritmische surveillance als uitgangspunt nemen. Transparantie is essentieel om discriminatie en fouten te voorkomen. In België weten mensen vaak niet welke algoritmen de overheid gebruikt. Er moet een democratische proportionaliteitstoets uitgevoerd worden vooraleer er geïnvesteerd wordt in technologie door de politie. Het WRR AI rapport stelt een AI-coördinatiecentrum voor, waar beleidsdirecties, toezichthouders en uitvoeringsorganisaties regelmatig met elkaar in contact kunnen treden.
Methode om burgers te betrekken bij het besluitvormingsproces zijn bijvoorbeeld debatten, raadplegingen, burgerjury's en citizen-science initiatieven. Door burgers actief te laten meedoen, kan de overheid hun expertise gebruiken. Volgens de relationele ethiek is het belangrijk dat kwetsbare groepen en gemeenschappen een stem krijgen in de besluitvorming.

In deze bijdrage heb ik gereflecteerd over de vraag of de huidige controle- en handhavingsmechanismen voor algoritmische surveillance aangepast moeten worden. Eerst heb ik drie sociotechnische ontwikkelingen besproken die deze mechanismen onder druk zetten. Daarna heb ik gekeken naar wat we kunnen leren als we deze mechanismen bekijken vanuit het relationele kader. Dit kader biedt interessante mogelijkheden om verder te denken over deze vraag.
