Controle op gebruik algoritmische surveillance onder druk? Exploratie door lens relationele ethiek. Technologische ontwikkelingen sinds einde 20ste eeuw leiden tot nieuwe mogelijkheden data verzamelen en analyseren. Big data en AI in 21ste eeuw interessant voor politie. Gebruik technologie door politie als algoritmische surveillance. Systemen die 1.
Gebruik maken van regelgebaseerde algoritmen om gegevens te classificeren, opslaan, combineren en doorzoeken, en om deze te vergelijken met andere gegevens om overeenkomsten te vinden. Machine-lerende algoritmen gebruiken om patronen en bruikbare kennis in grote datasets te voorspellen. Ondanks de toegenomen regelgeving, die bedoeld is om de democratische waarborgen te garanderen, lijkt het gebruik ervan te worden gestimuleerd. Bijvoorbeeld door de toename van het gebruik van 'intelligent' cameratoezicht in België en andere Europese landen. Dit stelt de vraag of de huidige controlemechanismen voldoende zijn om alle burgers te beschermen tegen de mogelijke gevolgen van algoritmische surveillance door de politie.
Het doel van dit artikel is om na te denken over de vraag of de huidige controle- en handhavingsmechanismen voor het gebruik van algoritmische surveillance door de politie aangepast moeten worden. Om hier een antwoord op te krijgen, zal ik in het eerste deel drie sociotechnische ontwikkelingen bespreken die het huidige kader onder druk zetten. In het tweede deel zal ik de huidige controle- en handhavingsmechanismen bekijken door de bril van de relationele ethiek om te onderzoeken hoe we hieruit kunnen leren om deze mechanismen aan te passen.

Door de opkomst van algoritmische surveillance in het politiewerk, kunnen drie sociotechnische ontwikkelingen geïdentificeerd worden die het traditionele controle- en handhavingskader onder druk zetten: 1) de fragmentatie en privatisering van politiewerk, 2)
Democratisering van surveillance, toename van collectieve schade en sociale gevolgen: deze ontwikkelingen zijn verweven en moeten als één geheel worden gezien. Fragmentatie en privatisering van politiewerk is al langer aan de gang. Sinds het einde van de twintigste eeuw is er in het Westen een toename van samenwerking met de private sector. Dit komt door de groei van de private sector en bezuinigingen in de publieke sector.
Technologische vooruitgang in big data en AI aan het begin van de 21ste eeuw heeft geleid tot meer macht voor technologiebedrijven. Door 'surveillance kapitalisme' worden gegevens verzameld voor economische doeleinden. Politiewerk is steeds meer 'platform policing', waarbij de politie gebruik maakt van digitale platformen en technologie. Hierdoor wordt de politie afhankelijker van technologiebedrijven, wat een negatieve invloed heeft op transparantie en controle. Daarnaast is er een 'democratisering' van surveillance, waarbij een groter deel van de bevolking onder surveillance staat. Dit vergroot het risico op machtsuitoefening door de staat en private actoren. Vroeger werden grootschalige surveillancepraktijken uitgevoerd door intelligentiediensten, maar nu spelen ook politiediensten en technologiebedrijven hierin een steeds grotere rol.
België is een voorbeeld van het toenemende gebruik van 'intelligent' cameratoezicht voor verschillende doeleinden, wat versterkt werd door de coronacrisis. Ook de infiltratie van het versleutelde Encrochat-netwerk, de gezichtsherkenningssoftware van Clearview AI die ook door de federale politie in België werd getest, of de dataverzamelingspraktijken van Europol die door sommigen vergeleken worden met de surveillancepraktijken van de Amerikaanse NSA, zijn voorbeelden. Daarnaast is er het gebruik van de spionagesoftware Pegasus om data te verzamelen van mobiele telefoons van activisten, politici en journalisten over de hele wereld. Ten slotte is er steeds meer sprake van collectieve en sociale schade naast individuele schade. Big data-analyses vinden plaats op geaggregeerd niveau.
Er worden geen persoonsgegevens verwerkt. Dit leidt tot een toename van sociale stratificatie, waarbij bepaalde groepen in de maatschappij onevenredig worden beïnvloed. Big data reproduceert onregelmatigheden en afwijkingen in datasets, wat kan leiden tot cumulatief nadeel voor kwetsbare groepen. Dit komt duidelijk tot uiting bij predictive policing, waarbij politie herhaaldelijk naar dezelfde wijken wordt gestuurd, ongeacht het werkelijke misdaadcijfer.
Dit leidt tot overpolicing en stigmatisering van bepaalde wijken en gemeenschappen. Uitspraken over het gebruik van SyRI bevestigen dat big data-analyses risico's op discriminatie en stigmatisering met zich meebrengen. Daarnaast toont de uitspraak ook aan dat big data-technologie sociale gevolgen heeft, zoals criminalisering van armoede en toename van ongelijkheid. Bovenstaande drie ontwikkelingen zetten huidige controlemechanismen onder druk. In deze bijdrage reflecteer ik over de vraag of huidige controlemechanismen hiermee om kunnen gaan, vanuit de lens van relationele ethiek.
Het huidige juridische kader is niet voldoende om de drie sociotechnische ontwikkelingen te beheersen en effectieve democratische waarborgen te bieden. De regels voor gegevensbescherming vormen het juridische kader. De huidige controle-instrumenten voor AI-gegevensverwerking, zoals toezichthouders, functionarissen voor gegevensbescherming en DPIA's, hebben een beperkte reikwijdte. De focus ligt voornamelijk op informatieveiligheid en de formele naleving van de wet.
Er wordt te weinig aandacht besteed aan de bescherming van fundamentele rechten, met name artikel 8 van het EVRM. De manier waarop deze instrumenten in België werken is niet democratisch, omdat burgers en het middenveld niet betrokken worden. Bovendien is de politie niet verplicht DPIA's te publiceren, waardoor publieke controle moeilijk wordt. Er zijn ook geen standaarden waaraan DPIA's moeten voldoen.
Er zijn geen standaardprofielen voor gegevensbescherming. Het huidige wetgevende kader beperkt zich tot algoritmische surveillance die persoonsgegevens verzamelt en verwerkt. De EU heeft een voorstel voor een AI-wet gepubliceerd met twee doelen: het beschermen van de grondrechten van het individu tegen de nadelige gevolgen van AI en het harmoniseren van de regelgeving van lidstaten om mogelijke handelsbelemmeringen op de interne markt weg te nemen. De schadelijke effecten van AI worden ingedeeld in risicocategorieën van laag naar hoog. De verordening gaat ook over risico's voor de samenleving, naast risico's voor het individu.
Verordening maakt niet duidelijk wat risico's zijn. Wat betreft controle- en handhavingsmechanismen is de verordening hoopvol. Lidstaten moeten autoriteiten aanwijzen voor toezicht en als contactpunt voor publiek. Handhavingsmechanismen kunnen versterkt worden door Europees coördinatiemechanisme met nieuwe eisen. Systeem voor registratie van hoog risico AI-toepassingen in openbare databank. Toegelaten op Europese markt als voldoen aan voorschriften en voorafgaande beoordeling. Manier waarop beoordelingen toegepast en gehandhaafd blijft vaag.
Hoe de conformiteitsmechanismen eruit zullen zien is onduidelijk. De verordening schiet tekort op democratisch vlak, omdat burgers of het middenveld niet betrokken worden. Bovendien kunnen burgers geen klacht indienen bij de nationale toezichthoudende autoriteit als zij denken dat de wet niet wordt nageleefd. Relationele ethiek, geïnspireerd door Ubuntufilosofie, kan ons helpen om op een andere manier na te denken over controle in de algoritmische politiepraktijk. Deze filosofie heeft zijn oorsprong in Afrikaanse landen ten zuiden van de Sahara. In tegenstelling tot traditionele rationele ethiek, waarbij personen menselijke waardigheid hebben door hun vermogen tot autonomie, heeft Ubuntufilosofie mensen menselijke waardigheid omdat ze de capaciteit hebben om zich tot de andere te verhouden op een gezamenlijke manier. Mensenrechtenschendingen richten zich op het schaden van het vermogen van mensen tot gemeenschappelijke betrekkingen, opgevat als identiteit en solidariteit.
Mensenwaardigheid is het vermogen om met anderen op een gemeenschappelijke manier om te gaan. Computerwetenschappers die zich laten inspireren door de Ubuntu-filosofie, stellen een fundamentele verandering voor in het denken over algoritmische onrechtvaardigheid en AI-bestuur, van rationele naar relationele ethiek. Volgens Birhane is relationele ethiek een kader dat ons aanzet om onze onderliggende werkhypothesen te herzien, machtsasymmetrieën te onderzoeken en de bredere context te bekijken waaruit algoritmische systemen voortkomen. Dit betekent dat schade en onrechtvaardigheid die door algoritmische systemen worden veroorzaakt, niet losgezien kunnen worden van de filosofische beginselen van de technologie en de economische, politieke en sociale structuren die het vormgeven. Hoe kan deze visie worden gecombineerd met het politie- en justitieapparaat dat steeds meer surveillance en samenwerking met de private sector vraagt? Dit impliceert dat ook politiewerk vanuit dezelfde ethiek zou moeten vertrekken.
Politie moet collectieve veiligheid beschermen in plaats van alleen criminaliteit en publieke orde. Vaak gaat het niet meer om veiligheid, maar om politieke motieven. Dit is 'surveillance theater'. Er moet meer aandacht besteed worden aan andere oorzaken van onveiligheid. Veiligheid is meer dan alleen bescherming tegen criminaliteit.
Gezond eten, schoon water, huisvesting, basisinkomen, gezondheidszorg, onderwijs en werk zijn basisrechten. Ook het niet worden gediscrimineerd, gepest, gehaat, geweld en disproportionele overheidscontrole. Deze sociale en economische rechten worden vaak niet meegenomen in het veiligheidsbeleid. Encryptie is belangrijk om mensenrechten en kwetsbaren te beschermen, omdat achterdeuren in technologie de veiligheid van activisten en journalisten in gevaar brengen. Om de meest kwetsbaren te beschermen, moeten de mazen van het net verfijnd worden. Als we kijken naar controle- en handhavingsmechanismen voor algoritmische surveillance, dan blijkt dat het 'rationele' kader, gebaseerd op gegevensbescherming, tekort schiet. Het kader gaat uit van mondige betrokkenen die hun rechten individueel kunnen beschermen, zonder rekening te houden met kwetsbare groepen.
Niet iedereen is gelijk. Er zijn verschillende meningen, kennisniveaus, besluitvaardigheid, bereidheid om informatie te delen en kwetsbare eigenschappen. Leeftijd, verstandelijke vermogens, armoede, geletterdheid of geslacht kunnen invloed hebben op het genieten en uitoefenen van individuele rechten op gegevensbescherming. Daarom moet controle verder gaan dan technische oplossingen en formele wetgeving. Er moet rekening gehouden worden met de dynamische context en sociale technische praktijken waarin de technologie is ingebed, er moet aandacht zijn voor de machtsverhoudingen tussen de betrokken partijen en de meest kwetsbaren moeten beschermd worden. Deze relatieve controle betekent dat de belangen van de meest kwetsbaren en hun vertegenwoordigers betrokken moeten worden bij het beleid en controlemechanismen die gebaseerd zijn op algoritmische surveillance. Transparantie is ook belangrijk om te voorkomen dat vooroordelen en fouten leiden tot schendingen van de mensenrechten, zoals het Federaal Instituut voor de Bescherming en Bevordering van de Rechten van de Mens (FIRM) aangeeft. Volgens het FIRM weten mensen in België vaak niet waarvoor de overheid algoritmen gebruikt.
Niet altijd is duidelijk hoe persoonsgegevens verwerkt worden. Daarom moet er nagedacht worden over hoe controlemechanismen kunnen worden geïmplementeerd om rekening te houden met asymmetrische machtsrelaties en de macht van technologiebedrijven. Ook moet er gekeken worden naar hoe collectieve en sociale schade voorkomen kan worden. Vooraleer de politie investeert in een technologie, moet er een democratische evidence-based proportionaliteitstoets worden uitgevoerd.
Burgers worden betrokken bij besluitvorming door middel van deze toets. Daarnaast wordt er rekening gehouden met de groeiende macht van de staat en private partijen, evenals met collectieve en sociale schade. Deze toets moet gebaseerd zijn op wetenschappelijke en objectieve analyse. Een orgaan zoals de WRR kan hierbij een rol spelen door samen met universiteiten en het middenveld beleidsgericht onderzoek te doen. Ook kan er onderzoek gedaan worden naar de collectieve en sociale schade van algoritmische surveillance en naar innovatieve controle- en handhavingsmechanismen.
Een AI-coördinatiecentrum, zoals voorgesteld in het WRR AI rapport, biedt een structuur om regelmatig met elkaar in contact te treden en van elkaar te leren. Het centrum moet politiek verankerd zijn om snel beleid te kunnen maken. Bij grootschalige surveillance door politiediensten moet de bevolking betrokken worden bij beslissingen. Er is debat nodig over de doelen die de samenleving wil nastreven en de vraag waar, waarvoor en onder welke condities AI gebruikt wordt. Methoden om dit te doen zijn publieke debatten, openbare raadplegingen, burgerjury's en citizen-science initiatieven. Door het publiek te betrekken als actieve deelnemers aan het proces, kan de overheid leren van de expertise van burgers. Het is van essentieel belang dat kwetsbare groepen en gemeenschappen een significante stem krijgen in beslissingsmakingsprocessen. 

Er is een AI-coördinatiecentrum nodig om regelmatig met elkaar in contact te treden en van elkaar te leren. Het centrum moet politiek verankerd zijn om snel beleid te kunnen maken. Bij grootschalige surveillance door politiediensten moet de bevolking betrokken worden bij beslissingen. Er is debat nodig over de doelen die de samenleving wil nastreven en de vraag waar, waarvoor en onder welke condities AI gebruikt wordt. Methoden om dit te doen zijn publieke debatten, openbare raadplegingen, burgerjury's en citizen-science initiatieven. Door het publiek te betrekken als actieve deelnemers aan het proces, kan de overheid leren van de expertise van burgers. Het is essentieel dat kwetsbare groepen en gemeenschappen een significante stem krijgen in beslissingsmakingsprocessen.
Samenvattend: Relationele controle biedt interessante mogelijkheden om de huidige controle- en handhavingsmechanismen voor algoritmische surveillance te herdenken, rekening houdend met de sociaal-technische ontwikkelingen die in deze bijdrage besproken zijn. We kunnen lessen trekken uit de relationele ethiek als we deze mechanismen bekijken.
De drie socio-technische ontwikkelingen tonen aan dat het huidige kader niet voldoende is om deze te beheersen. Relationele ethiek biedt een andere manier om naar controle en handhaving van algoritmische surveillance door de politie te kijken, wat aangeeft dat 'rationele' controlemechanismen niet voldoende zijn. Er zijn interessante mogelijkheden om verder te denken over de vraag, maar het antwoord blijft voorlopig, omdat verdere (empirische) onderzoeken nodig zijn om meer inzicht te krijgen.
