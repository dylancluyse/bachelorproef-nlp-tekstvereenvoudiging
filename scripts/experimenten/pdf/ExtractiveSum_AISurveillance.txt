Een exploratie door de lens van de  relationele ethiek  Sinds het einde van de 20ste eeuw zijn er als gevolg van technologische ontwikkelingen nieuwe mogelijkheden  ontstaan om data te verzamelen en te analyseren. De opkomst van ‘big data’ en de toegenomen  mogelijkheden van artificiële intelligentie (AI) in de 21ste eeuw zijn met veel interesse omarmd door de politie. Het gebruik van deze technologieën door de politie kan worden beschreven als algoritmische surveillance. Om tot een (voorlopig)  antwoord op die vraag te komen zal ik in het eerste deel van het artikel drie socio-technische ontwikkelingen  bespreken die het huidig kader onder druk zetten. Deze ontwikkelingen zijn overlappend en verstrengeld en moeten  niet als losstaande ontwikkelingen worden gezien.
Sinds het einde van 20ste eeuw is er in het Westen een stijging van de samenwerking met de  private sector en spelen private spelers een steeds grotere rol in politiewerk. Als gevolg van feedback loops, die ontstaan door  steekproefbias, wordt politie herhaaldelijk teruggestuurd naar dezelfde wijken ongeacht het werkelijke  misdaadcijfer.22 Dit leidt tot overpolicing en stigmatisering van bepaalde al geviseerde wijken en  gemeenschappen.23 Deze risico’s op discriminatie en stigmatisering door het gebruik van big data-analyses  worden ook bevestigd in de uitspraak in Nederland over het gebruik van SyRI, een algoritmisch systeem om  sociale fraude op te sporen. Daarnaast toont de uitspraak ook aan hoe big data-technologie sociale gevolgen  heeft en naast discriminatie en stigmatisering ook bijdraagt tot het criminaliseren van armoede en  kansarmoede en aan de toename van ongelijkheid in de samenleving.24 Hierboven heb ik drie socio-technische  ontwikkelingen beschreven die huidige controlemechanismen onder druk zetten. Daarnaast is de politie niet verplicht DPIA’s te publiceren volgens de  politie en justitierichtlijn. Het huidige wettelijke kader betreft enkel toepassingen van algoritmische surveillance  die ‘persoonsgegevens’ verzamelen en verwerken.26 De EU publiceerde intussen een voorstel van AI-wet  dat een tweevoudig doel heeft: de bescherming van de grondrechten van het individu tegen de nadelige  gevolgen van AI, en daarnaast de harmonisatie van de regelgeving van lidstaten om mogelijke  handelsbelemmeringen op de interne markt weg te nemen.
De nadelige gevolgen van AI worden opgesplitst in  risico-categorieën van laag naar hoog en er wordt in de verordening naast risico’s voor het individu ook  gesproken over risico’s voor de samenleving. De verordening maakt echter niet duidelijk wat deze risico’s juist  zijn.27  Wat betreft controle- en handhavingsmechanismen is de voorgestelde verordening hoopvol. Ook wordt benadrukt dat de handhavingsmechanismen versterkt kunnen worden “door de invoering  van een Europees coördinatiemechanisme dat in de passende capaciteit voorziet en audits van de AI-systemen  vergemakkelijkt met nieuwe eisen inzake documentatie, traceerbaarheid en transparantie”.28 geeft ook aan dat er een systeem zal opgezet worden om autonome AI-toepassingen met een hoog risico te  registreren in een openbare databank voor de hele EU en dat deze enkel toegelaten zullen worden op de  Europese markt indien zij voldoen aan “bepaalde dwingende voorschriften en vooraf een  conformiteitsbeoordeling ondergaan. ”29 De manier waarop deze beoordelingen concreet in de praktijk  toegepast en gehandhaafd zullen worden blijft echter vaag. Ook schiet de verordening tekort op democratisch vlak, omdat  burgers of het middenveld niet betrokken worden bij deze mechanismen.
Het zou betekenen dat de politieopdracht herdacht zou  moeten worden op een relationele manier, als het beschermen van collectieve veiligheid. In het huidig  beleid wordt veiligheid echter op een enge manier geïnterpreteerd als bescherming tegen criminaliteit en  handhaving van de publieke orde. Het rationele kader gaat uit van mondige betrokkenen die individueel hun rechten kunnen  beschermen door middel van informatieverzoeken, waarbij geen rekening wordt gehouden met kwetsbare  groepen. Ze hebben verschillende inzichten, niveaus van kennis,  besluitvaardigheid, neiging om hun gegevens bekend te maken, en individuele kwetsbare eigenschappen. Factoren als leeftijd, geestelijk vermogen, kansarmoede, geletterdheid of geslacht kunnen van invloed zijn op  het genot en de uitoefening van individuele rechten over gegevensbescherming.40   Controle moet daarom verder gaan dan enkel statische technische oplossingen en formele naleving van de  wet, naar een praktijk die rekening houdt met de dynamische historische context en sociaal-technische  praktijken waarin de technologie ingebed is, aandacht heeft voor machtsrelaties van de verschillende  betrokken actoren, en waarin de bescherming van de meest kwetsbaren in de maatschappij voorop staat.
Deze relationele controle impliceert het betrekken van de (belangen van) de meest kwetsbaren en  hun vertegenwoordigers in het beleid alsook in controlemechanismen die het sociaal-technisch  proces van algoritmische surveillance als uitgangspunt nemen.41 Daarnaast is transparantie cruciaal om  te vermijden dat vooroordelen en fouten leiden tot schendingen van de mensenrechten, zoals het  Federaal Instituut voor de bescherming en bevordering van de rechten van de mens (FIRM) aangeeft. Volgens  het FIRM weten mensen in België momenteel vaak niet voor welke beslissingen de overheid algoritmen  gebruikt. Hoe kunnen ze rekening houden met asymmetrische  machtsrelaties en de toenemende macht van technologiebedrijven? Vooraleer er besloten wordt om te investeren in (het ontwerpen van) een  bepaalde technologie door de politie, moet er een democratische evidence-based proportionaliteitstoets  uitgevoerd worden. Daarnaast zou men meer specifiek kunnen denken aan een AI-coördinatiecentrum, zoals het recente  WRR AI rapport voorstelt, dat aan beleidsdirecties, toezichthouders en uitvoeringsorganisaties een  structuur biedt om regelmatig en rond uiteenlopende kwesties met elkaar in contact te treden en van elkaar  te leren.
Methoden die hiervoor gebruikt kunnen  worden, zijn bijvoorbeeld het organiseren van publieke debatten, openbare raadplegingen46, burgerjury’s, maar  ook bijvoorbeeld de ondersteuning van citizen-science initiatieven.47 Door het publiek te betrekken als actieve  deelnemers aan het proces, kan de overheid leren van de expertise van burgers.48  Vanuit de relationele ethiek is het dan wel van essentieel belang dat kwetsbare groepen en  gemeenschappen een significante stem krijgen in beslissingsmakingsprocessen en dat dit niet enkel  ‘voor de show’ is. Conclusie  In deze bijdrage heb ik gereflecteerd over de vraag of huidige controle- en handhavingsmechanismen voor algoritmische surveillance herdacht zouden moeten worden. Eerst heb ik drie sociotechnische  ontwikkelingen besproken die huidige controlemechanismen onder druk zetten. Nadien  heb ik gekeken naar welke lessen we kunnen trekken als we controle- en handhavingsmechanismen  voor algoritmische surveillance bekijken vanuit de relationele ethiek. Het relationele kader biedt interessante pistes om  verder over de vooropgestelde vraag na te denken.
