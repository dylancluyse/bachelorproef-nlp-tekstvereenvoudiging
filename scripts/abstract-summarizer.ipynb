{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tiktoken\n",
    "import configparser, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')\n",
    "openai.api_key = config['openai']['api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "CHUNK_LENGTH = 3000\n",
    "\n",
    "# Define a function to break up a text into chunks of 500 tokens\n",
    "def chunk_text(text):\n",
    "    doc = nlp(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    chunk_size = CHUNK_LENGTH\n",
    "    for token in doc:\n",
    "        if len(current_chunk) + len(token.text_with_ws) > chunk_size:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = \"\"\n",
    "        current_chunk += token.text_with_ws\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"INTRODUCTION\\nThe definition of sexual discrimination could be a form of discrimination that a person is treated differently or unequally because of their sex/gender. Although under federal law sex discrimination in employment is illegal, such discrimination still happens in the job field and has always been hidden under the norm. For example, 42 % of women in the United States have faced gender discrimination on the job[2]. One of the purposes of creating AI is to help with diversity, solve problems like discrimination and racism, however, whether it works as people predicted is still a question mark. The concern is if AI and automation are not used in a gender responsible method, and it might reinforce pre-existing gender bias.\\nDue to better productivity and decrease of cost and laborers, it is not hard to imagine that in the future AI technology would involve more deeply in manufacturing and jobs in general. Automation, big data and algorithms could cause great impacts on women in jobs. The concern of job replacement, automated hiring system, privacy information releases to the public, poorly selected training data, the issue with algorithm design and issue due to data inequality would all raise or influence sexual discrimination. Companies and governments would need to take action to face changes caused by AI to ensure social orders, justice and equality.\\n2. AUTOMATION’S EFFECT\\nAlthough the world is facing unprecedented growth in both jobs and economics, automation still has a high potential to replace numerous jobs, and especially those technologies are repetitive and have little human interaction in comparison. The cost to automate would also be one of the considerations. Thus a majority of workers that involve predictable tasks and activities would have a high replacement rate. Then how is it going to relate to gender discrimination.\\nAlthough there is a bigger change that women’s job is prone to partial automation than being entirely replaced. In McKinsey's future prediction of women’s traditions by 2030, around 40 million to 160 million women might face a need to transition across occupations and skill sets to remain employed[3]. This number also established the need for higher education and different skills for success. It is 7 to 24 percent of women that are currently employed compared to the range of 8 to 28 percent for men. If women take advantage of transition opportunities, they could maintain their current share of employment. Advances in Social Science, Education and Humanities Research, volume 554 Proceedings of the 7th International Conference on Humanities and Social Science Research (ICHSSR 2021) Copyright © 2021 The Authors. Published by Atlantis Press SARL. This is an open access article distributed under the CC BY-NC 4.0 license -http://creativecommons.org/licenses/by-nc/4.0/. 858 If they cannot, gender inequality in work could worsen. Thus, if they cannot make the necessary transition, many women could face an\", 'intensifying wage gap relative to men. Since talking automation, the STEM field would have a higher possibility for employment requirements in the future. However, the data in 2019 determined that women accounted for only 27% of workers in STEM-qualified industries, not to mention onaverage women made 19% less than men[4]. Not to mention, with 78% of AI professionals being men, algorithms are created with male dominated experiences. Such gender bias could be significantly disadvantageous to female employers or resumes. Although robotization and automation in the job field would impact both gender, gender bias is likely to come into play and affect women disproportionately. Women over-represented in certain high-risk automation sectors could suffer more. The lack of mobility and flexibility could also be reasons for companies to unemploy women or reduce bargaining positions[5]. The risks in automation are real. The US Equal Employment Opportunity Commission is investigating at least two cases involving algorithms that could be discriminated toward certain groups of job applicants[6]. To prevent sexual discrimination, the education system needs to change from the beginning to stop discrimination, provide more opportunities and support to STEM field, and future job supplements caused by automation.\\n3. BIG DATA’S IMPACT\\nBig data analysis and algorithm would also affect discrimination in occupations, it might turnover the traditional hiring process. Since the application is based on the collection of big data. If an AI application is trained on bias data, the algorithms would likely be biased. Good grade, school, or capabilities would not just be the only measurement. In the content of collecting more types of data, the metadata of the social media content, family members, anything remotely relevant would all be a double-edged sword. All information online could be used to identify individuals however the privacy law was not designed to consider what personal information should it protect and how to protect[7]. The recruiting tool Amazon developed since 2014 could be one of the examples. The program was supposedly used to review applicants\\' resumes to search for the people that are the best in capability. Although the intention is meant to create a gender-neutral system, the result came to be overwhelmingly male dominated. The reason behind it is because Amazon\\'s system automatically downgraded the resumes that included the word \"women\\'s\" in their applications. The company disbanded the team and announced that the tool \"was never used by Amazon recruiters to evaluate candidates.[8,9]\" However, it is hard to authorize their words and worry about the possibility of companies using these discriminated data on recruiting and hiring. This would not be an exception, other companies while using AI automation would face the same problem. Despite those concerns, more companies are still pushing hard to automate more parts of recruitment and hiring. The company', '\\'s software developers would need to actively monitor the system to ensure that something like that wasn\\'t happening. In general, AI works more efficiently than the human brain but when the system is massive, and the software is making decisions obscured behind a dashboard, there would be concerns of the potential for serious legal trouble here. Although under the laws enforced by EEOC( Equal Employment Opportunity Commission ), it is illegal to discriminate against someone (applicant or employee) because of that person\\'s race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age (40 or older), disability or genetic information. It is also illegal to retaliate against a person because he or she complained about discrimination, filed a charge of discrimination, or participated in an employment discrimination investigation or lawsuit[10]. However, when the information is unintentionally included and judged by companies, it would raise multiple questions due to the responsibilities issue.\\n4. IMPACT OF ALGORITHMS \\nAlgorithms could lead to gender bias. When we search for \"CEO\" on Google it returns overwhelmingly male images, we tell ourselves that Google is just reflecting the world to us \" a world where discrimination exists\". We believe that the Google bots that crawl the web are color and gender blind. We trust the algorithms that answering our search queries are more objective than humans. One couldn\\'t possibly make an argument that Google\\'s search algorithm and its related ad-serving platforms are inherently biased. Although they are potentially designed to reduce bias, most hiring algorithms still drift toward bias by default. In a recent study from Northeastern University and USC, broadly targeted ads on Facebook for supermarket cashier positions were shown to an audience of 85% women[11]. This could be a typical case where Advances in Social Science, Education and Humanities Research, volume 554\\n859 algorithms introduce bias into the system without human intervention. To ensure diversity in input data, collecting more training data specifically with sensitive groups would help with unfairness. When talking about another company, Google also reinforces sexual discrimination using the algorithm and it shows that AdFisher, an automated tool that explores how user behaviors, Google\\'s ads, and Ad Settings interact. AdFisher can run browser-based experiments and analyze data using machine learning and significance tests. The Ad Settings was opaque about some features of a user\\'s profile including providing some choice on ads. When setting the gender to female, it will result in getting fewer instances of an ad related to high paying jobs than setting it to males.\\nSome possible reasons might be Google explicitly programming the system to show the ad less often to females. Males and female consumers respond differently to ads and Google\\'s targeting algorithm responds to the difference (e.g., Google', \"learned that males are more likely to click on this ad than females are). More competition existing for advertising to females causes the advertiser to win fewer ad slots for females. Some third parties (e.g., a hacker) manipulate the ad ecosystem.\\nA research uses data from a field test of an ad that was intended to promote job opportunities and training in STEM (Science, Technology, Engineering and Math). The ad was intended to be gender-neutral and was targeted neutrally. This ad was tested in 191 countries across the world. However empirically, the ad was shown to 20% more men than women[12].\\nMany reasons contributed to this result algorithm trying to maximize the clicks thus showing ads more to men than women, or women are less likely to visit websites with ads on them. All results point to the same point is that because a word already releases women to lower wages / not STEM jobs, these algorithms' sexual discrimination would create a higher possibility to increase rather than reduce. To ensure diversity in input data, collecting more training data specifically with sensitive groups would help with unfairness.\\n5. SUGGESTION\\nCompanies should invest in training and reskilling, provide possible training and apprenticeship programs for women. Also, companies and the government could consider reskilling opportunities for mid career women or women returning to the workforce. Companies should subsidize transition costs, government or corporate reskilling subsidies for targeted occupations and sectors. Community and governments could provide childcare subsidies for parents undergoing reskilling or pursuing higher education. Governments could invest in digital platforms, industry partnerships with massive open online courses. Companies should increase transparency on labour demand trends, contribute to more technical school or university curriculums co-created with industry, invest in informational campaigns targeting women.\\nData testing: Put in place AI development standards, testing procedures, controls, and other technical governance elements designed to make sure the data used in training AI applications are thoroughly vetted and certified against a biased perspective before the application goes into production. Output testing: Establish testing requirements and controls around the outputs produced or decisions made by the AI.\\nReview and challenge these outputs and decisions against a biased perspective to make sure they represent fair and positive outcomes that are in line with expectations and do not adversely and unfairly impact any group of people. Development teams: Make sure AI design and development teams are diverse and include female data scientists, programmers, designers, and other key team members who influence how an AI application is developed. Set targets and put in place training, recruiting, and rotation programs to move toward this target.\\n6. CONCLUSION\\nSince automation is the major trend in the future, companies and the\", 'government should be double careful with the use of it. The effect of biased data, algorithms in the workforce could turnover the equality in the workforce. Thus companies must develop and deploy AI applications in a responsible manner that proactively seeks to identify and eliminate existing societal biases so they are not encoded and amplified in the digital world. Toward this goal, these are some future suggestions for companies and the government. It is important to note that, even though the focus of this essay is gender bias, AI applications can and often do suffer from different types of societal biases, for example, around race, ethnicity, and religion. As a result, companies should expand the above efforts and measures to make sure the AI applications they put in place do not hurt any group of people.\\nAI has the potential to mitigate the corporate gender and leadership gaps by removing bias in recruiting, Advances in Social Science, Education and Humanities Research, volume 554\\n860\\nevaluation, and promotion decisions, helping improve retention of women employees, and, potentially, by intervening in the everyday interactions that affect employees’ sense of inclusion. The effect of AI would create multiple concerns but does not mean that society would stop exploring this field. The purpose of creating technology is always for greater goods. However if AI and automation are not fully developed and applied in a gender-responsible method, they are likely to reproduce and reinforce existing gender stereotypes and discriminatory social norms. Thus the companies and individuals would need to be particularly careful and take responses to reduce gender discrimination. The government would also need to take part in law making and education fields to support equality.\\nACKNOWLEDGMENT\\nI would like to thank all the people involved in the completion of this paper. These people including Ms. Jia Qiong Sun corrected the paper, our TA Yuan Shen that helped me explore the AI technology and understand the material. I would like to thank Professor Sonia Katyal for both inspiration to begin this paper and the encouragement to carry on during the hard time.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text = \"\"\"\n",
    "INTRODUCTION\n",
    "The definition of sexual discrimination could be a form of discrimination that a person is treated differently or unequally because of their sex/gender. Although under federal law sex discrimination in employment is illegal, such discrimination still happens in the job field and has always been hidden under the norm. For example, 42 % of women in the United States have faced gender discrimination on the job[2]. One of the purposes of creating AI is to help with diversity, solve problems like discrimination and racism, however, whether it works as people predicted is still a question mark. The concern is if AI and automation are not used in a gender responsible method, and it might reinforce pre-existing gender bias.\n",
    "Due to better productivity and decrease of cost and laborers, it is not hard to imagine that in the future AI technology would involve more deeply in manufacturing and jobs in general. Automation, big data and algorithms could cause great impacts on women in jobs. The concern of job replacement, automated hiring system, privacy information releases to the public, poorly selected training data, the issue with algorithm design and issue due to data inequality would all raise or influence sexual discrimination. Companies and governments would need to take action to face changes caused by AI to ensure social orders, justice and equality.\n",
    "2. AUTOMATION’S EFFECT\n",
    "Although the world is facing unprecedented growth in both jobs and economics, automation still has a high potential to replace numerous jobs, and especially those technologies are repetitive and have little human interaction in comparison. The cost to automate would also be one of the considerations. Thus a majority of workers that involve predictable tasks and activities would have a high replacement rate. Then how is it going to relate to gender discrimination.\n",
    "Although there is a bigger change that women’s job is prone to partial automation than being entirely replaced. In McKinsey's future prediction of women’s traditions by 2030, around 40 million to 160 million women might face a need to transition across occupations and skill sets to remain employed[3]. This number also established the need for higher education and different skills for success. It is 7 to 24 percent of women that are currently employed compared to the range of 8 to 28 percent for men. If women take advantage of transition opportunities, they could maintain their current share of employment. Advances in Social Science, Education and Humanities Research, volume 554 Proceedings of the 7th International Conference on Humanities and Social Science Research (ICHSSR 2021) Copyright © 2021 The Authors. Published by Atlantis Press SARL. This is an open access article distributed under the CC BY-NC 4.0 license -http://creativecommons.org/licenses/by-nc/4.0/. 858 If they cannot, gender inequality in work could worsen. Thus, if they cannot make the necessary transition, many women could face an intensifying wage gap relative to men. Since talking automation, the STEM field would have a higher possibility for employment requirements in the future. However, the data in 2019 determined that women accounted for only 27% of workers in STEM-qualified industries, not to mention onaverage women made 19% less than men[4]. Not to mention, with 78% of AI professionals being men, algorithms are created with male dominated experiences. Such gender bias could be significantly disadvantageous to female employers or resumes. Although robotization and automation in the job field would impact both gender, gender bias is likely to come into play and affect women disproportionately. Women over-represented in certain high-risk automation sectors could suffer more. The lack of mobility and flexibility could also be reasons for companies to unemploy women or reduce bargaining positions[5]. The risks in automation are real. The US Equal Employment Opportunity Commission is investigating at least two cases involving algorithms that could be discriminated toward certain groups of job applicants[6]. To prevent sexual discrimination, the education system needs to change from the beginning to stop discrimination, provide more opportunities and support to STEM field, and future job supplements caused by automation.\n",
    "3. BIG DATA’S IMPACT\n",
    "Big data analysis and algorithm would also affect discrimination in occupations, it might turnover the traditional hiring process. Since the application is based on the collection of big data. If an AI application is trained on bias data, the algorithms would likely be biased. Good grade, school, or capabilities would not just be the only measurement. In the content of collecting more types of data, the metadata of the social media content, family members, anything remotely relevant would all be a double-edged sword. All information online could be used to identify individuals however the privacy law was not designed to consider what personal information should it protect and how to protect[7]. The recruiting tool Amazon developed since 2014 could be one of the examples. The program was supposedly used to review applicants' resumes to search for the people that are the best in capability. Although the intention is meant to create a gender-neutral system, the result came to be overwhelmingly male dominated. The reason behind it is because Amazon's system automatically downgraded the resumes that included the word \"women's\" in their applications. The company disbanded the team and announced that the tool \"was never used by Amazon recruiters to evaluate candidates.[8,9]\" However, it is hard to authorize their words and worry about the possibility of companies using these discriminated data on recruiting and hiring. This would not be an exception, other companies while using AI automation would face the same problem. Despite those concerns, more companies are still pushing hard to automate more parts of recruitment and hiring. The company's software developers would need to actively monitor the system to ensure that something like that wasn't happening. In general, AI works more efficiently than the human brain but when the system is massive, and the software is making decisions obscured behind a dashboard, there would be concerns of the potential for serious legal trouble here. Although under the laws enforced by EEOC( Equal Employment Opportunity Commission ), it is illegal to discriminate against someone (applicant or employee) because of that person's race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age (40 or older), disability or genetic information. It is also illegal to retaliate against a person because he or she complained about discrimination, filed a charge of discrimination, or participated in an employment discrimination investigation or lawsuit[10]. However, when the information is unintentionally included and judged by companies, it would raise multiple questions due to the responsibilities issue.\n",
    "4. IMPACT OF ALGORITHMS \n",
    "Algorithms could lead to gender bias. When we search for \"CEO\" on Google it returns overwhelmingly male images, we tell ourselves that Google is just reflecting the world to us \" a world where discrimination exists\". We believe that the Google bots that crawl the web are color and gender blind. We trust the algorithms that answering our search queries are more objective than humans. One couldn't possibly make an argument that Google's search algorithm and its related ad-serving platforms are inherently biased. Although they are potentially designed to reduce bias, most hiring algorithms still drift toward bias by default. In a recent study from Northeastern University and USC, broadly targeted ads on Facebook for supermarket cashier positions were shown to an audience of 85% women[11]. This could be a typical case where Advances in Social Science, Education and Humanities Research, volume 554\n",
    "859 algorithms introduce bias into the system without human intervention. To ensure diversity in input data, collecting more training data specifically with sensitive groups would help with unfairness. When talking about another company, Google also reinforces sexual discrimination using the algorithm and it shows that AdFisher, an automated tool that explores how user behaviors, Google's ads, and Ad Settings interact. AdFisher can run browser-based experiments and analyze data using machine learning and significance tests. The Ad Settings was opaque about some features of a user's profile including providing some choice on ads. When setting the gender to female, it will result in getting fewer instances of an ad related to high paying jobs than setting it to males.\n",
    "Some possible reasons might be Google explicitly programming the system to show the ad less often to females. Males and female consumers respond differently to ads and Google's targeting algorithm responds to the difference (e.g., Google learned that males are more likely to click on this ad than females are). More competition existing for advertising to females causes the advertiser to win fewer ad slots for females. Some third parties (e.g., a hacker) manipulate the ad ecosystem.\n",
    "A research uses data from a field test of an ad that was intended to promote job opportunities and training in STEM (Science, Technology, Engineering and Math). The ad was intended to be gender-neutral and was targeted neutrally. This ad was tested in 191 countries across the world. However empirically, the ad was shown to 20% more men than women[12].\n",
    "Many reasons contributed to this result algorithm trying to maximize the clicks thus showing ads more to men than women, or women are less likely to visit websites with ads on them. All results point to the same point is that because a word already releases women to lower wages / not STEM jobs, these algorithms' sexual discrimination would create a higher possibility to increase rather than reduce. To ensure diversity in input data, collecting more training data specifically with sensitive groups would help with unfairness.\n",
    "5. SUGGESTION\n",
    "Companies should invest in training and reskilling, provide possible training and apprenticeship programs for women. Also, companies and the government could consider reskilling opportunities for mid career women or women returning to the workforce. Companies should subsidize transition costs, government or corporate reskilling subsidies for targeted occupations and sectors. Community and governments could provide childcare subsidies for parents undergoing reskilling or pursuing higher education. Governments could invest in digital platforms, industry partnerships with massive open online courses. Companies should increase transparency on labour demand trends, contribute to more technical school or university curriculums co-created with industry, invest in informational campaigns targeting women.\n",
    "Data testing: Put in place AI development standards, testing procedures, controls, and other technical governance elements designed to make sure the data used in training AI applications are thoroughly vetted and certified against a biased perspective before the application goes into production. Output testing: Establish testing requirements and controls around the outputs produced or decisions made by the AI.\n",
    "Review and challenge these outputs and decisions against a biased perspective to make sure they represent fair and positive outcomes that are in line with expectations and do not adversely and unfairly impact any group of people. Development teams: Make sure AI design and development teams are diverse and include female data scientists, programmers, designers, and other key team members who influence how an AI application is developed. Set targets and put in place training, recruiting, and rotation programs to move toward this target.\n",
    "6. CONCLUSION\n",
    "Since automation is the major trend in the future, companies and the government should be double careful with the use of it. The effect of biased data, algorithms in the workforce could turnover the equality in the workforce. Thus companies must develop and deploy AI applications in a responsible manner that proactively seeks to identify and eliminate existing societal biases so they are not encoded and amplified in the digital world. Toward this goal, these are some future suggestions for companies and the government. It is important to note that, even though the focus of this essay is gender bias, AI applications can and often do suffer from different types of societal biases, for example, around race, ethnicity, and religion. As a result, companies should expand the above efforts and measures to make sure the AI applications they put in place do not hurt any group of people.\n",
    "AI has the potential to mitigate the corporate gender and leadership gaps by removing bias in recruiting, Advances in Social Science, Education and Humanities Research, volume 554\n",
    "860\n",
    "evaluation, and promotion decisions, helping improve retention of women employees, and, potentially, by intervening in the everyday interactions that affect employees’ sense of inclusion. The effect of AI would create multiple concerns but does not mean that society would stop exploring this field. The purpose of creating technology is always for greater goods. However if AI and automation are not fully developed and applied in a gender-responsible method, they are likely to reproduce and reinforce existing gender stereotypes and discriminatory social norms. Thus the companies and individuals would need to be particularly careful and take responses to reduce gender discrimination. The government would also need to take part in law making and education fields to support equality.\n",
    "ACKNOWLEDGMENT\n",
    "I would like to thank all the people involved in the completion of this paper. These people including Ms. Jia Qiong Sun corrected the paper, our TA Yuan Shen that helped me explore the AI technology and understand the material. I would like to thank Professor Sonia Katyal for both inspiration to begin this paper and the encouragement to carry on during the hard time.\n",
    "\"\"\"\n",
    "\n",
    "chunks = chunk_text(full_text)\n",
    "print(chunks)\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geslachtsdiscriminatie is een vorm van ongelijkheid waarbij iemand anders wordt behandeld vanwege hun geslacht. Hoewel discriminatie op basis van geslacht in de Verenigde Staten volgens federale wetten verboden is, gebeurt het nog steeds in de arbeidsmarkt. 42% van de vrouwen in de VS heeft te maken gehad met discriminatie op basis van geslacht.\n",
      "\n",
      "AI is bedoeld om diversiteit te bevorderen en problemen zoals discriminatie en racisme op te lossen. Maar of het werkt zoals verwacht is nog de vraag. Er is zorg dat AI en automatisering niet op een genderverantwoorde manier worden gebruikt en bestaande genderongelijkheid kunnen versterken.\n",
      "\n",
      "Automatisering, big data en algoritmes kunnen grote gevolgen hebben voor vrouwen op de arbeidsmarkt. Er is zorg over vervanging van banen, automatisch aannemen, privacyinformatie die openbaar wordt, slecht geselecteerde trainingsdata, problemen met algoritmeontwerp en problemen door ongelijkheid in data. Bedrijven en overheden moeten actie ondernemen om de veranderingen door AI tegemoet te treden om sociale orde, rechtvaardigheid en gelijkheid te waarborgen.\n",
      "\n",
      "Hoewel de wereld een ongekende groei ziet in banen en economie, heeft automatisering nog steeds een hoog potentieel om veel banen te vervangen. Vooral banen met weinig menselijke interactie. Volgens McKinsey zou tussen 40 en 160 miljoen vrouwen in 2030 een overstap moeten maken naar andere beroepen en vaardigheden om in dienst te blijven. Als vrouwen deze kansen niet benutten, kan de ongelijkheid op de arbeidsmarkt toenemen. Daarom is het belangrijk dat vrouwen de benodigde overgang maken.\n",
      "Vrouwen hebben het moeilijker om een baan te vinden dan mannen. Automatisering heeft de kloof tussen de lonen van mannen en vrouwen vergroot. In 2019 was 27% van de werknemers in STEM-gerelateerde industrieën vrouw, en verdienden vrouwen gemiddeld 19% minder dan mannen. 78% van de AI-professionals is man, waardoor algoritmes worden gemaakt met mannelijke ervaringen. Dit genderbias kan vrouwen benadelen. Automatisering zal beide genders beïnvloeden, maar vrouwen zullen disproportioneel worden getroffen.\n",
      "\n",
      "Big data analyse en algoritmes kunnen ook discriminatie in banen veroorzaken. Als een AI-applicatie is getraind op vooroordelige data, zal het algoritme waarschijnlijk ook vooroordelig zijn. Goede cijfers, scholen of vaardigheden zijn niet langer de enige meting. Bedrijven kunnen persoonlijke informatie gebruiken om individuen te identificeren, maar de privacywetgeving is niet ontworpen om te beslissen welke persoonlijke informatie moet worden beschermd en hoe. Amazon ontwikkelde een rekruteringshulpmiddel dat vooral mannen bevoorrechtte. Het bedrijf stopte met het gebruik van het hulpmiddel, maar het is moeilijk om te weten of andere bedrijven hetzelfde doen. Om discriminatie te voorkomen, moet het onderwijssysteem veranderen om discriminatie te stoppen, meer kansen en ondersteuning te bieden aan STEM-velden en toekomstige banen te compenseren die worden veroorzaakt door automatisering.\n",
      "Softwareontwikkelaars moeten actief het systeem monitoren om te zorgen dat er geen discriminatie plaatsvindt. AI werkt meestal efficiënter dan het menselijk brein, maar als het systeem groot is en de software beslissingen maakt die verborgen zijn achter een dashboard, zijn er zorgen over de mogelijke gevolgen. Volgens de wetten van de Equal Employment Opportunity Commission (EEOC) is het illegaal om iemand (sollicitant of werknemer) te discrimineren op basis van ras, kleur, religie, geslacht (inclusief genderidentiteit, seksuele geaardheid en zwangerschap), nationaliteit, leeftijd (40 jaar of ouder), handicap of genetische informatie. Het is ook illegaal om wraak te nemen op iemand omdat hij of zij klachten heeft ingediend over discriminatie, een klacht van discriminatie heeft ingediend of heeft deelgenomen aan een onderzoek of rechtszaak.\n",
      "Algoritmes kunnen leiden tot genderbias. Als we bijvoorbeeld zoeken naar \"CEO\" op Google, krijgen we voornamelijk mannelijke afbeeldingen. We geloven dat de Google-bots die het web crawlen, kleur- en genderblind zijn. We vertrouwen erop dat algoritmes die onze zoekopdrachten beantwoorden, objectiever zijn dan mensen. Google's zoekalgoritme en gerelateerde advertentieplatformen kunnen echter ook onbewust voor bias zorgen. In een recent onderzoek van de Northeastern University en USC werden brede doelgerichte advertenties op Facebook voor supermarktkassiers aan een publiek van 85% vrouwen getoond. Google versterkt ook seksuele discriminatie met algoritmes. Om ongelijkheid te voorkomen, is het verzamelen van meer trainingsgegevens met specifieke groepen nodig.\n",
      "government should take steps to ensure that AI applications are not biased against any group of people. Companies should invest in training and reskilling, provide possible training and apprenticeship programs for women. Governments should invest in digital platforms, industry partnerships with massive open online courses. Data testing and output testing should be put in place to make sure the data used in training AI applications are thoroughly vetted and certified against a biased perspective. Development teams should be diverse and include female data scientists, programmers, designers, and other key team members. Alleen door deze stappen te nemen, kunnen bedrijven en de overheid ervoor zorgen dat AI-applicaties niet vooroordelen hebben tegen een bepaalde groep mensen.\n",
      "\n",
      "Bedrijven moeten investeren in opleiding en herscholing, mogelijke opleidingen en leerbanen aanbieden voor vrouwen. Overheden moeten investeren in digitale platformen, samenwerkingen met open online cursussen. Data- en outputtesten moeten worden uitgevoerd om ervoor te zorgen dat de data die wordt gebruikt bij het trainen van AI-applicaties grondig worden gecontroleerd en gecertificeerd tegen een vooroordelige achtergrond. Ontwikkelteams moeten divers zijn en vrouwelijke datawetenschappers, programmeurs, ontwerpers en andere belangrijke teamleden bevatten. Alleen door deze stappen te nemen, kunnen bedrijven en de overheid ervoor zorgen dat AI-applicaties niet vooroordelen hebben tegen een bepaalde groep mensen.\n",
      "AI heeft het potentieel om de gender- en leiderschapskloof in bedrijven te verkleinen door vooroordelen in het rekruteren, evalueren en bevorderen te verwijderen. Dit helpt om vrouwen in dienst te houden en kan mogelijk ook ingrijpen in dagelijkse interacties die invloed hebben op het gevoel van inclusie. Het effect van AI zou echter meerdere zorgen kunnen oproepen. Daarom is het belangrijk dat bedrijven en individuen voorzichtig zijn en maatregelen nemen om discriminatie tegen te gaan. Ook moet de overheid wetgeving en onderwijs ondersteunen om gelijkheid te bevorderen.\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    prompt = f\"\"\"prompt:\n",
    "Vereenvoudig deze tekst met max 150 woorden. Max. twee Nederlandstalige paragrafen. Max tien zinnen. Een zin is max 10 woorden lang.\n",
    "Gebruik regelmatige woordenschat en schrijf alle cijfers en acroniemen voluit\n",
    "Vervang moeilijke woorden, schrijf cijfers voluit\n",
    "Vermijd tangconstructies, voorzetsel- en verwijswoorden\n",
    "\n",
    "tekst:\n",
    "{chunk}\n",
    "\"\"\"\n",
    "\n",
    "    result = openai.Completion.create(\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=800,\n",
    "    top_p=0,\n",
    "    model=COMPLETIONS_MODEL\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
    "\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nINTRODUCTION\\nThe definition of sexual discrimination could be a form of discrimination that a person is treated differently or unequally because of their sex/gender. Although under federal law sex discrimination in employment is illegal, such discrimination still happens in the job field and has always been hidden under the norm. For example, 42 % of women in the United States have faced gender discrimination on the job[2]. One of the purposes of creating AI is to help with diversity, solve problems like discrimination and racism, however, whether it works as people predicted is still a question mark. The concern is if AI and automation are not used in a gender responsible method, and it might reinforce pre-existing gender bias.\\nDue to better productivity and decrease of cost and laborers, it is not hard to imagine that in the future AI technology would involve more deeply in manufacturing and jobs in general. Automation, big data and algorithms could cause great impacts on women in jobs. The concern of job replacement, automated hiring system, privacy information releases to the public, poorly selected training data, the issue with algorithm design and issue due to data inequality would all raise or influence sexual discrimination. Companies and governments would need to take action to face changes caused by AI to ensure social orders, justice and equality.\\n2. AUTOMATION’S EFFECT\\nAlthough the world is facing unprecedented growth in both jobs and economics, automation still has a high potential to replace numerous jobs, and especially those technologies are repetitive and have little human interaction in comparison. The cost to automate would also be one of the considerations. Thus a majority of workers that involve predictable tasks and activities would have a high replacement rate. Then how is it going to relate to gender discrimination.\\nAlthough there is a bigger change that women’s job is prone to partial automation than being entirely replaced. In McKinsey\\'s future prediction of women’s traditions by 2030, around 40 million to 160 million women might face a need to transition across occupations and skill sets to remain employed[3]. This number also established the need for higher education and different skills for success. It is 7 to 24 percent of women that are currently employed compared to the range of 8 to 28 percent for men. If women take advantage of transition opportunities, they could maintain their current share of employment. Advances in Social Science, Education and Humanities Research, volume 554 Proceedings of the 7th International Conference on Humanities and Social Science Research (ICHSSR 2021) Copyright © 2021 The Authors. Published by Atlantis Press SARL. This is an open access article distributed under the CC BY-NC 4.0 license -http://creativecommons.org/licenses/by-nc/4.0/. 858 If they cannot, gender inequality in work could worsen. Thus, if they cannot make the necessary transition, many women could face an intensifying wage gap relative to men. Since talking automation, the STEM field would have a higher possibility for employment requirements in the future. However, the data in 2019 determined that women accounted for only 27% of workers in STEM-qualified industries, not to mention onaverage women made 19% less than men[4]. Not to mention, with 78% of AI professionals being men, algorithms are created with male dominated experiences. Such gender bias could be significantly disadvantageous to female employers or resumes. Although robotization and automation in the job field would impact both gender, gender bias is likely to come into play and affect women disproportionately. Women over-represented in certain high-risk automation sectors could suffer more. The lack of mobility and flexibility could also be reasons for companies to unemploy women or reduce bargaining positions[5]. The risks in automation are real. The US Equal Employment Opportunity Commission is investigating at least two cases involving algorithms that could be discriminated toward certain groups of job applicants[6]. To prevent sexual discrimination, the education system needs to change from the beginning to stop discrimination, provide more opportunities and support to STEM field, and future job supplements caused by automation.\\n3. BIG DATA’S IMPACT\\nBig data analysis and algorithm would also affect discrimination in occupations, it might turnover the traditional hiring process. Since the application is based on the collection of big data. If an AI application is trained on bias data, the algorithms would likely be biased. Good grade, school, or capabilities would not just be the only measurement. In the content of collecting more types of data, the metadata of the social media content, family members, anything remotely relevant would all be a double-edged sword. All information online could be used to identify individuals however the privacy law was not designed to consider what personal information should it protect and how to protect[7]. The recruiting tool Amazon developed since 2014 could be one of the examples. The program was supposedly used to review applicants\\' resumes to search for the people that are the best in capability. Although the intention is meant to create a gender-neutral system, the result came to be overwhelmingly male dominated. The reason behind it is because Amazon\\'s system automatically downgraded the resumes that included the word \"women\\'s\" in their applications. The company disbanded the team and announced that the tool \"was never used by Amazon recruiters to evaluate candidates.[8,9]\" However, it is hard to authorize their words and worry about the possibility of companies using these discriminated data on recruiting and hiring. This would not be an exception, other companies while using AI automation would face the same problem. Despite those concerns, more companies are still pushing hard to automate more parts of recruitment and hiring. The company\\'s software developers would need to actively monitor the system to ensure that something like that wasn\\'t happening. In general, AI works more efficiently than the human brain but when the system is massive, and the software is making decisions obscured behind a dashboard, there would be concerns of the potential for serious legal trouble here. Although under the laws enforced by EEOC( Equal Employment Opportunity Commission ), it is illegal to discriminate against someone (applicant or employee) because of that person\\'s race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age (40 or older), disability or genetic information. It is also illegal to retaliate against a person because he or she complained about discrimination, filed a charge of discrimination, or participated in an employment discrimination investigation or lawsuit[10]. However, when the information is unintentionally included and judged by companies, it would raise multiple questions due to the responsibilities issue.\\n4. IMPACT OF ALGORITHMS \\nAlgorithms could lead to gender bias. When we search for \"CEO\" on Google it returns overwhelmingly male images, we tell ourselves that Google is just reflecting the world to us \" a world where discrimination exists\". We believe that the Google bots that crawl the web are color and gender blind. We trust the algorithms that answering our search queries are more objective than humans. One couldn\\'t possibly make an argument that Google\\'s search algorithm and its related ad-serving platforms are inherently biased. Although they are potentially designed to reduce bias, most hiring algorithms still drift toward bias by default. In a recent study from Northeastern University and USC, broadly targeted ads on Facebook for supermarket cashier positions were shown to an audience of 85% women[11]. This could be a typical case where Advances in Social Science, Education and Humanities Research, volume 554\\n859 algorithms introduce bias into the system without human intervention. To ensure diversity in input data, collecting more training data specifically with sensitive groups would help with unfairness. When talking about another company, Google also reinforces sexual discrimination using the algorithm and it shows that AdFisher, an automated tool that explores how user behaviors, Google\\'s ads, and Ad Settings interact. AdFisher can run browser-based experiments and analyze data using machine learning and significance tests. The Ad Settings was opaque about some features of a user\\'s profile including providing some choice on ads. When setting the gender to female, it will result in getting fewer instances of an ad related to high paying jobs than setting it to males.\\nSome possible reasons might be Google explicitly programming the system to show the ad less often to females. Males and female consumers respond differently to ads and Google\\'s targeting algorithm responds to the difference (e.g., Google learned that males are more likely to click on this ad than females are). More competition existing for advertising to females causes the advertiser to win fewer ad slots for females. Some third parties (e.g., a hacker) manipulate the ad ecosystem.\\nA research uses data from a field test of an ad that was intended to promote job opportunities and training in STEM (Science, Technology, Engineering and Math). The ad was intended to be gender-neutral and was targeted neutrally. This ad was tested in 191 countries across the world. However empirically, the ad was shown to 20% more men than women[12].\\nMany reasons contributed to this result algorithm trying to maximize the clicks thus showing ads more to men than women, or women are less likely to visit websites with ads on them. All results point to the same point is that because a word already releases women to lower wages / not STEM jobs, these algorithms\\' sexual discrimination would create a higher possibility to increase rather than reduce. To ensure diversity in input data, collecting more training data specifically with sensitive groups would help with unfairness.\\n5. SUGGESTION\\nCompanies should invest in training and reskilling, provide possible training and apprenticeship programs for women. Also, companies and the government could consider reskilling opportunities for mid career women or women returning to the workforce. Companies should subsidize transition costs, government or corporate reskilling subsidies for targeted occupations and sectors. Community and governments could provide childcare subsidies for parents undergoing reskilling or pursuing higher education. Governments could invest in digital platforms, industry partnerships with massive open online courses. Companies should increase transparency on labour demand trends, contribute to more technical school or university curriculums co-created with industry, invest in informational campaigns targeting women.\\nData testing: Put in place AI development standards, testing procedures, controls, and other technical governance elements designed to make sure the data used in training AI applications are thoroughly vetted and certified against a biased perspective before the application goes into production. Output testing: Establish testing requirements and controls around the outputs produced or decisions made by the AI.\\nReview and challenge these outputs and decisions against a biased perspective to make sure they represent fair and positive outcomes that are in line with expectations and do not adversely and unfairly impact any group of people. Development teams: Make sure AI design and development teams are diverse and include female data scientists, programmers, designers, and other key team members who influence how an AI application is developed. Set targets and put in place training, recruiting, and rotation programs to move toward this target.\\n6. CONCLUSION\\nSince automation is the major trend in the future, companies and the government should be double careful with the use of it. The effect of biased data, algorithms in the workforce could turnover the equality in the workforce. Thus companies must develop and deploy AI applications in a responsible manner that proactively seeks to identify and eliminate existing societal biases so they are not encoded and amplified in the digital world. Toward this goal, these are some future suggestions for companies and the government. It is important to note that, even though the focus of this essay is gender bias, AI applications can and often do suffer from different types of societal biases, for example, around race, ethnicity, and religion. As a result, companies should expand the above efforts and measures to make sure the AI applications they put in place do not hurt any group of people.\\nAI has the potential to mitigate the corporate gender and leadership gaps by removing bias in recruiting, Advances in Social Science, Education and Humanities Research, volume 554\\n860\\nevaluation, and promotion decisions, helping improve retention of women employees, and, potentially, by intervening in the everyday interactions that affect employees’ sense of inclusion. The effect of AI would create multiple concerns but does not mean that society would stop exploring this field. The purpose of creating technology is always for greater goods. However if AI and automation are not fully developed and applied in a gender-responsible method, they are likely to reproduce and reinforce existing gender stereotypes and discriminatory social norms. Thus the companies and individuals would need to be particularly careful and take responses to reduce gender discrimination. The government would also need to take part in law making and education fields to support equality.\\nACKNOWLEDGMENT\\nI would like to thank all the people involved in the completion of this paper. These people including Ms. Jia Qiong Sun corrected the paper, our TA Yuan Shen that helped me explore the AI technology and understand the material. I would like to thank Professor Sonia Katyal for both inspiration to begin this paper and the encouragement to carry on during the hard time.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from summarizer import Summarizer\n",
    "\n",
    "summarizer = Summarizer()\n",
    "result = summarizer(\n",
    "    #algorithm=...,\n",
    "    body=full_text,\n",
    "    num_sentences=10,\n",
    "    #ratio=...,\n",
    "    #return_as_list=...,\n",
    "    #use_first=...,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.replace('.','.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTRODUCTION\n",
      "The definition of sexual discrimination could be a form of discrimination that a person is treated differently or unequally because of their sex/gender.\n",
      " The concern is if AI and automation are not used in a gender responsible method, and it might reinforce pre-existing gender bias.\n",
      " Thus, if they cannot make the necessary transition, many women could face an intensifying wage gap relative to men.\n",
      " To ensure diversity in input data, collecting more training data specifically with sensitive groups would help with unfairness.\n",
      " AdFisher can run browser-based experiments and analyze data using machine learning and significance tests.\n",
      " When setting the gender to female, it will result in getting fewer instances of an ad related to high paying jobs than setting it to males.\n",
      " Many reasons contributed to this result algorithm trying to maximize the clicks thus showing ads more to men than women, or women are less likely to visit websites with ads on them.\n",
      " Companies should subsidize transition costs, government or corporate reskilling subsidies for targeted occupations and sectors.\n",
      " Development teams: Make sure AI design and development teams are diverse and include female data scientists, programmers, designers, and other key team members who influence how an AI application is developed.\n",
      " CONCLUSION\n",
      "Since automation is the major trend in the future, companies and the government should be double careful with the use of it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for zin in result.split('\\n'):\n",
    "    print(zin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"INTRODUCTION\\nThe definition of sexual discrimination could be a form of discrimination that a person is treated differently or unequally because of their sex/gender. Although under federal law sex discrimination in employment is illegal, such discrimination still happens in the job field and has always been hidden under the norm. For example, 42 % of women in the United States have faced gender discrimination on the job[2]. One of the purposes of creating AI is to help with diversity, solve problems like discrimination and racism, however, whether it works as people predicted is still a question mark. The concern is if AI and automation are not used in a gender responsible method, and it might reinforce pre-existing gender bias.\\nDue to better productivity and decrease of cost and laborers, it is not hard to imagine that in the future AI technology would involve more deeply in manufacturing and jobs in general. Automation, big data and algorithms could cause great impacts on women in jobs. The concern of job replacement, automated hiring system, privacy information releases to the public, poorly selected training data, the issue with algorithm design and issue due to data inequality would all raise or influence sexual discrimination. Companies and governments would need to take action to face changes caused by AI to ensure social orders, justice and equality.\\n2. AUTOMATION’S EFFECT\\nAlthough the world is facing unprecedented growth in both jobs and economics, automation still has a high potential to replace numerous jobs, and especially those technologies are repetitive and have little human interaction in comparison. The cost to automate would also be one of the considerations. Thus a majority of workers that involve predictable tasks and activities would have a high replacement rate. Then how is it going to relate to gender discrimination.\\nAlthough there is a bigger change that women’s job is prone to partial automation than being entirely replaced. In McKinsey's future prediction of women’s traditions by 2030, around 40 million to 160 million women might face a need to transition across occupations and skill sets to remain employed[3]. This number also established the need for higher education and different skills for success. It is 7 to 24 percent of women that are currently employed compared to the range of 8 to 28 percent for men. If women take advantage of transition opportunities, they could maintain their current share of employment. Advances in Social Science, Education and Humanities Research, volume 554 Proceedings of the 7th International Conference on Humanities and Social Science Research (ICHSSR 2021) Copyright © 2021 The Authors. Published by Atlantis Press SARL. This is an open access article distributed under the CC BY-NC 4.0 license -http://creativecommons.org/licenses/by-nc/4.0/. 858 If they cannot, gender inequality in work could worsen. Thus, if they cannot make the necessary transition, many women could face an\", 'intensifying wage gap relative to men. Since talking automation, the STEM field would have a higher possibility for employment requirements in the future. However, the data in 2019 determined that women accounted for only 27% of workers in STEM-qualified industries, not to mention onaverage women made 19% less than men[4]. Not to mention, with 78% of AI professionals being men, algorithms are created with male dominated experiences. Such gender bias could be significantly disadvantageous to female employers or resumes. Although robotization and automation in the job field would impact both gender, gender bias is likely to come into play and affect women disproportionately. Women over-represented in certain high-risk automation sectors could suffer more. The lack of mobility and flexibility could also be reasons for companies to unemploy women or reduce bargaining positions[5]. The risks in automation are real. The US Equal Employment Opportunity Commission is investigating at least two cases involving algorithms that could be discriminated toward certain groups of job applicants[6]. To prevent sexual discrimination, the education system needs to change from the beginning to stop discrimination, provide more opportunities and support to STEM field, and future job supplements caused by automation.\\n3. BIG DATA’S IMPACT\\nBig data analysis and algorithm would also affect discrimination in occupations, it might turnover the traditional hiring process. Since the application is based on the collection of big data. If an AI application is trained on bias data, the algorithms would likely be biased. Good grade, school, or capabilities would not just be the only measurement. In the content of collecting more types of data, the metadata of the social media content, family members, anything remotely relevant would all be a double-edged sword. All information online could be used to identify individuals however the privacy law was not designed to consider what personal information should it protect and how to protect[7]. The recruiting tool Amazon developed since 2014 could be one of the examples. The program was supposedly used to review applicants\\' resumes to search for the people that are the best in capability. Although the intention is meant to create a gender-neutral system, the result came to be overwhelmingly male dominated. The reason behind it is because Amazon\\'s system automatically downgraded the resumes that included the word \"women\\'s\" in their applications. The company disbanded the team and announced that the tool \"was never used by Amazon recruiters to evaluate candidates.[8,9]\" However, it is hard to authorize their words and worry about the possibility of companies using these discriminated data on recruiting and hiring. This would not be an exception, other companies while using AI automation would face the same problem. Despite those concerns, more companies are still pushing hard to automate more parts of recruitment and hiring. The company', '\\'s software developers would need to actively monitor the system to ensure that something like that wasn\\'t happening. In general, AI works more efficiently than the human brain but when the system is massive, and the software is making decisions obscured behind a dashboard, there would be concerns of the potential for serious legal trouble here. Although under the laws enforced by EEOC( Equal Employment Opportunity Commission ), it is illegal to discriminate against someone (applicant or employee) because of that person\\'s race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age (40 or older), disability or genetic information. It is also illegal to retaliate against a person because he or she complained about discrimination, filed a charge of discrimination, or participated in an employment discrimination investigation or lawsuit[10]. However, when the information is unintentionally included and judged by companies, it would raise multiple questions due to the responsibilities issue.\\n4. IMPACT OF ALGORITHMS \\nAlgorithms could lead to gender bias. When we search for \"CEO\" on Google it returns overwhelmingly male images, we tell ourselves that Google is just reflecting the world to us \" a world where discrimination exists\". We believe that the Google bots that crawl the web are color and gender blind. We trust the algorithms that answering our search queries are more objective than humans. One couldn\\'t possibly make an argument that Google\\'s search algorithm and its related ad-serving platforms are inherently biased. Although they are potentially designed to reduce bias, most hiring algorithms still drift toward bias by default. In a recent study from Northeastern University and USC, broadly targeted ads on Facebook for supermarket cashier positions were shown to an audience of 85% women[11]. This could be a typical case where Advances in Social Science, Education and Humanities Research, volume 554\\n859 algorithms introduce bias into the system without human intervention. To ensure diversity in input data, collecting more training data specifically with sensitive groups would help with unfairness. When talking about another company, Google also reinforces sexual discrimination using the algorithm and it shows that AdFisher, an automated tool that explores how user behaviors, Google\\'s ads, and Ad Settings interact. AdFisher can run browser-based experiments and analyze data using machine learning and significance tests. The Ad Settings was opaque about some features of a user\\'s profile including providing some choice on ads. When setting the gender to female, it will result in getting fewer instances of an ad related to high paying jobs than setting it to males.\\nSome possible reasons might be Google explicitly programming the system to show the ad less often to females. Males and female consumers respond differently to ads and Google\\'s targeting algorithm responds to the difference (e.g., Google', \"learned that males are more likely to click on this ad than females are). More competition existing for advertising to females causes the advertiser to win fewer ad slots for females. Some third parties (e.g., a hacker) manipulate the ad ecosystem.\\nA research uses data from a field test of an ad that was intended to promote job opportunities and training in STEM (Science, Technology, Engineering and Math). The ad was intended to be gender-neutral and was targeted neutrally. This ad was tested in 191 countries across the world. However empirically, the ad was shown to 20% more men than women[12].\\nMany reasons contributed to this result algorithm trying to maximize the clicks thus showing ads more to men than women, or women are less likely to visit websites with ads on them. All results point to the same point is that because a word already releases women to lower wages / not STEM jobs, these algorithms' sexual discrimination would create a higher possibility to increase rather than reduce. To ensure diversity in input data, collecting more training data specifically with sensitive groups would help with unfairness.\\n5. SUGGESTION\\nCompanies should invest in training and reskilling, provide possible training and apprenticeship programs for women. Also, companies and the government could consider reskilling opportunities for mid career women or women returning to the workforce. Companies should subsidize transition costs, government or corporate reskilling subsidies for targeted occupations and sectors. Community and governments could provide childcare subsidies for parents undergoing reskilling or pursuing higher education. Governments could invest in digital platforms, industry partnerships with massive open online courses. Companies should increase transparency on labour demand trends, contribute to more technical school or university curriculums co-created with industry, invest in informational campaigns targeting women.\\nData testing: Put in place AI development standards, testing procedures, controls, and other technical governance elements designed to make sure the data used in training AI applications are thoroughly vetted and certified against a biased perspective before the application goes into production. Output testing: Establish testing requirements and controls around the outputs produced or decisions made by the AI.\\nReview and challenge these outputs and decisions against a biased perspective to make sure they represent fair and positive outcomes that are in line with expectations and do not adversely and unfairly impact any group of people. Development teams: Make sure AI design and development teams are diverse and include female data scientists, programmers, designers, and other key team members who influence how an AI application is developed. Set targets and put in place training, recruiting, and rotation programs to move toward this target.\\n6. CONCLUSION\\nSince automation is the major trend in the future, companies and the\", 'government should be double careful with the use of it. The effect of biased data, algorithms in the workforce could turnover the equality in the workforce. Thus companies must develop and deploy AI applications in a responsible manner that proactively seeks to identify and eliminate existing societal biases so they are not encoded and amplified in the digital world. Toward this goal, these are some future suggestions for companies and the government. It is important to note that, even though the focus of this essay is gender bias, AI applications can and often do suffer from different types of societal biases, for example, around race, ethnicity, and religion. As a result, companies should expand the above efforts and measures to make sure the AI applications they put in place do not hurt any group of people.\\nAI has the potential to mitigate the corporate gender and leadership gaps by removing bias in recruiting, Advances in Social Science, Education and Humanities Research, volume 554\\n860\\nevaluation, and promotion decisions, helping improve retention of women employees, and, potentially, by intervening in the everyday interactions that affect employees’ sense of inclusion. The effect of AI would create multiple concerns but does not mean that society would stop exploring this field. The purpose of creating technology is always for greater goods. However if AI and automation are not fully developed and applied in a gender-responsible method, they are likely to reproduce and reinforce existing gender stereotypes and discriminatory social norms. Thus the companies and individuals would need to be particularly careful and take responses to reduce gender discrimination. The government would also need to take part in law making and education fields to support equality.\\nACKNOWLEDGMENT\\nI would like to thank all the people involved in the completion of this paper. These people including Ms. Jia Qiong Sun corrected the paper, our TA Yuan Shen that helped me explore the AI technology and understand the material. I would like to thank Professor Sonia Katyal for both inspiration to begin this paper and the encouragement to carry on during the hard time.']\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_text(full_text)\n",
    "print([chunk for chunk in chunks])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89da10480825c8ad24c96d10788e772e9a68ac77e314b3c42d655a8c40ed70a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
