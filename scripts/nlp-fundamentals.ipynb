{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lookups import Lookups\n",
    "\n",
    "\n",
    "# nltk\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"nl_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenisatie\n",
    "doc = nlp(\"This is the so-called lemmatization\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatiseren\n",
    "lookups = Lookups()\n",
    "lemmatizer = Lemmatizer(lookups)\n",
    "\n",
    "words_to_lemmatize = [\"cats\", \"brings\", \"sings\"]\n",
    "\n",
    "for w in words_to_lemmatize:\n",
    "    lemma = lemmatizer(w, \"NOUN\")\n",
    "    print(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "doc = 'I prefer not to argue'\n",
    "for token in doc.split(\" \"):\n",
    "    print(token, '=>' , stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zinsegmentatie\n",
    "doc = nlp(\"I love coding and programming. I also love sleeping!\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
